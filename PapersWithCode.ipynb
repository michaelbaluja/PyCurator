{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19e712f2",
   "metadata": {},
   "source": [
    "# Papers With Code API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b24e9",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f1c86",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da05af",
   "metadata": {},
   "source": [
    "This notebook utilizes the Papers With Code API. Follow these steps in order to get the necessary credentials to continue:\n",
    "1. Create a Papers With Code account at https://paperswithcode.com/accounts/register?next=/\n",
    "2. After logging in, click on the user account icon in the top right corner, and click on 'Get API token'\n",
    "3. Click on 'Generate API Token'\n",
    "4. Load API key:\n",
    "    - For repeated use, follow the ```pickle_tutorial.ipynb``` instructions to create create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'PAPERSWITHCODE_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "    - For sparser use, users can run the credentials cell and paste their API key when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db88895",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25968f4",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Papers With Code API ([Papers With Code](https://paperswithcode.com/api/v1/docs/))\n",
    "- Papers With Code API ([readthedocs](https://paperswithcode-client.readthedocs.io/en/latest/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7557e9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c9ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "from utils import flatten_nested_df\n",
    "from flatten_json import flatten\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57d0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "try:\n",
    "    with open('credentials.pkl', 'rb') as credentials:\n",
    "        PWC_TOKEN = pickle.load(credentials)['PAPERSWITHCODE_TOKEN']\n",
    "except:\n",
    "    PWC_TOKEN = input('Please enter your Papers With Code API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7abd8",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f91bf3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d51843",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://paperswithcode.com/api/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c51e35",
   "metadata": {},
   "source": [
    "## Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f74c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_types, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the Papers With Code API for each search type. \n",
    "    Results are retured in results['({type},)'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_types : list-like \n",
    "        collection of search types to query over\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - results : dict\n",
    "        dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_type in search_types:\n",
    "        results[(search_type,)] = get_individual_search_output(search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca19562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conduct_search_over_pages(search_url, search_params, flatten_output=False):\n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    # Conduct a search, extract json results\n",
    "    response = requests.get(url = search_url, params=search_params)\n",
    "    output = response.json()\n",
    "\n",
    "    # Search over all valid pages\n",
    "    while output.get('results'):\n",
    "        # Flatten nested json\n",
    "        if flatten_output:\n",
    "            output = [flatten(result) for result in output['results']]\n",
    "        else:\n",
    "            output = output['results']\n",
    "\n",
    "        # Add results to cumulative DataFrame\n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['page'] = search_params['page']\n",
    "\n",
    "        search_df = pd.concat([search_df, output_df]).reset_index(drop=True)\n",
    "\n",
    "        # Increment page for search\n",
    "        search_params['page'] += 1\n",
    "        \n",
    "        # Conduct a search\n",
    "        response = requests.get(url = search_url, params=search_params)\n",
    "        \n",
    "        # Ensure we've received results if they exist\n",
    "        # 200: OK, 404: page not found\n",
    "        while response.status_code not in [200, 404]:\n",
    "            print(f'{search_type} search error {response.status_code} on page {search_params[\"page\"]}')\n",
    "            search_params['page'] += 1\n",
    "            # Conduct a search, extract json results\n",
    "            response = requests.get(url = search_url, params=search_params)\n",
    "            \n",
    "        # Extract json results\n",
    "        output = response.json()\n",
    "    \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5254c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Calls the Papers With Code API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_type : str\n",
    "        Must be in ('conferences', 'datasets', 'evaluations', 'papers', 'tasks')\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "   \n",
    "    Returns:\n",
    "    - pandas.DataFrame\n",
    "        DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert search_type in ('conferences', 'datasets', 'evaluations', 'papers', 'tasks'), \\\n",
    "        f'Invalid search type \"{search_type}\"'\n",
    "    \n",
    "    # Set search variables\n",
    "    start_page = 1\n",
    "    page_size = 500 # Seems to be max size\n",
    "    search_url = f'{BASE_URL}/{search_type}'\n",
    "    \n",
    "    search_params = {\n",
    "        'page': start_page,\n",
    "        'items_per_page': page_size\n",
    "        }\n",
    "    \n",
    "    return _conduct_search_over_pages(search_url, search_params, flatten_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6850ad",
   "metadata": {},
   "source": [
    "### Run initial API query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ca6e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_types = ['papers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b783810",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b3a741eca8ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch_output_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_search_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5bf287bb1009>\u001b[0m in \u001b[0;36mget_all_search_outputs\u001b[0;34m(search_types, flatten_output)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msearch_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_individual_search_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a7116aa3c219>\u001b[0m in \u001b[0;36mget_individual_search_output\u001b[0;34m(search_type, flatten_output)\u001b[0m\n\u001b[1;32m     28\u001b[0m         }\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_conduct_search_over_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-c8f271599c8e>\u001b[0m in \u001b[0;36m_conduct_search_over_pages\u001b[0;34m(search_url, search_params, flatten_output)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# 200: OK, 404: page not found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{search_type} search error {response.status_code} on page {search_params[\"page\"]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0msearch_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'page'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Conduct a search, extract json results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search_type' is not defined"
     ]
    }
   ],
   "source": [
    "search_output_dict = get_all_search_outputs(search_types, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a5b07",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4dea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the file/files listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths : str/list-like\n",
    "        string or list of strings containing the paths for the objects\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_dict : dict\n",
    "        Dictionary of DataFrames containing metadata for the requested datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "    \n",
    "    metadata_types = ('methods', 'repositories', 'results', 'tasks')\n",
    "    \n",
    "    start_page = 1\n",
    "    metadata_dict = dict()\n",
    "    \n",
    "    # Searches for each of the metadata types that are present for the search type we conducted\n",
    "    for metadata_type in metadata_types:\n",
    "        search_df = pd.DataFrame()\n",
    "        print(f'Querying {metadata_type}')\n",
    "        \n",
    "        # Searches over each object\n",
    "        for object_path in tqdm(object_paths):\n",
    "            search_url = f'{BASE_URL}/papers/{object_path}/{metadata_type}'\n",
    "            search_params = {'page': start_page}\n",
    "\n",
    "            # Conduct the search & add supplementary material to the DataFrame\n",
    "            object_df = _conduct_search_over_pages(search_url, search_params, flatten_output)\n",
    "            object_df['id'] = object_path\n",
    "            object_df['page'] = search_params['page']\n",
    "            \n",
    "            # Merge with the cumulative search DataFrame\n",
    "            search_df = pd.concat([search_df, object_df]).reset_index(drop=True)\n",
    "            \n",
    "        metadata_dict[(metadata_type, )] = search_df\n",
    "\n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43441df7",
   "metadata": {},
   "source": [
    "### Retrieve Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eccc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves all of the metadata that relates to the provided DataFrames\n",
    "    \n",
    "    Params:\n",
    "    - search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output  \n",
    "      \n",
    "    Returns:\n",
    "    - metadata_dict : collections.OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query\n",
    "        Order matches the order of search_output_dict\n",
    "    \"\"\"\n",
    "    metadata_dict = OrderedDict()\n",
    "    for query, df in search_output_dict.items():\n",
    "        print(f'Retrieving {query} metadata')\n",
    "        # Create object paths\n",
    "        object_paths = df.id.values\n",
    "\n",
    "        metadata_dict[query] = get_query_metadata(object_paths, flatten_output)\n",
    "    \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc8600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_dict = get_all_metadata(search_output_dict, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e71239",
   "metadata": {},
   "source": [
    "### Take a look at the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ebcdf",
   "metadata": {},
   "source": [
    "Since we stored the metadata and DataFrames in our dictionaries via tuple keys, we index the metadata_dict as \n",
    "\n",
    "```metadata_dict[('SEARCH_TYPE',)][('METADATA_TYPE', )]```\n",
    "\n",
    "Note that the tuple keys each have a comma after the sole value in order to preserve the tuple structure and relate in form to the other notebooks used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which metadata options we have access to\n",
    "for key, dict_ in metadata_dict.items():\n",
    "    print(f'{key[0]}: {[item[0] for item in dict_.keys()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe2150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_dict[('papers',)][('results',)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
