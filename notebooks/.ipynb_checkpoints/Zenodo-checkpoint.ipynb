{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zenodo API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the Zenodo API. Follow these steps in order to get the necessary credentials to continue:\n",
    "1. Create a Zenodo account at https://zenodo.org/signup/\n",
    "2. After logging in, click on the user dropdown menu in the top right corner, and click on 'Applications'\n",
    "3. Access API key either by:\n",
    "    - Create a Developer Application by clicking on 'New application'\n",
    "    - Create a Personal Access Token by clicking on 'New Token'\n",
    "4. Load API key:\n",
    "    - For repeated use, follow the ```pickle_tutorial.ipynb``` instructions to create create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'ZENODO_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "    - For sparser use, users can run the credentials cell and paste their API key when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Zenodo API ([Zenodo](https://developers.zenodo.org))\n",
    "- Zenodo Search Guide ([Guide](https://help.zenodo.org/guides/search/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Zenodo_workflow.jpg\" width=500 height=500 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "try:\n",
    "    with open('credentials.pkl', 'rb') as credentials:\n",
    "        ZENODO_TOKEN = pickle.load(credentials)['ZENODO_TOKEN']\n",
    "except:\n",
    "    ZENODO_TOKEN = input('Please enter your Zenodo API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_URL = 'https://zenodo.org/api/records'\n",
    "HEADERS = {'Authorization': f'Bearer {ZENODO_TOKEN}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query #1: query API query based on search terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_all_search_outputs` queries the Zenodo API for all search terms specified\n",
    "- Calls function `get_individual_search_output` for each search term\n",
    "- To account for Zenodo search limits, queries API for search term in one-year increments\n",
    "- Appends each resulting dataframe to main dataframe\n",
    "- Flattens highly nested JSON output if specified in argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the Figshare API for each search term and search type. \n",
    "    Results are retured in results[(search_term)] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms (list-like): collection of search terms to query over\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - results (dict): dictionary consisting of returned DataFrames from get_individual_search_output for each query\n",
    "    \"\"\"\n",
    "\n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term in search_terms:\n",
    "        results[(search_term,)] = get_individual_search_output(search_term, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_individual_search_output` queries the Zenodo API with the specified search term (e.g., “machine learning”)\n",
    "- Searches across all returned pages\n",
    "- Result is a dataframe\n",
    "    - Each dataframe contains *full metadata* about each object as well as high level summary statistics of search (i.e., number of hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Calls the Zenodo API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term (str): keyword to seach for\n",
    "    - flatten (bool): optional (default=False)\n",
    "   \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure out input is valid\n",
    "    assert isinstance(search_term, str), 'Search term must be a string'\n",
    "    \n",
    "    # Set search variables\n",
    "    start_page = 1\n",
    "    page_size = 1000 # Max = 10,000, Default = 10\n",
    "    search_year = 2021\n",
    "    search_df = pd.DataFrame()\n",
    "    start_date = f'{search_year}-01-01'\n",
    "    end_date = f'{search_year}-12-31'\n",
    "    \n",
    "    search_params = {\n",
    "        'q': f'{search_term} AND created:[{start_date} TO {end_date}]',\n",
    "        'page': start_page,\n",
    "        'size': page_size,\n",
    "        }\n",
    "    \n",
    "    # Run initial search & extract output\n",
    "    response = requests.get(SEARCH_URL, #Records — search published records\n",
    "                        params = search_params)\n",
    "    output = response.json()\n",
    "    \n",
    "    # Gather high-level search information from the 'aggregations' entry\n",
    "    search_aggregation_info = output['aggregations']\n",
    "    \n",
    "    # Loop over search years - searches until the current search year does not return any results\n",
    "    while output.get('hits').get('total'):\n",
    "        # Loop over pages - searches until the current page is empty \n",
    "        while response.status_code == 200 and output.get('hits').get('hits'):\n",
    "            # Flatten output\n",
    "            if flatten_output:\n",
    "                output_list = [flatten(result) for result in output['hits']['hits']]\n",
    "            else:\n",
    "                output_list = output['hits']['hits']\n",
    "            \n",
    "            # Turn outputs into DataFrame & add page info\n",
    "            output_df = pd.DataFrame(output_list)\n",
    "            output_df['page'] = search_params['page']\n",
    "            \n",
    "            # Append modified output df to our cumulative search DataFrame\n",
    "            search_df = pd.concat([search_df, output_df]).reset_index(drop=True)\n",
    "\n",
    "            # Increment page\n",
    "            search_params['page'] += 1 \n",
    "            \n",
    "             # Run search & extract output\n",
    "            response = requests.get(SEARCH_URL, #Records — search published records\n",
    "                                params = search_params)\n",
    "            output = response.json()\n",
    "            \n",
    "        # Change search year, reset search page\n",
    "        search_year -= 1\n",
    "        start_date = f'{search_year}-01-01'\n",
    "        end_date = f'{search_year}-12-31'\n",
    "\n",
    "        search_params['q'] = f'{search_term} AND created:[{start_date} TO {end_date}]'\n",
    "        search_params['page'] = start_page\n",
    "\n",
    "        # Run search & extract output\n",
    "        response = requests.get(SEARCH_URL, #Records — search published records\n",
    "                            params = search_params)\n",
    "        output = response.json()\n",
    "        \n",
    "    return search_aggregation_info, search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['\\\"machine learning\\\" OR \\\"artificial intelligence\\\"', \n",
    "                '\\\"machine learning\\\"', \n",
    "                '\\\"artificial intelligence\\\"',\n",
    "                '\\\"deep learning\\\"',\n",
    "                '\\\"neural network\\\"',\n",
    "                '\\\"supervised learning\\\"',\n",
    "                '\\\"unsupervised learning\\\"',\n",
    "                '\\\"reinforcement learning\\\"',\n",
    "                '\\\"training data\\\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, flatten_output=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
