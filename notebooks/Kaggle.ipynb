{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the Kaggle API. Follow these steps in order to get the necessary credentials to continue:\n",
    "\n",
    "1. Sign up for a Kaggle account at https://www.kaggle.com.\n",
    "2. Go to the 'Account' tab of your user profile - ```https://www.kaggle.com/{username}/account```.\n",
    "3. Select 'Create New API Token' under 'API' section.\n",
    "    - This will trigger the download of kaggle.json, a file containing your API credentials. \n",
    "4. Place this file in the location:\n",
    "    - ~/.kaggle/kaggle.json (for macOS/unix)\n",
    "    - C:/Users/username/.kaggle/kaggle.json (for Windows) \n",
    "    - You can check the exact location, sans drive, with echo %HOMEPATH%). \n",
    "    - You can define a shell environment variable KAGGLE_CONFIG_DIR to change this location to:\n",
    "        - $KAGGLE_CONFIG_DIR/kaggle.json (for macOS/unix)\n",
    "        - %KAGGLE_CONFIG_DIR%\\kaggle.json (for Windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this notebook uses functions written in Python to query the Kaggle API. **This code will only work for python 3.7 or later**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Kaggle API ([Kaggle](https://www.kaggle.com/docs/api))\n",
    "- Kaggle API ([GitHub](https://github.com/Kaggle/kaggle-api)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Kaggle_workflow.jpg\" width=650 height=650 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allow system to search parent folder for local imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Kaggle-specific imports\n",
    "from kaggle import KaggleApi\n",
    "from kaggle.rest import ApiException\n",
    "\n",
    "import pandas as pd # For storing/manipulating command data\n",
    "import json # Reading back the metadata files\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "import os # Exporting saved results\n",
    "from collections import OrderedDict\n",
    "from utils import flatten_nested_df\n",
    "from flatten_json import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query #1: query API based on search terms and search types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_all_search_outputs` queries the Kaggle API for all combinations of search terms and search types specified and returns the results as a dictionary of dataframes (one dataframe for each query combination)\n",
    "- Calls function `get_individual_search_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, search_types, flatten_output=False):\n",
    "    \"\"\"Call the Kaggle API for each search term and search type. \n",
    "    Results are retured in results['{term}_{type}'] = df.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_terms : list-like\n",
    "        Collection of search terms to query over.\n",
    "    search_types : list-like\n",
    "        Collection of search types to query over.\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for flattening nested columns of output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : OrderedDict\n",
    "        Dictionary consisting of returned DataFrames from get_search_output for each query.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = OrderedDict()\n",
    "    \n",
    "    for search_term, search_type in itertools.product(search_terms, search_types):\n",
    "        results[(search_term, search_type)] = get_individual_search_output(search_term, search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_individual_search_output` queries the Kaggle API with the specified search term (e.g., “machine learning”) and search type (must be either “datasets” or “kernels”)\n",
    "- Searches across all returned pages\n",
    "- Calls function `_convert_string_csv_output_to_dataframe` which converts results from API (strings in semi-structured table) to dataframe format\n",
    "- Result is a dataframe (one dataframe per search term/search type combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, search_type, flatten_output=False):\n",
    "    \"\"\"Calls the Kaggle API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_term : str\n",
    "        Keyword to seach for.\n",
    "    search_type : str \n",
    "        Objects to search over (must be either datasets or kernels).\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for flattening nested columns of output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        DataFrame containing the output of the search query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert isinstance(search_type, str), 'Search term must be a string'\n",
    "    assert search_type in ('datasets', 'kernels'), 'Search can only be conducted over datasets or kernels'\n",
    "    \n",
    "    # Use search type to get relevant API function\n",
    "    list_queries = getattr(api, f'{search_type}_list')\n",
    "    \n",
    "    page_idx = 1\n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    # Pulls the records for a single page of results for the given search term\n",
    "    output = list_queries(search=search_term, page=page_idx)\n",
    "\n",
    "    # Continue searching until we no longer receive results\n",
    "    while output:\n",
    "        if search_type == 'kernels':\n",
    "            output = [vars(result) for result in output]\n",
    "        if flatten_output:\n",
    "            output = [flatten(result) for result in output]\n",
    "            \n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['page'] = page_idx\n",
    "\n",
    "        search_df = pd.concat([search_df, output_df]).reset_index(drop=True)\n",
    "        \n",
    "        # Increments the page count for searching\n",
    "        page_idx += 1\n",
    "\n",
    "        # Pulls the records for a single page of results for the given search term\n",
    "        output = list_queries(search = search_term, page=page_idx)\n",
    "        \n",
    "    # Modify columns for metadata merge\n",
    "    search_df = search_df.rename(columns={'id': 'datasetId', 'ref': 'id'})\n",
    "    if search_type == 'datasets':\n",
    "        \n",
    "        search_df = search_df.drop(columns=['viewCount', 'voteCount'])\n",
    "    search_df = search_df.convert_dtypes()\n",
    "\n",
    "    \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `_convert_string_csv_output_to_dataframe` converts raw results from API (strings in semi-structured table) to dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_string_csv_output_to_dataframe(output):\n",
    "    \"\"\"Given a string variable in csv format, returns a Pandas DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output : str\n",
    "        CSV-styled string to be converted.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        DataFrame consisting of data from 'output' string variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create DataFrame of results\n",
    "    output = StringIO(output)\n",
    "    df = pd.read_csv(output)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run query #1 functions - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['\"machine learning\"']\n",
    "search_types = ['datasets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, search_types, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query #2: query API for full metadata for hits from initial query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Unable to find a way to store metadata in memory as opposed to saving file, but this workaround appears to be functional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `_retrieve_object_json` uses the path for each object (each dataframe in result #1 ordered dictionary to query API for metadata associated with each object). Loads JSON results as dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_object_json(object_path, flatten_output=False):\n",
    "    \"\"\"Queries Kaggle for metadata json file & returns the json data as a dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    object_path : str \n",
    "        Path for the dataset.\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for flattening nested columns of output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metadata_dict : dict \n",
    "        Dictionary containing json metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download the metadata\n",
    "    try:\n",
    "        api.dataset_metadata(object_path, path='../data/')\n",
    "    # Error occurs when there is no metadata to return\n",
    "    except (TypeError, ApiException) as e:\n",
    "        if (isinstance(e, ApiException) and \n",
    "            e.status != 404 and\n",
    "            'bigquery' not in e.headers['Turbolinks-Location']):\n",
    "            raise e\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        # Access the metadata and load it in as a dictionary\n",
    "        with open('../data/dataset-metadata.json') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        if flatten_output:\n",
    "            json_data = flatten(json_data)\n",
    "\n",
    "        return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_query_metadata` extracts metadata associated with each object and formats as dataframe\n",
    "- Calls function `_retrieve_object_json`\n",
    "- For each object, appends JSON results from `_retrieve_object_json` to a dataframe (converting to dataframe format)\n",
    "- Output is single dataframe for each search query (matching each dataframe in result #1 ordered dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, flatten_output=False):\n",
    "    \"\"\"Retrieves the metadata for the file/files listed in object_paths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    object_paths : str/list-like\n",
    "        String or list of strings containing the paths for the objects.\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for flattening nested columns of output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metadata_df : DataFrame\n",
    "        DataFrame containing metadata for the requested datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "        \n",
    "    # Create DataFrame to store metadata in, using columns found in first query, and then add query info\n",
    "    metadata_df = pd.DataFrame()\n",
    "        \n",
    "    # Pulls metadata information for each dataset found above\n",
    "    for object_path in tqdm(object_paths[:50]):\n",
    "        # Download & load the metadata\n",
    "        json_data = _retrieve_object_json(object_path, flatten_output)\n",
    "\n",
    "        # Store the metadata into our DataFrame created above\n",
    "        metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    # Modify dtypes\n",
    "    metadata_df = metadata_df.convert_dtypes()\n",
    "        \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_all_metadata` uses a `for` loop to put dataframes into an ordered dictionary, matching result #1 ordered dictionary\n",
    "- Calls function `get_query_metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"Retrieves all of the metadata that relates to the provided DataFrames.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs.\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for flattening nested columns of output.\n",
    "      \n",
    "    Returns\n",
    "    -------\n",
    "    metadata_dict : OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query.\n",
    "        Order matches the order of search_output_dict.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Extract IDs from DataFrame, and returns as list of strings\n",
    "    metadata_dict = OrderedDict()\n",
    "\n",
    "    for query, df in search_output_dict.items():\n",
    "        # Kernels do not have metadata\n",
    "        if 'kernels' not in query:\n",
    "            print(f'Retrieving {query} metadata')\n",
    "            # Create object paths\n",
    "            _, search_type = query\n",
    "            object_paths = df.id.values\n",
    "\n",
    "            metadata_dict[query] = get_query_metadata(object_paths, flatten_output)\n",
    "        \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_dict = get_all_metadata(search_output_dict, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "metadata_dict[('\"machine learning\"', 'datasets')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine results of query #1 and query #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `merge_search_and_metadata_dicts` merges the output dictionaries from query #1 and query #2 to a single ordered dictionary and (optional) saves the results as a single csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"Merges together all of the search and metadata DataFrames by the given 'on' key.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_dict : dict\n",
    "        Dictionary of search output results.\n",
    "    metadata_dict : dict\n",
    "        Dictionary of metadata results.\n",
    "    on : str/list-like, optional (default=None)\n",
    "        Column name(s) to merge the two dicts on.\n",
    "    left_on : str/list-like, optional (default=None)\n",
    "        Column name(s) to merge the left dict on.\n",
    "    right_on : str/list-like, optional (default=None)\n",
    "        Column name(s) to merge the right dict on.\n",
    "    save : boolean/list-like, optional (default=False)\n",
    "        Specifies if the output DataFrames should be saved.\n",
    "        If True: saves to file of format 'data/kaggle/kaggle_{search_term}_{search_type}.csv'.\n",
    "        If list-like: saves to respective location in list of save locations.\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types)).\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    df_dict : OrderedDict \n",
    "        OrderedDict containing all of the merged search/metadata dicts.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "\n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for query_key, save_loc in zip(search_dict.keys(), save):\n",
    "        search_df = search_dict[query_key]\n",
    "        if query_key in metadata_dict:\n",
    "            # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "            metadata_df = metadata_dict[query_key]\n",
    "            df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "        else:\n",
    "            df_all = search_df\n",
    "        \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'kaggle')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure kaggle directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError('Save type must be bool or str')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run merge function\n",
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict[sample_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results of query #1\n",
    "output_df = search_output_dict[sample_key]\n",
    "\n",
    "#results of query #2\n",
    "metadata_df = metadata_dict[sample_key]\n",
    "\n",
    "#result of merging datasets into \"full\" dataframe\n",
    "full_df = df_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
