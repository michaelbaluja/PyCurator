{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figshare API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the Figshare API. Follow these steps in order to get the necessary credentials to continue:\n",
    "1. Create a Figshare account at https://figshare.com/account/register\n",
    "2. After logging in, click on your account photo in the top right corner, and then click on 'Applications'\n",
    "3. Access API key either by:\n",
    "    - Create an application by clicking on 'Create Application'\n",
    "    - Create an API key by clicking on 'Create Personal Token'\n",
    "4. Load API key:\n",
    "    - For repeated use, follow the ```pickle_tutorial.ipynb``` instructions to create create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'FIGSHARE_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "    - For sparser use, users can run the credentials cell and paste their API key when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Figshare API ([Figshare](https://docs.figshare.com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Figshare_workflow.jpg\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For querying data from API\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "from collections import OrderedDict # For ordering search and metadata results for combination\n",
    "from flatten_json import flatten # For flattening nested data\n",
    "import pickle # For loading credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API access tokens have been stored in credentials.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "\n",
    "# Check for credentials file\n",
    "try:\n",
    "    with open('credentials.pkl', 'rb') as credentials:\n",
    "        FIGSHARE_TOKEN = pickle.load(credentials)['FIGSHARE_TOKEN']\n",
    "except:\n",
    "    FIGSHARE_TOKEN = input('Please enter your Figshare API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search constants\n",
    "BASE_URL = 'https://api.figshare.com/v2'\n",
    "HEADERS = {'Authorization': f'token {FIGSHARE_TOKEN}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query #1: query API based on search terms and search types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_all_search_outputs` queries the Figshare API for all combinations of search terms and search types specified and returns the results as a dictionary of dataframes (one dataframe for each query combination)\n",
    "- Calls function `get_individual_search_output` for each combination of search term and search type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, search_types, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the Figshare API for each search term and search type. \n",
    "    Results are retured in results['{term}_{type}'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms (list-like): collection of search terms to query over\n",
    "    - search_types (list-like): collection of search types to query over\n",
    "    - flatten_output (bool): optional, (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - results (dict): dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    num_searches = len(search_terms) * len(search_types)\n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term, search_type in itertools.product(search_terms, search_types):\n",
    "        results[(search_term, search_type)] = get_individual_search_output(search_term, search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_individual_search_output` queries the Figshare API with the specified search term (e.g., “machine learning”) and search type (i.e., articles, collections, projects)\n",
    "- Figshare allows a variety of search types. For this script, we search: (1) articles, (2) collections, and (3) projects.\n",
    "    - \"Articles\" in this context include a variety of item types: 1 - Figure, 2 - Media, 3 - Dataset, 5 - Poster, 6 - Journal contribution, 7 - Presentation, 8 - Thesis, 9 - Software, 11 - Online resource, 12 - Preprint, 13 - Book, 14 - Conference contribution, 15 - Chapter, 16 - Peer review, 17 - Educational resource, 18 - Report, 19 - Standard, 20 - Composition, 21 - Funding, 22 - Physical object, 23 - Data management plan, 24 - Workflow, 25 - Monograph, 26 - Performance, 27 - Event, 28 - Service, 29 - Model\n",
    "- Searches across all returned pages\n",
    "- Result is a dataframe (one dataframe per search term/search type combination)\n",
    "- Each dataframe contains high level information about each object (i.e., id, title, doi, URL, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Calls the Figshare API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term (str): keyword to seach for\n",
    "    - search_type (str): objects to search over (must be either datasets or kernels)\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "   \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert isinstance(search_term, str), 'Search term must be a string'\n",
    "    assert search_type in ('articles', 'collections', 'projects'), \\\n",
    "        'Search can only be conducted over articles and collections'\n",
    "        \n",
    "    # Set search variables\n",
    "    start_page = 1\n",
    "    page_size = 1000 # Maximum page size (min = 10)\n",
    "    output = None\n",
    "    search_df = pd.DataFrame()\n",
    "    search_year = 1950\n",
    "    prev_date = None\n",
    "    search_date = f'{search_year}-01-01'\n",
    "    \n",
    "    search_params = {\n",
    "        'search_for': search_term,\n",
    "        'published_since': search_date,\n",
    "        'order_direction': 'asc',\n",
    "        'page': start_page, \n",
    "        'page_size': page_size,  \n",
    "        }\n",
    "        \n",
    "    search_url = f'{BASE_URL}/{search_type}'\n",
    "    \n",
    "    ## Run search for public articles\n",
    "    response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "\n",
    "    ## Put output into json format\n",
    "    output = response.json()\n",
    "\n",
    "    while response.status_code == 200:\n",
    "        while response.status_code == 200 and output:\n",
    "            # Flatten output if needed\n",
    "            if flatten_output:\n",
    "                output = [flatten(result) for result in output]\n",
    "\n",
    "            # Turn outputs into DataFrame & add page info\n",
    "            output_df = pd.DataFrame(output)\n",
    "            output_df['search_page'] = search_params['page']\n",
    "            output_df['publish_query'] = search_params['published_since']\n",
    "\n",
    "            # Append modified output df to our cumulative search DataFrame\n",
    "            search_df = pd.concat([search_df, output_df]).reset_index(drop=True)\n",
    "\n",
    "            # Increment page number to query\n",
    "            search_params['page'] += 1\n",
    "\n",
    "            ## Run search for public articles\n",
    "            response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "\n",
    "            ## Put output into json format\n",
    "            output = response.json()\n",
    "\n",
    "        if output_df.shape[0] < search_params['page_size']:\n",
    "            return search_df\n",
    "\n",
    "        # Get new date to search\n",
    "        search_date = search_df['published_date'].values[-1].split('T')[0]\n",
    "        search_params['published_since'] = search_date\n",
    "        search_params['page'] = start_page\n",
    "        \n",
    "        ## Run search for public articles\n",
    "        response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "\n",
    "        ## Put output into json format\n",
    "        output = response.json()\n",
    "        \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run query #1 functions - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['Tensorflow']\n",
    "search_types = ['articles', 'projects', 'collections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, search_types, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query #2: query API for full metadata for hits from query #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `_retrieve_object_json` uses the URL for each object (from dataframe in result #1 \"ordered_dict\") to query API for metadata associated with each object. \n",
    "- Returns flattened JSON object   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_object_json(object_url, flatten_output=False):\n",
    "    '''\n",
    "    Queries Figshare for object data (json file) & returns the json data as a dictionary\n",
    "    \n",
    "    Params:\n",
    "    - object_url (str): path for the dataset\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - object_data_dict (dict): dictionary containing json data\n",
    "    '''\n",
    "    \n",
    "    # Download the metadata\n",
    "    response = requests.get(object_url, headers=HEADERS)\n",
    "    json_data = response.json()\n",
    "    \n",
    "    # Flatten json\n",
    "    if flatten_output:\n",
    "        json_data = flatten(json_data)\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_query_metadata` extracts metadata associated with each object and formats as dataframe\n",
    "- Calls function `_retrieve_object_json` to get full metadata for each object\n",
    "- Output is single dataframe for each search query (matching each dataframe in result #1 dictionary output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the object/objects listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths (str/list-like): string or list of strings containing the paths for the objects\n",
    "    - flatten_output (bool): optional, (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df (pandas.DataFrame): DataFrame containing metadata for the requested objects\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "    \n",
    "    #create empty pandas dataframe to put results in\n",
    "    metadata_df = pd.DataFrame()\n",
    "\n",
    "    #for each path, get full object details\n",
    "    for object_path in tqdm(object_paths):\n",
    "        #URL syntax for object details is: https://api.figshare.com/v2/{search_type}/{object_id}        \n",
    "        json_data = _retrieve_object_json(object_path, flatten_output)\n",
    "        \n",
    "        #appending json collapses first level, which is a start\n",
    "        #for now, can leave files, custom fields, author, etc as list of dictionary\n",
    "        metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `get_all_metadata` uses a `for` loop to put dataframes into an ordered dictionary, matching result #1 \"ordered_dict\" object\n",
    "- Calls function `get_query_metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves all of the metadata that relates to the provided DataFrames\n",
    "    \n",
    "    Params:\n",
    "    - search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output  \n",
    "      \n",
    "    Returns:\n",
    "    - metadata_dict : collections.OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query\n",
    "        Order matches the order of search_output_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Extract IDs from DataFrame, and returns as list of strings\n",
    "    metadata_dict = OrderedDict()\n",
    "\n",
    "    for query, df in search_output_dict.items():\n",
    "        print(f'Retrieving {query} metadata')\n",
    "        # Create object paths\n",
    "        _, search_type = query\n",
    "        object_ids = df.id.convert_dtypes(convert_string=True).tolist()\n",
    "        object_paths = [f'{BASE_URL}/{search_type}/{object_id}' for object_id in object_ids]\n",
    "\n",
    "        metadata_dict[query] = get_query_metadata(object_paths, flatten_output)\n",
    "    \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('Tensorflow', 'articles') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [04:27<00:00,  1.49s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('Tensorflow', 'projects') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.12s/it]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('Tensorflow', 'collections') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:34<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "metadata_dict = get_all_metadata(search_output_dict, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine results of query #1 and query #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `merge_search_and_metadata_dicts` merges the output dictionaries from query #1 and query #2 to a single ordered dictionary and (optional) saves the results as a single csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict (dict): dictionary of search output results\n",
    "    - metadata_dict (dict): dictionary of metadata results\n",
    "    - on (str/list-like): column name(s) to merge the two dicts on\n",
    "    - left_on (str/list-like): column name(s) to merge the left dict on\n",
    "    - right_on (str/list-like): column name(s) to merge the right dict on\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/figshare/figshare_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    Returns:\n",
    "    - df_dict (OrderedDict): OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "        \n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'figshare')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure figshare directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError(f'Save type must be bool or str, not {type(save_loc)}')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run merge function - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results of query #1\n",
    "output_df = search_output_dict[sample_key]\n",
    "\n",
    "#results of query #2\n",
    "metadata_df = metadata_dict[sample_key]\n",
    "\n",
    "#result of merging datasets into \"full\" dataframe\n",
    "full_df = df_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>handle</th>\n",
       "      <th>url</th>\n",
       "      <th>published_date</th>\n",
       "      <th>thumb</th>\n",
       "      <th>defined_type</th>\n",
       "      <th>defined_type_name</th>\n",
       "      <th>group_id</th>\n",
       "      <th>...</th>\n",
       "      <th>files_99_supplied_md5</th>\n",
       "      <th>custom_fields_10_value_0</th>\n",
       "      <th>tags_21</th>\n",
       "      <th>tags_22</th>\n",
       "      <th>tags_23</th>\n",
       "      <th>funding_list_1_funder_name</th>\n",
       "      <th>funding_list_1_grant_code</th>\n",
       "      <th>funding_list_1_id</th>\n",
       "      <th>funding_list_1_is_user_defined</th>\n",
       "      <th>funding_list_1_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4555483</td>\n",
       "      <td>speech-to-text-wavenet VCTK training checkpoint</td>\n",
       "      <td>10.6084/m9.figshare.4555483.v1</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/articles/4555483</td>\n",
       "      <td>2017-01-15T16:15:03Z</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4667377</td>\n",
       "      <td>Free software file coauthorship graphs</td>\n",
       "      <td>10.6084/m9.figshare.4667377.v1</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/articles/4667377</td>\n",
       "      <td>2017-02-17T19:14:14Z</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5442451</td>\n",
       "      <td>Code and data for construction of a network ar...</td>\n",
       "      <td>10.6084/m9.figshare.5442451.v1</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/articles/5442451</td>\n",
       "      <td>2017-12-29T18:20:11Z</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>software</td>\n",
       "      <td>11383</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5826174</td>\n",
       "      <td>Inner and Outer Recursive Neural Networks for ...</td>\n",
       "      <td>10.1021/acs.jcim.7b00384.s001</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/articles/5826174</td>\n",
       "      <td>2018-01-10T00:00:00Z</td>\n",
       "      <td>https://s3-eu-west-1.amazonaws.com/ppreviews-a...</td>\n",
       "      <td>6</td>\n",
       "      <td>journal contribution</td>\n",
       "      <td>2409</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6060311</td>\n",
       "      <td>Deep Learning in Industry</td>\n",
       "      <td>10.4225/03/5abc50d08a0d4</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/articles/6060311</td>\n",
       "      <td>2018-03-29T02:34:55Z</td>\n",
       "      <td>https://s3-eu-west-1.amazonaws.com/ppreviews-m...</td>\n",
       "      <td>7</td>\n",
       "      <td>presentation</td>\n",
       "      <td>1467</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1579 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  4555483    speech-to-text-wavenet VCTK training checkpoint   \n",
       "1  4667377             Free software file coauthorship graphs   \n",
       "2  5442451  Code and data for construction of a network ar...   \n",
       "3  5826174  Inner and Outer Recursive Neural Networks for ...   \n",
       "4  6060311                          Deep Learning in Industry   \n",
       "\n",
       "                              doi handle  \\\n",
       "0  10.6084/m9.figshare.4555483.v1          \n",
       "1  10.6084/m9.figshare.4667377.v1          \n",
       "2  10.6084/m9.figshare.5442451.v1          \n",
       "3   10.1021/acs.jcim.7b00384.s001          \n",
       "4        10.4225/03/5abc50d08a0d4          \n",
       "\n",
       "                                            url        published_date  \\\n",
       "0  https://api.figshare.com/v2/articles/4555483  2017-01-15T16:15:03Z   \n",
       "1  https://api.figshare.com/v2/articles/4667377  2017-02-17T19:14:14Z   \n",
       "2  https://api.figshare.com/v2/articles/5442451  2017-12-29T18:20:11Z   \n",
       "3  https://api.figshare.com/v2/articles/5826174  2018-01-10T00:00:00Z   \n",
       "4  https://api.figshare.com/v2/articles/6060311  2018-03-29T02:34:55Z   \n",
       "\n",
       "                                               thumb  defined_type  \\\n",
       "0                                                                3   \n",
       "1                                                                3   \n",
       "2                                                                9   \n",
       "3  https://s3-eu-west-1.amazonaws.com/ppreviews-a...             6   \n",
       "4  https://s3-eu-west-1.amazonaws.com/ppreviews-m...             7   \n",
       "\n",
       "      defined_type_name group_id  ... files_99_supplied_md5  \\\n",
       "0               dataset      NaN  ...                   NaN   \n",
       "1               dataset      NaN  ...                   NaN   \n",
       "2              software    11383  ...                   NaN   \n",
       "3  journal contribution     2409  ...                   NaN   \n",
       "4          presentation     1467  ...                   NaN   \n",
       "\n",
       "  custom_fields_10_value_0 tags_21 tags_22 tags_23 funding_list_1_funder_name  \\\n",
       "0                      NaN     NaN     NaN     NaN                        NaN   \n",
       "1                      NaN     NaN     NaN     NaN                        NaN   \n",
       "2                      NaN     NaN     NaN     NaN                        NaN   \n",
       "3                      NaN     NaN     NaN     NaN                        NaN   \n",
       "4                      NaN     NaN     NaN     NaN                        NaN   \n",
       "\n",
       "  funding_list_1_grant_code funding_list_1_id funding_list_1_is_user_defined  \\\n",
       "0                       NaN               NaN                            NaN   \n",
       "1                       NaN               NaN                            NaN   \n",
       "2                       NaN               NaN                            NaN   \n",
       "3                       NaN               NaN                            NaN   \n",
       "4                       NaN               NaN                            NaN   \n",
       "\n",
       "  funding_list_1_title  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  NaN  \n",
       "\n",
       "[5 rows x 1579 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
