{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67be28d",
   "metadata": {},
   "source": [
    "# Dryad API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c0306",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7a89a",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0ba2d",
   "metadata": {},
   "source": [
    "This notebook utilizes the Data Dryad API. No API Token is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a482b82a",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663fd589",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Dryad API ([Dryad](https://datadryad.org/api/v2/docs/))\n",
    "- Dryad API ([GitHub](https://github.com/CDL-Dryad/dryad-app/tree/main/documentation/apis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1bd98",
   "metadata": {},
   "source": [
    "## Overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594334e0",
   "metadata": {},
   "source": [
    "<img src=\"../images/Dryad_workflow.jpg\" width=600 height=600 align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04804426",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For querying data from API\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "from collections import OrderedDict\n",
    "from flatten_json import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7044c7d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14547b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search constants\n",
    "BASE_URL = 'https://datadryad.org/api/v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b086205f",
   "metadata": {},
   "source": [
    "## Query #1: query API based on search terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ee3ef",
   "metadata": {},
   "source": [
    "Function `get_all_search_outputs` queries the Dryad API for all search terms specified and returns the results as a dictionary of dataframes (one dataframe for each query combination)\n",
    "- Calls function `get_individual_search_output` for each search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fe635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, flatten_output=False):\n",
    "    \"\"\"Call the Data Dryad API for each search term. \n",
    "    Results are retured in results['({term},)'] = df.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_terms : list-like \n",
    "        Collection of search terms to query over.\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for flattening nested columns of output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Dictionary consisting of returned DataFrames from get_search_output for each query.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term in search_terms:\n",
    "        results[(search_term,)] = get_individual_search_output(search_term, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52733523",
   "metadata": {},
   "source": [
    "Function `_conduct_search_over_pages` is a helper function used to iterate over search result pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c27e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conduct_search_over_pages(search_url, search_params, flatten_output=False, delim=None):\n",
    "    # Make sure proper delim passed in\n",
    "    if delim:\n",
    "        assert isinstance(delim, str), 'Incorrect delim parameter passed in. Must be of type str'\n",
    "    \n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    # Perform initial search & convert results to json\n",
    "    response = requests.get(search_url, params=search_params)\n",
    "    output = response.json()\n",
    "    \n",
    "    # Loops over the search as long as the page was not empty\n",
    "    while output.get('count'):\n",
    "        # Extract relevant output data\n",
    "        output = output['_embedded']\n",
    "        \n",
    "        if delim:\n",
    "            output = output[delim]\n",
    "        \n",
    "        # Flatten output if necessary\n",
    "        if flatten_output:\n",
    "            output = [flatten(result) for result in output]\n",
    "        else:\n",
    "            output = output\n",
    "        \n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['page'] = search_params['page']\n",
    "        \n",
    "        search_df = pd.concat([search_df, output_df]).reset_index(drop=True)\n",
    "        \n",
    "        # Increment the page number to perform another search\n",
    "        search_params['page'] += 1\n",
    "        \n",
    "        # Perform next search and convert results to json\n",
    "        response = requests.get(search_url, params=search_params)\n",
    "        output = response.json()\n",
    "\n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c327197",
   "metadata": {},
   "source": [
    "Function `get_individual_search_output` queries the Dryad API for the specified search term and returns the result as a dataframe\n",
    "- Calls function `_conduct_search_over_pages` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ddbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, flatten_output=False):\n",
    "    \"\"\"Returns a list of all datasets available from the Data Dryad API.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_term : str\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for flattening nested columns of output.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the output of the search query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set search params\n",
    "    search_url = f'{BASE_URL}/search'\n",
    "    start_page = 1\n",
    "    page_size = 100\n",
    "    \n",
    "    search_params = {\n",
    "        'q': search_term,\n",
    "        'page': start_page,\n",
    "        'per_page': page_size\n",
    "    }\n",
    "    \n",
    "    return _conduct_search_over_pages(search_url, search_params, flatten_output, delim='stash:datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf9a63",
   "metadata": {},
   "source": [
    "#### Run query #1 functions - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ed137",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['\\\"machine learning\\\"', '\\\"artificial intelligence\\\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89723ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0],)\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7fe4f",
   "metadata": {},
   "source": [
    "## Query #2: query API for full metadata for hits from query #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae2787",
   "metadata": {},
   "source": [
    "Function `get_query_metadata` extracts metadata associated with each object and formats as dataframe\n",
    "- Output is single dataframe for each search query (matching each dataframe in result #1 ordered dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8be645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, flatten_output=False):\n",
    "    \"\"\"Retrieves the metadata for the file/files listed in object_paths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    object_paths : str/list-like\n",
    "        String or list of strings containing the paths for the objects.\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for flattening nested columns of output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metadata_df : pandas.DataFrame\n",
    "        DataFrame containing metadata for the requested dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "    \n",
    "    # Set search variables\n",
    "    start_page = 1\n",
    "    metadata_df = pd.DataFrame()\n",
    "    \n",
    "    # Request the metadata for each object\n",
    "    for object_path in tqdm(object_paths):\n",
    "        # Set search variables\n",
    "        search_url = f'{BASE_URL}/versions/{object_path}/files'\n",
    "        search_params = {'page': start_page}\n",
    "        \n",
    "        # Conduct search\n",
    "        object_df = _conduct_search_over_pages(search_url, search_params, flatten_output, delim='stash:files')\n",
    "\n",
    "        # Add relevant data to DataFrame and merge\n",
    "        object_df['id'] = object_path\n",
    "        object_df['page'] = search_params['page']\n",
    "        metadata_df = pd.concat([metadata_df, object_df]).reset_index(drop=True)\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4949742",
   "metadata": {},
   "source": [
    "Function `get_all_metadata` uses a `for` loop to put dataframes into an ordered dictionary, matching result #1 ordered_dictionary\n",
    "- Calls function `get_query_metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"Retrieves all of the metadata that relates to the provided DataFrames.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs.\n",
    "    flatten_output : boolean, optional (default=False)\n",
    "        Flag for flattening nested columns of output.\n",
    "      \n",
    "    Returns\n",
    "    -------\n",
    "    metadata_dict : collections.OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query.\n",
    "        Order matches the order of search_output_dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Extract IDs from DataFrame, and returns as list of strings\n",
    "    metadata_dict = OrderedDict()\n",
    "\n",
    "    for query, df in search_output_dict.items():\n",
    "        search_term = query[0]\n",
    "        print(f'Retrieving {search_term} metadata')\n",
    "        \n",
    "        # Create object paths\n",
    "        object_paths = df.id.convert_dtypes(convert_string=True).tolist()\n",
    "\n",
    "        metadata_dict[query] = get_query_metadata(object_paths, flatten_output)\n",
    "    \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4777e",
   "metadata": {},
   "source": [
    "#### Run query #2 functions - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e41f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = get_all_metadata(search_output_dict, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7eae6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metadata_dict[sample_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe24d1",
   "metadata": {},
   "source": [
    "## Combine results of query #1 and query #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c74a48",
   "metadata": {},
   "source": [
    "Function `merge_search_and_metadata_dicts` merges the output dictionaries from query #1 and query #2 to a single ordered dictionary and (optional) saves the results as a single csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98addfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"Merges together all of the search and metadata DataFrames by the given 'on' key.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    search_dict : dict\n",
    "        Dictionary of search output results.\n",
    "    metadata_dict : dict\n",
    "        Dictionary of metadata results.\n",
    "    on : str/list-like\n",
    "        Column name(s) to merge the two dicts on.\n",
    "    left_on : str/list-like\n",
    "        Column name(s) to merge the left dict on.\n",
    "    right_on : str/list-like\n",
    "        Column name(s) to merge the right dict on.\n",
    "    save : boolean, optional (default=False)\n",
    "        Specifies if the output DataFrames should be saved.\n",
    "        If True: saves to file of format 'data/openml/openml_{search_term}_{search_type}.csv'.\n",
    "        If list-like: saves to respective location in list of save locations.\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types)).\n",
    "            \n",
    "    If the on/left_on/right_on values are not explicitely specified, behavior defaults to what is done\n",
    "    in the pandas documentation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_dict : OrderedDict\n",
    "        OrderedDict containing all of the merged search/metadata dicts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "        \n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'dryad')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError(f'Save type must be bool or str, not {type(save_loc)}')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f50687",
   "metadata": {},
   "source": [
    "#### Run merge function - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0285fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35908bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict[sample_key]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
