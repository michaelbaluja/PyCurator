{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a907556",
   "metadata": {},
   "source": [
    "# Dryad API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b03c2d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417dcf3",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643d5f0",
   "metadata": {},
   "source": [
    "This notebook utilizes the Data Dryad API. No API Token is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca5bdb",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770563a2",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Dryad API ([Dryad](https://datadryad.org/api/v2/docs/))\n",
    "- Dryad API ([GitHub](https://github.com/CDL-Dryad/dryad-app/tree/main/documentation/apis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8997601",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a86c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For querying data from API\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "from collections import OrderedDict\n",
    "from flatten_json import flatten\n",
    "\n",
    "# For loading credentials\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1fead",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0967e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a2bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search constants\n",
    "BASE_URL = 'https://datadryad.org/api/v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2bde6c",
   "metadata": {},
   "source": [
    "## Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf0b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the Data Dryad API for each search term. \n",
    "    Results are retured in results['({term},)'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms : list-like \n",
    "        collection of search terms to query over\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - results : dict\n",
    "        dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term in search_terms:\n",
    "        results[(search_term,)] = get_individual_search_output(search_term, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2465cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conduct_search_over_pages(search_url, search_params, flatten_output=False, delim=None):\n",
    "    # Make sure proper delim passed in\n",
    "    if delim:\n",
    "        assert isinstance(delim, str), 'Incorrect delim parameter passed in. Must be of type str'\n",
    "    \n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    # Perform initial search & convert results to json\n",
    "    response = requests.get(search_url, params=search_params)\n",
    "    output = response.json()\n",
    "    \n",
    "    # Loops over the search as long as the page was not empty\n",
    "    while output.get('count'):\n",
    "        # Extract relevant output data\n",
    "        output = output['_embedded']\n",
    "        \n",
    "        if delim:\n",
    "            output = output[delim]\n",
    "        \n",
    "        # Flatten output if necessary\n",
    "        if flatten_output:\n",
    "            output = [flatten(result) for result in output]\n",
    "        else:\n",
    "            output = output\n",
    "        \n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['page'] = search_params['page']\n",
    "        \n",
    "        search_df = pd.concat([search_df, output_df]).reset_index(drop=True)\n",
    "        \n",
    "        # Increment the page number to perform another search\n",
    "        search_params['page'] += 1\n",
    "        \n",
    "        # Perform next search and convert results to json\n",
    "        response = requests.get(search_url, params=search_params)\n",
    "        output = response.json()\n",
    "\n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5a2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Returns a list of all datasets available from the Data Dryad API\n",
    "    \n",
    "    Params:\n",
    "    - search_term : str\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "        \n",
    "    Returns:\n",
    "    - pandas.DataFrame\n",
    "        DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set search params\n",
    "    search_url = f'{BASE_URL}/search'\n",
    "    start_page = 1\n",
    "    page_size = 100\n",
    "    \n",
    "    search_params = {\n",
    "        'q': search_term,\n",
    "        'page': start_page,\n",
    "        'per_page': page_size\n",
    "    }\n",
    "    \n",
    "    return _conduct_search_over_pages(search_url, search_params, flatten_output, delim='stash:datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f90e0",
   "metadata": {},
   "source": [
    "### Run initial API query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbb7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['\\\"machine learning\\\" OR \\\"artificial intelligence\\\"', \n",
    "                '\\\"machine learning\\\"', \n",
    "                '\\\"artificial intelligence\\\"',\n",
    "                '\\\"deep learning\\\"',\n",
    "                '\\\"neural network\\\"',\n",
    "                '\\\"supervised learning\\\"',\n",
    "                '\\\"unsupervised learning\\\"',\n",
    "                '\\\"reinforcement learning\\\"',\n",
    "                '\\\"training data\\\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c1cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b8636",
   "metadata": {},
   "source": [
    "#### Take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b480fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "search_output_dict[('machine learning',)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf24498",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the file/files listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths : str/list-like\n",
    "        string or list of strings containing the paths for the objects\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df : pandas.DataFrame\n",
    "        DataFrame containing metadata for the requested dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "    \n",
    "    # Set search variables\n",
    "    start_page = 1\n",
    "    metadata_df = pd.DataFrame()\n",
    "    \n",
    "    # Request the metadata for each object\n",
    "    for object_path in tqdm(object_paths):\n",
    "        # Set search variables\n",
    "        search_url = f'{BASE_URL}/versions/{object_path}/files'\n",
    "        search_params = {'page': start_page}\n",
    "        \n",
    "        # Conduct search\n",
    "        object_df = _conduct_search_over_pages(search_url, search_params, flatten_output, delim='stash:files')\n",
    "\n",
    "        # Add relevant data to DataFrame and merge\n",
    "        object_df['id'] = object_path\n",
    "        object_df['page'] = search_params['page']\n",
    "        metadata_df = pd.concat([metadata_df, object_df]).reset_index(drop=True)\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves all of the metadata that relates to the provided DataFrames\n",
    "    \n",
    "    Params:\n",
    "    - search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output  \n",
    "      \n",
    "    Returns:\n",
    "    - metadata_dict : collections.OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query\n",
    "        Order matches the order of search_output_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Extract IDs from DataFrame, and returns as list of strings\n",
    "    metadata_dict = OrderedDict()\n",
    "\n",
    "    for query, df in search_output_dict.items():\n",
    "        search_term = query[0]\n",
    "        print(f'Retrieving {search_term} metadata')\n",
    "        \n",
    "        # Create object paths\n",
    "        object_paths = df.id.convert_dtypes(convert_string=True).tolist()\n",
    "\n",
    "        metadata_dict[query] = get_query_metadata(object_paths, flatten_output)\n",
    "    \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cabfc",
   "metadata": {},
   "source": [
    "### Run metadata API query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = get_all_metadata(search_output_dict, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636b593c",
   "metadata": {},
   "source": [
    "#### Take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe88fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metadata_dict[('machine learning',)].to_csv('dryad__metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257cfb2b",
   "metadata": {},
   "source": [
    "## Merge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06550867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict (dict): dictionary of search output results\n",
    "    - metadata_dict (dict): dictionary of metadata results\n",
    "    - on (str/list-like): column name(s) to merge the two dicts on\n",
    "    - left_on (str/list-like): column name(s) to merge the left dict on\n",
    "    - right_on (str/list-like): column name(s) to merge the right dict on\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/kaggle/kaggle_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    Returns:\n",
    "    - df_dict (OrderedDict): OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "        \n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'dryad')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure figshare directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError(f'Save type must be bool or str, not {type(save_loc)}')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db6c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict[('machine learning',)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
