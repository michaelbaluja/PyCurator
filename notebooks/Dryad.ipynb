{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a583ab",
   "metadata": {},
   "source": [
    "# Dryad API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b87dc",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd55b7d",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc29ba",
   "metadata": {},
   "source": [
    "This notebook utilizes the Data Dryad API. No API Token is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105c185",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c98469",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Dryad API ([Dryad](https://datadryad.org/api/v2/docs/))\n",
    "- Dryad API ([GitHub](https://github.com/CDL-Dryad/dryad-app/tree/main/documentation/apis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b1101",
   "metadata": {},
   "source": [
    "## Overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d4f30",
   "metadata": {},
   "source": [
    "<img src=\"../images/Dryad_workflow.jpg\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3cb45",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02202107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For querying data from API\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "from collections import OrderedDict\n",
    "from flatten_json import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84859571",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec482f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search constants\n",
    "BASE_URL = 'https://datadryad.org/api/v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c669244",
   "metadata": {},
   "source": [
    "## Query #1: query API based on search terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d52582",
   "metadata": {},
   "source": [
    "Function `get_all_search_outputs` queries the Dryad API for all search term specified and returns the results as a dictionary of dataframes (one dataframe for each query combination)\n",
    "- Calls function `get_individual_search_output` for each search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc7db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the Data Dryad API for each search term. \n",
    "    Results are retured in results['({term},)'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms : list-like \n",
    "        collection of search terms to query over\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - results : dict\n",
    "        dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term in search_terms:\n",
    "        results[(search_term,)] = get_individual_search_output(search_term, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1ec06",
   "metadata": {},
   "source": [
    "Function `_conduct_search_over_pages` is a helper function used to iterate over search result pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36158536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conduct_search_over_pages(search_url, search_params, flatten_output=False, delim=None):\n",
    "    # Make sure proper delim passed in\n",
    "    if delim:\n",
    "        assert isinstance(delim, str), 'Incorrect delim parameter passed in. Must be of type str'\n",
    "    \n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    # Perform initial search & convert results to json\n",
    "    response = requests.get(search_url, params=search_params)\n",
    "    output = response.json()\n",
    "    \n",
    "    # Loops over the search as long as the page was not empty\n",
    "    while output.get('count'):\n",
    "        # Extract relevant output data\n",
    "        output = output['_embedded']\n",
    "        \n",
    "        if delim:\n",
    "            output = output[delim]\n",
    "        \n",
    "        # Flatten output if necessary\n",
    "        if flatten_output:\n",
    "            output = [flatten(result) for result in output]\n",
    "        else:\n",
    "            output = output\n",
    "        \n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['page'] = search_params['page']\n",
    "        \n",
    "        search_df = pd.concat([search_df, output_df]).reset_index(drop=True)\n",
    "        \n",
    "        # Increment the page number to perform another search\n",
    "        search_params['page'] += 1\n",
    "        \n",
    "        # Perform next search and convert results to json\n",
    "        response = requests.get(search_url, params=search_params)\n",
    "        output = response.json()\n",
    "\n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32feb7",
   "metadata": {},
   "source": [
    "Function `get_individual_search_output` queries the Dryad API for the specified search term and returns the result as a dataframe\n",
    "- Calls function `_conduct_search_over_pages` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a9f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Returns a list of all datasets available from the Data Dryad API\n",
    "    \n",
    "    Params:\n",
    "    - search_term : str\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "        \n",
    "    Returns:\n",
    "    - pandas.DataFrame\n",
    "        DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set search params\n",
    "    search_url = f'{BASE_URL}/search'\n",
    "    start_page = 1\n",
    "    page_size = 100\n",
    "    \n",
    "    search_params = {\n",
    "        'q': search_term,\n",
    "        'page': start_page,\n",
    "        'per_page': page_size\n",
    "    }\n",
    "    \n",
    "    return _conduct_search_over_pages(search_url, search_params, flatten_output, delim='stash:datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5217cca",
   "metadata": {},
   "source": [
    "#### Run query #1 functions - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee521dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['\\\"machine learning\\\" OR \\\"artificial intelligence\\\"', \n",
    "                '\\\"machine learning\\\"', \n",
    "                '\\\"artificial intelligence\\\"',\n",
    "                '\\\"deep learning\\\"',\n",
    "                '\\\"neural network\\\"',\n",
    "                '\\\"supervised learning\\\"',\n",
    "                '\\\"unsupervised learning\\\"',\n",
    "                '\\\"reinforcement learning\\\"',\n",
    "                '\\\"training data\\\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c80ed786",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056fff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "search_output_dict[('machine learning',)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217503cd",
   "metadata": {},
   "source": [
    "## Query #2: query API for full metadata for hits from query #1## Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffacb039",
   "metadata": {},
   "source": [
    "Function `get_query_metadata` extracts metadata associated with each object and formats as dataframe\n",
    "- Output is single dataframe for each search query (matching each dataframe in result #1 \"ordered_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ee787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the file/files listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths : str/list-like\n",
    "        string or list of strings containing the paths for the objects\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df : pandas.DataFrame\n",
    "        DataFrame containing metadata for the requested dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "    \n",
    "    # Set search variables\n",
    "    start_page = 1\n",
    "    metadata_df = pd.DataFrame()\n",
    "    \n",
    "    # Request the metadata for each object\n",
    "    for object_path in tqdm(object_paths):\n",
    "        # Set search variables\n",
    "        search_url = f'{BASE_URL}/versions/{object_path}/files'\n",
    "        search_params = {'page': start_page}\n",
    "        \n",
    "        # Conduct search\n",
    "        object_df = _conduct_search_over_pages(search_url, search_params, flatten_output, delim='stash:files')\n",
    "\n",
    "        # Add relevant data to DataFrame and merge\n",
    "        object_df['id'] = object_path\n",
    "        object_df['page'] = search_params['page']\n",
    "        metadata_df = pd.concat([metadata_df, object_df]).reset_index(drop=True)\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db7419",
   "metadata": {},
   "source": [
    "Function `get_all_metadata` uses a `for` loop to put dataframes into an ordered dictionary, matching result #1 \"ordered_dict\" object\n",
    "- Calls function `get_query_metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc8d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves all of the metadata that relates to the provided DataFrames\n",
    "    \n",
    "    Params:\n",
    "    - search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output  \n",
    "      \n",
    "    Returns:\n",
    "    - metadata_dict : collections.OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query\n",
    "        Order matches the order of search_output_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Extract IDs from DataFrame, and returns as list of strings\n",
    "    metadata_dict = OrderedDict()\n",
    "\n",
    "    for query, df in search_output_dict.items():\n",
    "        search_term = query[0]\n",
    "        print(f'Retrieving {search_term} metadata')\n",
    "        \n",
    "        # Create object paths\n",
    "        object_paths = df.id.convert_dtypes(convert_string=True).tolist()\n",
    "\n",
    "        metadata_dict[query] = get_query_metadata(object_paths, flatten_output)\n",
    "    \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21a719",
   "metadata": {},
   "source": [
    "#### Run query #2 functions - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = get_all_metadata(search_output_dict, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43839b8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metadata_dict[('machine learning',)].to_csv('dryad__metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c01ce1",
   "metadata": {},
   "source": [
    "## Combine results of query #1 and query #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22557be",
   "metadata": {},
   "source": [
    "Function `merge_search_and_metadata_dicts` merges the output dictionaries from query #1 and query #2 to a single ordered dictionary and (optional) saves the results as a single csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b154d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict (dict): dictionary of search output results\n",
    "    - metadata_dict (dict): dictionary of metadata results\n",
    "    - on (str/list-like): column name(s) to merge the two dicts on\n",
    "    - left_on (str/list-like): column name(s) to merge the left dict on\n",
    "    - right_on (str/list-like): column name(s) to merge the right dict on\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/dryad/dryad_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    Returns:\n",
    "    - df_dict (OrderedDict): OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "        \n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'dryad')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure figshare directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError(f'Save type must be bool or str, not {type(save_loc)}')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b95e46",
   "metadata": {},
   "source": [
    "#### Run merge function - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80586aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict[('machine learning',)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
