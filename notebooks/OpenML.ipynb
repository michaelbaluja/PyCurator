{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc0da43",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684a927",
   "metadata": {},
   "source": [
    "NOTE: A few of the changes made (global variables being referenced from inside functions) were done in order to ease the transition to object oriented design without having to change any of the function structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe5f55",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66512cf2",
   "metadata": {},
   "source": [
    "This notebook utilizes the OpenML API. Follow these steps in order to get the necessary credentials to continue (additional information is available at the OpenML documentation under \"Additional Information\" below):\n",
    "\n",
    "1. Create an OpenML account at https://www.openml.org/register\n",
    "2. After logging in, open your account page (click the avatar on the top right)\n",
    "3. Open 'Account Settings', then 'API authentication' to find your API key\n",
    "\n",
    "There are multiple ways of authenticating. Any of the following will work for this notebook:\n",
    "\n",
    "Temporarily:\n",
    "- When prompted below (if none of the following methods are completed), enter your API key in the text box.\n",
    "    - This method is the easiest, but must be repeated every time the notebook is loaded.\n",
    "\n",
    "Permanently:\n",
    "- Following the pickle_tutorial.ipynb instructions, create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'OPENML_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "- Use the openml CLI tool with ```openml configure apikey MYKEY```, with MYKEY being your API key.\n",
    "- Create a plain text file ```~/.openml/config``` that contains the line ```apikey=MYKEY```, with MYKEY being your API key. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90092b9",
   "metadata": {},
   "source": [
    "Issues:\n",
    "- When importing arff exceptions, they may not be found. If this is the case, uninstall arff and install liac-arff\n",
    "- Datasets and Tasks are slow to iterate over after ~100-120 queries. Shouldn't have anything to do with setup since the loop over query id's is the same as the API code w/ added error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d27626",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be0c60",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- OpenML API ([OpenML](https://docs.openml.org/Python-start/))\n",
    "- OpenML API ([GitHub](https://github.com/openml/openml-python)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091269a",
   "metadata": {},
   "source": [
    "## Overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50a7cf",
   "metadata": {},
   "source": [
    "<img src=\"../images/OpenML_workflow.jpg\" width=600 height=600 align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78bd94b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ac981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow system to search parent folder for local imports\n",
    "import sys\n",
    "sys.path.append('..')    \n",
    "    \n",
    "import pandas as pd # For storing/manipulating query data\n",
    "import pickle # For loading credentials\n",
    "import os # For loading credentials\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "from utils import flatten_nested_df\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e418987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "\n",
    "# Check if config file or CLI variable already set key value\n",
    "try:\n",
    "    assert openml.config.apikey != ''\n",
    "except AssertionError:\n",
    "    # Check for credentials file\n",
    "    if os.path.exists('credentials.pkl'):\n",
    "        with open('credentials.pkl', 'rb') as credentials:\n",
    "            openml.config.apikey = pickle.load(credentials)['OPENML_TOKEN']\n",
    "    else:\n",
    "        openml.config.apikey = input('Please enter your OpenML API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380169b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_value_attributes(obj):\n",
    "    \"\"\"\n",
    "    Given an object, returns a list of the object's value-based variables\n",
    "    \n",
    "    Params:\n",
    "    - obj : list-like \n",
    "        object to be analyzed \n",
    "    \n",
    "    Returns:\n",
    "    - attributes : list\n",
    "        value-based variables for the object given\n",
    "    \"\"\"  \n",
    "    \n",
    "    # This code will pull all of the attributes of the provided class that are not callable or \"private\" \n",
    "    # for the class. \n",
    "    attributes = [attr for attr in dir(obj) if \n",
    "                           not hasattr(getattr(obj, attr), '__call__')\n",
    "                           and not attr.startswith('_')]\n",
    "    \n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_evaluations_search_output(flatten_output=False):\n",
    "    # Get different evaluation measures we can search for\n",
    "    evaluations_measures = openml.evaluations.list_evaluation_measures()\n",
    "    \n",
    "    # Create DataFrame to store attributes\n",
    "    evaluations_df = pd.DataFrame()\n",
    "\n",
    "    # Get evaluation data for each available measure\n",
    "    for measure in tqdm(evaluations_measures):\n",
    "        # Query all data for a given evaluation measure\n",
    "        evaluations_dict = openml.evaluations.list_evaluations(measure, size=size_limit)\n",
    "\n",
    "        try:\n",
    "            # Grab one of the evaluations in order to extract attributes\n",
    "            sample_evaluation = next(iter(evaluations_dict.items()))[1]\n",
    "        # StopIteration will occur in the preceding code if an evaluation search returns no results for a given measure\n",
    "        except StopIteration:\n",
    "            continue\n",
    "\n",
    "        # Get list of attributes the evaluation offers\n",
    "        evaluations_attributes = _get_value_attributes(sample_evaluation) \n",
    "\n",
    "        # Adds the queried data to the DataFrame\n",
    "        for query in evaluations_dict.values():\n",
    "            attribute_dict = {attribute: getattr(query, attribute) for attribute in evaluations_attributes}\n",
    "            evaluations_df = evaluations_df.append(attribute_dict, ignore_index=True)\n",
    "\n",
    "        evaluations_df = flatten_nested_df(evaluations_df)\n",
    "        \n",
    "    return evaluations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b066bdd5",
   "metadata": {},
   "source": [
    "## Query #1: query API based on search types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6823925",
   "metadata": {},
   "source": [
    "Function `get_all_search_outputs` queries the OpenML API for all search types specified and returns the results as a dictionary of dataframes (one dataframe for each query combination)\n",
    "- Calls function `get_individual_search_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4082cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_types, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the OpenML API for each search type. \n",
    "    Results are retured in results['({type},)'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_types : list-like \n",
    "        collection of search types to query over\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - results : dict\n",
    "        dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_type in search_types:\n",
    "        results[(search_type,)] = get_individual_search_output(search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b2982",
   "metadata": {},
   "source": [
    "Function `get_individual_search_output` queries the OpenML API with the specified search type ('conferences', 'datasets', 'evaluations', 'papers', or 'tasks')\n",
    "- Searches across all returned pages\n",
    "- If search type is 'evaluations', calls function `_get_evaluations_search_output`, which in turn calls function `_get_value_attributes`\n",
    "- Result is a dataframe (one dataframe per search type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Calls the OpenML API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_type : str\n",
    "        Must be in ('conferences', 'datasets', 'evaluations', 'papers', 'tasks')\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "   \n",
    "    Returns:\n",
    "    - query_df : pandas.DataFrame\n",
    "        DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    # Ensure proper instance type is passed in\n",
    "    try:\n",
    "        assert search_type in ('datasets', 'runs', 'tasks', 'evaluations')\n",
    "    except AssertionError:\n",
    "        raise ValueError(f'\\'{search_type}\\' is not a valid instance type')\n",
    "    \n",
    "    # Handle special case for evaluations\n",
    "    if search_type == 'evaluations':\n",
    "        return _get_evaluations_search_output(flatten_output)\n",
    "    \n",
    "    # Use query type to get necessary openml api functions\n",
    "    base_command = getattr(openml, search_type)\n",
    "    list_queries = getattr(base_command, f'list_{search_type}')\n",
    "\n",
    "    # Get base information about every object listed on OpenML for the given query type\n",
    "    ## Since there's too many runs to get all at once, we need to search with offsets and rest \n",
    "    ## periods so the server doesn't weep\n",
    "    \n",
    "    # Set search params\n",
    "    index = 0\n",
    "    size = 10000\n",
    "    query_df = pd.DataFrame()\n",
    "    \n",
    "    # Perform initial search\n",
    "    query_dict = list_queries(offset=(index * size), size=size)\n",
    "    \n",
    "    # Serach until all queries have been returned\n",
    "    while query_dict:\n",
    "        # Flatten output (if necessary)\n",
    "        if flatten_output:\n",
    "            query_df = flatten_nested_df(query_df)\n",
    "        \n",
    "        # Add results to cumulative output df\n",
    "        output_df = pd.DataFrame(query_dict).transpose()\n",
    "        output_df['page'] = index + 1\n",
    "        query_df = pd.concat([query_df, output_df]).reset_index(drop=True)\n",
    "        \n",
    "        # Increment search range\n",
    "        index += 1\n",
    "        \n",
    "        # Perform next search\n",
    "        query_dict = list_queries(offset=(index * size), size=size)\n",
    "    \n",
    "    # Flatten the nested DataFrame\n",
    "    if flatten_output:\n",
    "        query_df = flatten_nested_df(query_df)\n",
    "    \n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc92aa",
   "metadata": {},
   "source": [
    "#### Run query #1 functions - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e045a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes, we set the following \"small\"-scale range over which collections to search\n",
    "#size_limit = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_types = ['datasets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_types, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict[('datasets',)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38ffb9",
   "metadata": {},
   "source": [
    "## Query #2: query API for full metadata for hits from initial query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969e1c6",
   "metadata": {},
   "source": [
    "Function `get_query_metadata` extracts metadata associated with each object based on object path and formats as dataframe\n",
    "- Calls function `_get_value_attributes`\n",
    "- Output is single dataframe for each search type (matching each dataframe in result #1 dictionary output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b5bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the object/objects listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths : str/list-like\n",
    "    - search_type : str\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df : pandas.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "    \n",
    "    base_command = getattr(openml, search_type)\n",
    "    get_query = getattr(base_command, f'get_{search_type[:-1:]}')\n",
    "    \n",
    "    # Request each query\n",
    "    queries = []\n",
    "    error_queries = []\n",
    "    for object_path in tqdm(object_paths):\n",
    "        try:\n",
    "            queries.append(get_query(object_path))\n",
    "        except:\n",
    "            error_queries.append(object_path)\n",
    "    \n",
    "    \n",
    "    # Get list of attributes the queries offer\n",
    "    query_attributes = _get_value_attributes(queries[0])\n",
    "\n",
    "    # Create DataFrame to store attributes\n",
    "    query_attribute_df = pd.DataFrame(columns=query_attributes)\n",
    "\n",
    "    # Append attributes of each dataset to the DataFrame\n",
    "    for query in tqdm(queries):\n",
    "        attribute_dict = {attribute: getattr(query, attribute) for attribute in query_attributes}\n",
    "        query_attribute_df = query_attribute_df.append(attribute_dict, ignore_index=True)\n",
    "        \n",
    "    # Flatten the nested DataFrame\n",
    "    if flatten_output:\n",
    "        query_attribute_df = flatten_nested_df(query_attribute_df)\n",
    "\n",
    "    return query_attribute_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f803bd",
   "metadata": {},
   "source": [
    "Function `get_all_metadata` uses a `for` loop to put dataframes into an ordered dictionary, matching result #1 ordered_dictionary\n",
    "- Calls function `get_query_metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d7041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves all of the metadata that relates to the provided DataFrames\n",
    "    \n",
    "    Params:\n",
    "    - search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output  \n",
    "      \n",
    "    Returns:\n",
    "    - metadata_dict : collections.OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query\n",
    "        Order matches the order of search_output_dict\n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dict = OrderedDict()\n",
    "\n",
    "    for query, df in search_output_dict.items():\n",
    "        print(f'Retrieving {query} metadata')\n",
    "\n",
    "        # Get ID name\n",
    "        search_type = query[0]\n",
    "\n",
    "        if search_type == 'datasets':\n",
    "            id_name = 'did'\n",
    "        elif search_type == 'runs':\n",
    "            id_name = 'run_id'\n",
    "        elif search_type == 'tasks':\n",
    "            id_name = 'tid'\n",
    "\n",
    "        # Grab the object paths as the id's from the DataFrame\n",
    "        object_paths = df[id_name].values\n",
    "\n",
    "        metadata_dict[query] = get_query_metadata(object_paths, search_type, flatten_output)\n",
    "        \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b7869",
   "metadata": {},
   "source": [
    "#### Run query #2 functions - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = get_all_metadata(search_output_dict, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f646b7",
   "metadata": {},
   "source": [
    "## Combine results of query #1 and query #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeebf77",
   "metadata": {},
   "source": [
    "Function `merge_search_and_metadata_dicts` merges the output dictionaries from query #1 and query #2 to a single ordered dictionary and (optional) saves the results as a single csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict : dict\n",
    "        dictionary of search output results\n",
    "    - metadata_dict : dict\n",
    "        dictionary of metadata results\n",
    "    - on : str/list-like\n",
    "        column name(s) to merge the two dicts on\n",
    "    - left_on : str/list-like\n",
    "        column name(s) to merge the left dict on\n",
    "    - right_on : str/list-like\n",
    "        column name(s) to merge the right dict on\n",
    "    - save : bool, optional (default=False)\n",
    "        specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/openml/openml_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    If the on/left_on/right_on values are not explicitely specified, behavior defaults to what is done\n",
    "    in the pandas documentation\n",
    "    \n",
    "    Returns:\n",
    "    - df_dict : OrderedDict\n",
    "        OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "\n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'openml')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure kaggle directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "\n",
    "                search_type = query_key[0]\n",
    "                output_file = f'{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError('Save type must be bool or str')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9647e0e",
   "metadata": {},
   "source": [
    "#### Run merge function - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add evaluations data (doesn't have metadata so had to be handled separately)\n",
    "df_dict[('evaluations',)] = get_individual_search_output('evaluations', flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23307888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict[('evaluations',)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
