{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714add95",
   "metadata": {},
   "source": [
    "# Harvard Dataverse API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a818eb",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ad5de",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810745da",
   "metadata": {},
   "source": [
    "This notebook utilizes the Harvard Dataverse API. Follow these steps in order to get the necessary credentials to continue:\n",
    "\n",
    "1. Create a Harvard Dataverse account at [Harvard Dataverse](https://dataverse.harvard.edu/dataverseuser.xhtml;jsessionid=797ccf2a28f987da3f1895ad81df?editMode=CREATE&redirectPage=%2Fdataverse_homepage.xhtml)\n",
    "2. After logging in, click on the user dropdown menu in the top right corner, and click on 'API Token'\n",
    "3. Click on 'Create Token' to receive API Token\n",
    "4. Load API Token:\n",
    "    - For repeated use, follow the ```pickle_tutorial.ipynb``` instructions to create create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'DATAVERSE_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "    - For sparser use, users can run the credentials cell and paste their API key when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364655c",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a091a4f",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Dataverse API ([Dataverse](https://guides.dataverse.org/en/latest/user/index.html))\n",
    "- Harvard Dataverse ([Harvard](https://dataverse.harvard.edu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75a3b8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544b6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For querying data from API\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "from collections import OrderedDict\n",
    "from flatten_json import flatten\n",
    "\n",
    "# For loading credentials\n",
    "import pickle\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495b8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "try:\n",
    "    with open('credentials.pkl', 'rb') as credentials:\n",
    "        DATAVERSE_TOKEN = pickle.load(credentials)['DATAVERSE_TOKEN']\n",
    "except:\n",
    "    DATAVERSE_TOKEN = input('Please enter your Dataverse API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2304196",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0233a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e34f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://dataverse.harvard.edu/api'\n",
    "HEADERS = {'X-Dataverse-key': DATAVERSE_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7273d48",
   "metadata": {},
   "source": [
    "## Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929dfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, search_types, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the Dataverse API for each search term. \n",
    "    Results are retured in results[(search_term)] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms : list-like\n",
    "        collection of search terms to query over\n",
    "    - search_types : list-like\n",
    "        collection of objects to search over (must be either dataset or file)\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - results : dict\n",
    "        dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "\n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term, search_type in tqdm(itertools.product(search_terms, search_types)):\n",
    "        results[(search_term, search_type)] = get_individual_search_output(search_term, search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0707c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_major_minor_version(row):\n",
    "    major = int(row['majorVersion'])\n",
    "    minor = int(row['minorVersion'])\n",
    "    return float(f'{major}.{minor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4702532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Calls the Dataverse API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term : str\n",
    "    - search_type : str\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "   \n",
    "    Returns:\n",
    "    - df : pandas.DataFrame\n",
    "        DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set search URL\n",
    "    search_url = f'{BASE_URL}/search'\n",
    "    \n",
    "    # Make sure out input is valid\n",
    "    assert isinstance(search_term, str), 'Search term must be a string'\n",
    "    assert isinstance(search_type, str), 'Search type must be a string'\n",
    "    assert search_type in ('dataset', 'file'), 'Search can only be conducted over \"dataset\" or \"file\"'\n",
    "    \n",
    "    # Set search parameters\n",
    "    start = 0\n",
    "    page_size = 100\n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    search_params = {\n",
    "        'q': search_term,\n",
    "        'per_page': page_size,\n",
    "        'start': start,\n",
    "        'type': search_type\n",
    "    }\n",
    "    \n",
    "    # Conduct initial query, extract json results\n",
    "    response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "    output = response.json()\n",
    "    output = output['data']\n",
    "    \n",
    "    # Search until no more items are returned\n",
    "    while output.get('items'):\n",
    "        # Extract relevant output data\n",
    "        output = output['items']\n",
    "        \n",
    "        # Flatten output if necessary\n",
    "        if flatten_output:\n",
    "            output = [flatten(result) for result in output]\n",
    "        \n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['page'] = search_params['start'] // search_params['per_page'] + 1\n",
    "        \n",
    "        search_df = pd.concat([search_df, output_df]).reset_index(drop=True)\n",
    "        \n",
    "        # Increment result offset to perform another search\n",
    "        search_params['start'] += search_params['per_page']\n",
    "        \n",
    "        # Perform next search and convert results to json\n",
    "        response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "        output = response.json()\n",
    "        output = output['data']\n",
    "    \n",
    "    # Drop null versions since version is required for metadata extraction\n",
    "    search_df = search_df.dropna(subset = ('majorVersion', 'minorVersion'), how='any')\n",
    "    # Add query-friendly dataset version column (for metadata extraction)\n",
    "    search_df['version'] = search_df.apply(_convert_major_minor_version, axis=1)\n",
    "\n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578d0b8",
   "metadata": {},
   "source": [
    "### Run initial API query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40934737",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['\\\"machine learning\\\" OR \\\"artificial intelligence\\\"', \n",
    "                '\\\"machine learning\\\"', \n",
    "                '\\\"artificial intelligence\\\"',\n",
    "                '\\\"deep learning\\\"',\n",
    "                '\\\"neural network\\\"',\n",
    "                '\\\"supervised learning\\\"',\n",
    "                '\\\"unsupervised learning\\\"',\n",
    "                '\\\"reinforcement learning\\\"',\n",
    "                '\\\"training data\\\"']\n",
    "search_types = ['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c34242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [05:21, 35.68s/it] \n"
     ]
    }
   ],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, search_types, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f522a",
   "metadata": {},
   "source": [
    "#### Take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c87dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "search_output_dict[('machine learning', 'dataset')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53bc27",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://dataverse.harvard.edu/api/datasets/2/versions/1.1/files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d746b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74236fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the dataset metadata for the object/objects listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths : str/list-like\n",
    "        Paths should be tuple of the form (dataset_id, dataset_version)\n",
    "    - search_type : str\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df : pandas.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure that each object_path has the correct number of values\n",
    "    assert all([len(path) == 2 for path in object_paths])\n",
    "    # Make sure search type is valid\n",
    "    assert search_type in ('files', 'metadata')\n",
    "    \n",
    "    metadata_df = pd.DataFrame()\n",
    "    \n",
    "    for object_path in tqdm(object_paths):\n",
    "        dataset_id, dataset_version = object_paths\n",
    "        search_url = f'{BASE_URL}/datasets/{dataset_id}/versions/{dataset_version}/{search_type}'\n",
    "        \n",
    "        # Request metadata info & extract results\n",
    "        response = requests.get(search_url, headers=HEADERS)\n",
    "        output = response.json()\n",
    "        \n",
    "        # Convert json results into DataFrame\n",
    "        if search_type == 'files':\n",
    "            output = output['data']\n",
    "        elif search_type == 'metadata':\n",
    "            output = output['data']['citation']['fields']\n",
    "            \n",
    "        object_df = pd.DataFrame(output)\n",
    "        \n",
    "        # Add relevant data to DataFrame and merge\n",
    "        object_df['id'] = dataset_id\n",
    "        object_df['from_version'] = dataset_version\n",
    "        metadata_df = pd.concat([metadata_df, object_df]).reset_index(drop=True)\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves all of the metadata that relates to the provided DataFrames\n",
    "    \n",
    "    Params:\n",
    "    - search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output  \n",
    "      \n",
    "    Returns:\n",
    "    - metadata_dict : collections.OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query\n",
    "        Order matches the order of search_output_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Extract IDs from DataFrame, and returns as list of strings\n",
    "    metadata_dict = OrderedDict()\n",
    "\n",
    "    for query, df in search_output_dict.items():\n",
    "        search_term, search_type = query\n",
    "        if search_type == 'file':\n",
    "            continue\n",
    "        \n",
    "        for search_type in ('files', 'metadata'):\n",
    "            query = (search_term, f'dataset_{search_type}')\n",
    "            print(f'Retrieving {search_term} metadata')\n",
    "\n",
    "            # Create object paths\n",
    "            object_ids = df.id.convert_dtypes(convert_string=True).tolist()\n",
    "            object_versions = df.version.convert_dypes\n",
    "            object_paths = (object_ids, object_versions)\n",
    "\n",
    "            metadata_dict[query] = get_query_metadata(object_paths, search_type, flatten_output)\n",
    "    \n",
    "    return metadata_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
