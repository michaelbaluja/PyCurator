{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714add95",
   "metadata": {},
   "source": [
    "# Harvard Dataverse API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a818eb",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ad5de",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810745da",
   "metadata": {},
   "source": [
    "This notebook utilizes the Harvard Dataverse API. Follow these steps in order to get the necessary credentials to continue:\n",
    "\n",
    "1. Create a Harvard Dataverse account at [Harvard Dataverse](https://dataverse.harvard.edu/dataverseuser.xhtml;jsessionid=797ccf2a28f987da3f1895ad81df?editMode=CREATE&redirectPage=%2Fdataverse_homepage.xhtml)\n",
    "2. After logging in, click on the user dropdown menu in the top right corner, and click on 'API Token'\n",
    "3. Click on 'Create Token' to receive API Token\n",
    "4. Load API Token:\n",
    "    - For repeated use, follow the ```pickle_tutorial.ipynb``` instructions to create create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'DATAVERSE_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "    - For sparser use, users can run the credentials cell and paste their API key when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364655c",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a091a4f",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Dataverse API ([Dataverse](https://guides.dataverse.org/en/latest/user/index.html))\n",
    "- Harvard Dataverse ([Harvard](https://dataverse.harvard.edu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75a3b8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544b6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For querying data from API\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "from collections import OrderedDict\n",
    "from flatten_json import flatten\n",
    "\n",
    "import pickle # For loading credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495b8288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "try:\n",
    "    with open('credentials.pkl', 'rb') as credentials:\n",
    "        DATAVERSE_TOKEN = pickle.load(credentials)['DATAVERSE_TOKEN']\n",
    "except:\n",
    "    DATAVERSE_TOKEN = input('Please enter your Dataverse API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2304196",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0233a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e34f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://dataverse.harvard.edu/api'\n",
    "HEADERS = {'X-Dataverse-key': DATAVERSE_TOKEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7273d48",
   "metadata": {},
   "source": [
    "## Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929dfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, search_types, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the Dataverse API for each search term. \n",
    "    Results are retured in results[(search_term)] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms : list-like\n",
    "        collection of search terms to query over\n",
    "    - search_types : list-like\n",
    "        collection of objects to search over (must be either dataset or file)\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - results : dict\n",
    "        dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "\n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term, search_type in tqdm(itertools.product(search_terms, search_types)):\n",
    "        results[(search_term, search_type)] = get_individual_search_output(search_term, search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0707c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_major_minor_version(row):\n",
    "    major = int(row['majorVersion'])\n",
    "    minor = int(row['minorVersion'])\n",
    "    return float(f'{major}.{minor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4702532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Calls the Dataverse API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term : str\n",
    "    - search_type : str\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "   \n",
    "    Returns:\n",
    "    - df : pandas.DataFrame\n",
    "        DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set search URL\n",
    "    search_url = f'{BASE_URL}/search'\n",
    "    \n",
    "    # Make sure out input is valid\n",
    "    assert isinstance(search_term, str), 'Search term must be a string'\n",
    "    assert isinstance(search_type, str), 'Search type must be a string'\n",
    "    assert search_type in ('dataset', 'file'), 'Search can only be conducted over \"dataset\" or \"file\"'\n",
    "    \n",
    "    # Set search parameters\n",
    "    start = 0\n",
    "    page_size = 100\n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    search_params = {\n",
    "        'q': search_term,\n",
    "        'per_page': page_size,\n",
    "        'start': start,\n",
    "        'type': search_type\n",
    "    }\n",
    "    \n",
    "    # Conduct initial query, extract json results\n",
    "    response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "    output = response.json()\n",
    "    output = output['data']\n",
    "    \n",
    "    # Search until no more items are returned\n",
    "    while output.get('items'):\n",
    "        # Extract relevant output data\n",
    "        output = output['items']\n",
    "        \n",
    "        # Flatten output if necessary\n",
    "        if flatten_output:\n",
    "            output = [flatten(result) for result in output]\n",
    "        \n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['page'] = search_params['start'] // search_params['per_page'] + 1\n",
    "        \n",
    "        search_df = pd.concat([search_df, output_df]).reset_index(drop=True)\n",
    "        \n",
    "        # Increment result offset to perform another search\n",
    "        search_params['start'] += search_params['per_page']\n",
    "        \n",
    "        # Perform next search and convert results to json\n",
    "        response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "        output = response.json()\n",
    "        output = output['data']\n",
    "    \n",
    "    # Drop null versions since version is required for metadata extraction\n",
    "    search_df = search_df.dropna(subset = ('majorVersion', 'minorVersion'), how='any')\n",
    "    # Add query-friendly dataset version column (for metadata extraction)\n",
    "    search_df['version'] = search_df.apply(_convert_major_minor_version, axis=1)\n",
    "\n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578d0b8",
   "metadata": {},
   "source": [
    "### Run initial API query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40934737",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['\\\"machine learning\\\"', '\\\"artificial intelligence\\\"']\n",
    "search_types = ['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c34242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:38, 49.35s/it]\n"
     ]
    }
   ],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, search_types, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f522a",
   "metadata": {},
   "source": [
    "#### Take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da8c87dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056bba22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>global_id</th>\n",
       "      <th>description</th>\n",
       "      <th>published_at</th>\n",
       "      <th>publisher</th>\n",
       "      <th>citationHtml</th>\n",
       "      <th>identifier_of_dataverse</th>\n",
       "      <th>name_of_dataverse</th>\n",
       "      <th>...</th>\n",
       "      <th>dataSources_21</th>\n",
       "      <th>dataSources_22</th>\n",
       "      <th>dataSources_23</th>\n",
       "      <th>relatedMaterial_1</th>\n",
       "      <th>geographicCoverage_2_other</th>\n",
       "      <th>geographicCoverage_0_city</th>\n",
       "      <th>geographicCoverage_2_city</th>\n",
       "      <th>geographicCoverage_3_city</th>\n",
       "      <th>geographicCoverage_4_city</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conformer models and training datasets</td>\n",
       "      <td>dataset</td>\n",
       "      <td>https://doi.org/10.7910/DVN/N4VLQL</td>\n",
       "      <td>doi:10.7910/DVN/N4VLQL</td>\n",
       "      <td>Here you can find pre-trained machine learning...</td>\n",
       "      <td>2021-02-11T22:02:13Z</td>\n",
       "      <td>Machine learning with conformer ensembles</td>\n",
       "      <td>Axelrod, Simon; Gomez-Bombarelli, Rafael, 2021...</td>\n",
       "      <td>ml_confs</td>\n",
       "      <td>Machine learning with conformer ensembles</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEOM</td>\n",
       "      <td>dataset</td>\n",
       "      <td>https://doi.org/10.7910/DVN/JNGTDF</td>\n",
       "      <td>doi:10.7910/DVN/JNGTDF</td>\n",
       "      <td>Here you can find GEOM, a dataset with over 33...</td>\n",
       "      <td>2021-02-01T19:44:41Z</td>\n",
       "      <td>Machine learning with conformer ensembles</td>\n",
       "      <td>Axelrod, Simon; Gomez-Bombarelli, Rafael, 2021...</td>\n",
       "      <td>ml_confs</td>\n",
       "      <td>Machine learning with conformer ensembles</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Base data for publication - The association of...</td>\n",
       "      <td>dataset</td>\n",
       "      <td>https://doi.org/10.7910/DVN/BFXCAM</td>\n",
       "      <td>doi:10.7910/DVN/BFXCAM</td>\n",
       "      <td>Base data for publication - The association of...</td>\n",
       "      <td>2020-06-15T18:12:08Z</td>\n",
       "      <td>The association of Coronavirus Disease-19 mort...</td>\n",
       "      <td>Nag, Swapnika; Puri, Ankur; Brooks, Nathan; Ga...</td>\n",
       "      <td>covid19bcgML</td>\n",
       "      <td>The association of Coronavirus Disease-19 mort...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine learn for glaucoma</td>\n",
       "      <td>dataset</td>\n",
       "      <td>https://doi.org/10.7910/DVN/1YRRAC</td>\n",
       "      <td>doi:10.7910/DVN/1YRRAC</td>\n",
       "      <td>Machine learn for glaucoma</td>\n",
       "      <td>2018-11-15T07:21:36Z</td>\n",
       "      <td>Harvard Dataverse</td>\n",
       "      <td>Kim, Ungsoo, 2018, \"Machine learn for glaucoma...</td>\n",
       "      <td>harvard</td>\n",
       "      <td>Harvard Dataverse</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iris dataset for machine learning</td>\n",
       "      <td>dataset</td>\n",
       "      <td>https://doi.org/10.7910/DVN/R2RGXR</td>\n",
       "      <td>doi:10.7910/DVN/R2RGXR</td>\n",
       "      <td>This is an iris dataset commonly used in machi...</td>\n",
       "      <td>2020-10-19T17:50:03Z</td>\n",
       "      <td>Harvard Dataverse</td>\n",
       "      <td>Monahan, Kyle M., 2020, \"Iris dataset for mach...</td>\n",
       "      <td>harvard</td>\n",
       "      <td>Harvard Dataverse</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name     type  \\\n",
       "0             Conformer models and training datasets  dataset   \n",
       "1                                               GEOM  dataset   \n",
       "2  Base data for publication - The association of...  dataset   \n",
       "3                         Machine learn for glaucoma  dataset   \n",
       "4                  Iris dataset for machine learning  dataset   \n",
       "\n",
       "                                  url               global_id  \\\n",
       "0  https://doi.org/10.7910/DVN/N4VLQL  doi:10.7910/DVN/N4VLQL   \n",
       "1  https://doi.org/10.7910/DVN/JNGTDF  doi:10.7910/DVN/JNGTDF   \n",
       "2  https://doi.org/10.7910/DVN/BFXCAM  doi:10.7910/DVN/BFXCAM   \n",
       "3  https://doi.org/10.7910/DVN/1YRRAC  doi:10.7910/DVN/1YRRAC   \n",
       "4  https://doi.org/10.7910/DVN/R2RGXR  doi:10.7910/DVN/R2RGXR   \n",
       "\n",
       "                                         description          published_at  \\\n",
       "0  Here you can find pre-trained machine learning...  2021-02-11T22:02:13Z   \n",
       "1  Here you can find GEOM, a dataset with over 33...  2021-02-01T19:44:41Z   \n",
       "2  Base data for publication - The association of...  2020-06-15T18:12:08Z   \n",
       "3                         Machine learn for glaucoma  2018-11-15T07:21:36Z   \n",
       "4  This is an iris dataset commonly used in machi...  2020-10-19T17:50:03Z   \n",
       "\n",
       "                                           publisher  \\\n",
       "0          Machine learning with conformer ensembles   \n",
       "1          Machine learning with conformer ensembles   \n",
       "2  The association of Coronavirus Disease-19 mort...   \n",
       "3                                  Harvard Dataverse   \n",
       "4                                  Harvard Dataverse   \n",
       "\n",
       "                                        citationHtml identifier_of_dataverse  \\\n",
       "0  Axelrod, Simon; Gomez-Bombarelli, Rafael, 2021...                ml_confs   \n",
       "1  Axelrod, Simon; Gomez-Bombarelli, Rafael, 2021...                ml_confs   \n",
       "2  Nag, Swapnika; Puri, Ankur; Brooks, Nathan; Ga...            covid19bcgML   \n",
       "3  Kim, Ungsoo, 2018, \"Machine learn for glaucoma...                 harvard   \n",
       "4  Monahan, Kyle M., 2020, \"Iris dataset for mach...                 harvard   \n",
       "\n",
       "                                   name_of_dataverse  ... dataSources_21  \\\n",
       "0          Machine learning with conformer ensembles  ...            NaN   \n",
       "1          Machine learning with conformer ensembles  ...            NaN   \n",
       "2  The association of Coronavirus Disease-19 mort...  ...            NaN   \n",
       "3                                  Harvard Dataverse  ...            NaN   \n",
       "4                                  Harvard Dataverse  ...            NaN   \n",
       "\n",
       "  dataSources_22 dataSources_23 relatedMaterial_1 geographicCoverage_2_other  \\\n",
       "0            NaN            NaN               NaN                        NaN   \n",
       "1            NaN            NaN               NaN                        NaN   \n",
       "2            NaN            NaN               NaN                        NaN   \n",
       "3            NaN            NaN               NaN                        NaN   \n",
       "4            NaN            NaN               NaN                        NaN   \n",
       "\n",
       "  geographicCoverage_0_city geographicCoverage_2_city  \\\n",
       "0                       NaN                       NaN   \n",
       "1                       NaN                       NaN   \n",
       "2                       NaN                       NaN   \n",
       "3                       NaN                       NaN   \n",
       "4                       NaN                       NaN   \n",
       "\n",
       "   geographicCoverage_3_city  geographicCoverage_4_city version  \n",
       "0                        NaN                        NaN     3.0  \n",
       "1                        NaN                        NaN     2.0  \n",
       "2                        NaN                        NaN     1.0  \n",
       "3                        NaN                        NaN     1.0  \n",
       "4                        NaN                        NaN     1.0  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53bc27",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b74236fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the dataset metadata for the object/objects listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths : str/list-like\n",
    "        Paths should be tuple of the form (dataset_id, dataset_version)\n",
    "    - search_type : str\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df : pandas.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure that each object_path has the correct number of values\n",
    "    assert all([len(path) == 2 for path in object_paths])\n",
    "    # Make sure search type is valid\n",
    "    assert search_type in ('files', 'metadata')\n",
    "    \n",
    "    metadata_df = pd.DataFrame()\n",
    "    \n",
    "    for object_path in tqdm(object_paths):\n",
    "        dataset_id, dataset_version = object_paths\n",
    "        search_url = f'{BASE_URL}/datasets/{dataset_id}/versions/{dataset_version}/{search_type}'\n",
    "        \n",
    "        # Request metadata info & extract results\n",
    "        response = requests.get(search_url, headers=HEADERS)\n",
    "        output = response.json()\n",
    "        \n",
    "        # Convert json results into DataFrame\n",
    "        if search_type == 'files':\n",
    "            output = output['data']\n",
    "        elif search_type == 'metadata':\n",
    "            output = output['data']['citation']['fields']\n",
    "            \n",
    "        object_df = pd.DataFrame(output)\n",
    "        \n",
    "        # Add relevant data to DataFrame and merge\n",
    "        object_df['id'] = dataset_id\n",
    "        object_df['from_version'] = dataset_version\n",
    "        metadata_df = pd.concat([metadata_df, object_df]).reset_index(drop=True)\n",
    "    \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "217e766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves all of the metadata that relates to the provided DataFrames\n",
    "    \n",
    "    Params:\n",
    "    - search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output  \n",
    "      \n",
    "    Returns:\n",
    "    - metadata_dict : collections.OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query\n",
    "        Order matches the order of search_output_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Extract IDs from DataFrame, and returns as list of strings\n",
    "    metadata_dict = OrderedDict()\n",
    "\n",
    "    for query, df in search_output_dict.items():\n",
    "        search_term, search_type = query\n",
    "        if search_type == 'file':\n",
    "            continue\n",
    "        \n",
    "        for search_type in ('files', 'metadata'):\n",
    "            query = (search_term, f'dataset_{search_type}')\n",
    "            print(f'Retrieving {search_term} metadata')\n",
    "\n",
    "            # Create object paths\n",
    "            object_ids = df.id.convert_dtypes(convert_string=True).tolist()\n",
    "            object_versions = df.version.convert_dypes\n",
    "            object_paths = (object_ids, object_versions)\n",
    "\n",
    "            metadata_dict[query] = get_query_metadata(object_paths, search_type, flatten_output)\n",
    "    \n",
    "    return metadata_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
