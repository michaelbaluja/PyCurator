{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs the kaggle library\n",
    "!pip3 install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlike other API notebooks, no need to import a .py file with API token info\n",
    "#for kaggle, lives locally in ~/.kaggle/kaggle.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the Kaggle API, sign up for a Kaggle account at https://www.kaggle.com. Then go to the 'Account' tab of your user profile (https://www.kaggle.com/<username>/account) and select 'Create API Token'. This will trigger the download of kaggle.json, a file containing your API credentials. Place this file in the location ~/.kaggle/kaggle.json (on Windows in the location C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json - you can check the exact location, sans drive, with echo %HOMEPATH%). You can define a shell environment variable KAGGLE_CONFIG_DIR to change this location to $KAGGLE_CONFIG_DIR/kaggle.json (on Windows it will be %KAGGLE_CONFIG_DIR%\\kaggle.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are Pythonic options, but seems easiest to just run terminal commands from within notebook\n",
    "\n",
    "#example:\n",
    "! kaggle competitions list -s health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search datasets for term\n",
    "! kaggle datasets list -s \"machine learning\"\n",
    "\n",
    "#this returns by default one page of results at a time (matches manual search of datasets page 1)\n",
    "#need to extract \"ref\" variable, which is <owner>/<dataset-name>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching datasets is a start, but we'll also want to search notebooks, competitions, etc\n",
    "#looks like can search datasets and kernels\n",
    "#and get related files for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once get list of files in format <owner>/<dataset-name>, can get list of files\n",
    "#example:\n",
    "! kaggle datasets files kaggle/kaggle-survey-2018 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#can get metadata for a dataset\n",
    "#default downloads into current working directory\n",
    "#need a way to store in memory and iterate for all files\n",
    "! kaggle datasets metadata -p /Kaggle_results/ kaggle/kaggle-survey-2018 \n",
    "\n",
    "#this currently barfs, not sure why - something with where putting output,b ut not sure why\n",
    "#TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonic-ish Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess # Used to run unix commands\n",
    "import pandas as pd # For storing/manipulating command data\n",
    "from io import StringIO # Lets us read csv string output from command into DataFrame\n",
    "import json # Reading back the metadata files\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not depracated, but not needed, but spent time to write so saving in case needed later\n",
    "def extract_dataset_paths(output):\n",
    "    '''\n",
    "    Takes the output from our kaggle run command and returns the dataset paths\n",
    "    Params:\n",
    "    - output (str): command output to be parsed\n",
    "    Returns:\n",
    "    - dataset_paths (list): list of dataset paths present in output\n",
    "    '''\n",
    "    # Remove newline characters\n",
    "    output = output.replace('\\n', '')\n",
    "    \n",
    "    # Extracts all instances of output that contain a '/' character\n",
    "    # (/ used exclusively in dataset names at time of writing)\n",
    "    dataset_paths = [out for out in output.split(' ') if '/' in out]\n",
    "    return dataset_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SEARCH_COMPLETION = 'No datasets found\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting/extracting dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the terms to search over\n",
    "search_terms = ['korea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "page_idx = 1\n",
    "dataset_output = ''\n",
    "cumulative_output = ''\n",
    "dataset_paths = []\n",
    "\n",
    "for search_term in tqdm(search_terms):\n",
    "    while dataset_output != DATA_SEARCH_COMPLETION:\n",
    "        # Pulls the records for a single page of datasets for the given search term\n",
    "        # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "        dataset_output = subprocess.run(['kaggle', 'datasets', 'list', '-v',\n",
    "                                         '-s', f'\"{search_term}\"', \n",
    "                                         '-p', str(page_idx)], \n",
    "                                        capture_output=True).stdout.decode()\n",
    "        \n",
    "        # Accumulate the output\n",
    "        cumulative_output = cumulative_output + dataset_output\n",
    "\n",
    "        # Increments the page count for searching\n",
    "        page_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of results & clean it up\n",
    "cumulative_output = StringIO(cumulative_output)\n",
    "dataset_df = pd.read_csv(cumulative_output).drop_duplicates().reset_index(drop=True)\n",
    "# Note: we drop duplicates because otherwise each page will add headers as a row\n",
    "# This should also remedy situations where the same entry comes for multiple search terms\n",
    "\n",
    "# Removes last row, which is Null entry from stopping criteria set above\n",
    "dataset_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling dataset metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Unable to find a way to store metadata in memory as opposed to saving file, but this workaround appears to be functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_HEADERS = ['id', 'id_no', 'datasetId', 'datasetSlug', 'ownerUser', 'usabilityRating', 'totalViews', \n",
    "                'totalVotes', 'totalDownloads', 'title', 'subtitle', 'description', 'isPrivate', 'keywords', \n",
    "                'licenses', 'collaborators', 'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create DataFrame to store metadata in\n",
    "metadata_df = pd.DataFrame(columns=JSON_HEADERS, dtype=object)\n",
    "\n",
    "# Pulls metadata information for each dataset found above\n",
    "for data_path in tqdm(dataset_df.ref):\n",
    "    # Download the metadata\n",
    "    subprocess.run(['kaggle', 'datasets', 'metadata', data_path])\n",
    "    \n",
    "    # Access the metadata and load it in as a dictionary\n",
    "    with open('dataset-metadata.json') as file:\n",
    "        json_data = json.load(file)\n",
    "        \n",
    "    # Store the metadata into our DataFrame created above\n",
    "    metadata_df = metadata_df.append(json_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
