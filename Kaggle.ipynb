{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the Kaggle API. Follow these steps in order to get the necessary credentials to continue:\n",
    "\n",
    "1. Sign up for a Kaggle account at https://www.kaggle.com.\n",
    "2. Go to the 'Account' tab of your user profile - ```https://www.kaggle.com/{username}/account```.\n",
    "3. Select 'Create New API Token' under 'API' section.\n",
    "    - This will trigger the download of kaggle.json, a file containing your API credentials. \n",
    "4. Place this file in the location:\n",
    "    - ~/.kaggle/kaggle.json (for macOS/unix)\n",
    "    - C:/Users/username/.kaggle/kaggle.json (for Windows) \n",
    "    - You can check the exact location, sans drive, with echo %HOMEPATH%). \n",
    "    - You can define a shell environment variable KAGGLE_CONFIG_DIR to change this location to:\n",
    "        - $KAGGLE_CONFIG_DIR/kaggle.json (for macOS/unix)\n",
    "        - %KAGGLE_CONFIG_DIR%\\kaggle.json (for Windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this notebook uses functions written in Python to query the Kaggle API. **This code will only work for python 3.7 or later**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Kaggle API ([Kaggle](https://www.kaggle.com/docs/api))\n",
    "- Kaggle API ([GitHub](https://github.com/Kaggle/kaggle-api)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries\n",
    "\n",
    "#### Query API based on search terms and search types\n",
    "\n",
    "Define functions to query API based on search terms and search types:\n",
    "1.\tFunction `get_search_output` queries the Kaggle API with the specified search term (e.g., “machine learning”) and search type (must be either “datasets” or “kernels”)\n",
    "    - Searches across all returned pages\n",
    "    - Calls function `_convert_string_csv_output_to_dataframe` clean up results (which include X)\n",
    "    \n",
    "    \n",
    "2.\tFunction `_convert_string_csv_output_to_dataframe` converts results from API (strings in semi-structured table) to dataframe format\n",
    "\n",
    "\n",
    "3.\tFunction `get_search_outputs` queries the Kaggle API for all combinations of search terms and search types specified and returns the results as a dictionary of dataframes (one dataframe for each query combination)\n",
    "    - Calls function `get_search_output`\n",
    "\n",
    "Run functions for specified search terms and search types\n",
    "\n",
    "\n",
    "#### Query API for full metadata for hits from initial query\n",
    "\n",
    "Following the initial query to return high level metadata for hits of initial query, define functions to retrieve full metadata associated with each object\n",
    "\n",
    "4.\tFunction `_retrieve_object_json` uses the path for each object (e.g., <example>) to query the API for the associated metadata\n",
    "    \n",
    "    \n",
    "5.\tFunction `get_metadata` extracts metadata associated with each object and formats as dataframe\n",
    "    - Calls function `retrieve_object_json`\n",
    "    - <need clarification on section “perform metadata extraction” – this calls `get_metadata` and puts results in a dictionary?>\n",
    "\n",
    "Run functions to pull metadata for each object returned by first query.\n",
    "\n",
    "We now have a dictionary of results from the first API query (which includes XYZ) and a dictionary of results with metadata for each object, based on path from first query.\n",
    "\n",
    "#### Merge results\n",
    "6.\tFunction `merge_search_and_metadata_dicts` merges these two dictionaries and (optional) saves the resulting dataframe to csv\n",
    "    - <need clarification on if `left_on` and `right_on` always need to be specified, in which case, can this be an argument in the function itself, to streamline>\n",
    "    - <need clarification on output format – can save dataframe as csv, but output is ordered dict – why not dataframe?>\n",
    "\n",
    "Run merge function to output a csv (??) of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import kaggle, installing if necessary\n",
    "try:\n",
    "    import kaggle\n",
    "except ImportError as e:\n",
    "    !pip3 install kaggle\n",
    "    import kaggle\n",
    "    \n",
    "import subprocess # Used to run unix commands\n",
    "import pandas as pd # For storing/manipulating command data\n",
    "from io import StringIO # Lets us read csv string output from command into DataFrame\n",
    "import json # Reading back the metadata files\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "import os # Exporting saved results\n",
    "from collections import OrderedDict\n",
    "from utils import flatten_nested_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query API based on search terms and search types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_outputs(search_terms, search_types, flatten=False):\n",
    "    \"\"\"\n",
    "    Call the Kaggle API for each search term and search type. \n",
    "    Results are retured in results['{term}_{type}'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms (list-like): collection of search terms to query over\n",
    "    - search_types (list-like): collection of search types to query over\n",
    "    - flatten (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - results (dict): dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    num_searches = len(search_terms) * len(search_types)\n",
    "    results = OrderedDict()\n",
    "    \n",
    "    for search_term, search_type in itertools.product(search_terms, search_types):\n",
    "        results[(search_term, search_type)] = get_search_output(search_term, search_type)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_output(search_term, search_type, flatten=False):\n",
    "    \"\"\"\n",
    "    Calls the Kaggle API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term (str): keyword to seach for\n",
    "    - search_type (str): objects to search over (must be either datasets or kernels)\n",
    "    - flatten (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert isinstance(search_type, str), 'Search term must be a string'\n",
    "    assert search_type in ('datasets', 'kernels'), 'Search can only be conducted over datasets or kernels'\n",
    "    \n",
    "    # Set search variables\n",
    "    page_idx = 1\n",
    "    search_output = ''\n",
    "    cumulative_output = ''\n",
    "    completion_phrase = f'No {search_type} found\\n'\n",
    "    \n",
    "    # Pulls the records for a single page of datasets for the given search term\n",
    "    # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "    search_output = subprocess.run(['kaggle', search_type, 'list', '-v',\n",
    "                                     '-s', f'\"{search_term}\"', \n",
    "                                     '-p', str(page_idx)], \n",
    "                                    capture_output=True).stdout.decode()\n",
    "\n",
    "    # Once we no longer see new output, we stop\n",
    "    while search_output != completion_phrase:\n",
    "        # Accumulate the output\n",
    "        cumulative_output = cumulative_output + search_output\n",
    "\n",
    "        # Increments the page count for searching\n",
    "        page_idx += 1\n",
    "\n",
    "        # Pulls the records for a single page of datasets for the given search term\n",
    "        # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "        search_output = subprocess.run(['kaggle', search_type, 'list', '-v',\n",
    "                                         '-s', f'\"{search_term}\"', \n",
    "                                         '-p', str(page_idx)], \n",
    "                                        capture_output=True).stdout.decode()\n",
    "\n",
    "        # Remove header row\n",
    "        if search_output != completion_phrase:\n",
    "            search_output = '\\r\\n'.join(search_output.split('\\r\\n')[1::])\n",
    "    \n",
    "    # Convert search output to DataFrame\n",
    "    search_df = _convert_string_csv_output_to_dataframe(cumulative_output)\n",
    "    \n",
    "    # Rename columns to match names present in metadata df\n",
    "    search_df.rename(columns={'ref': 'id', \n",
    "                              'downloadCount': 'totalDownloads', \n",
    "                              'voteCount': 'totalVotes'}, \n",
    "                     inplace=True)\n",
    "        \n",
    "    if flatten:\n",
    "        search_df = flatten_nested_df(search_df)\n",
    "    \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_string_csv_output_to_dataframe(output):\n",
    "    \"\"\"\n",
    "    Given a string variable in csv format, returns a Pandas DataFrame\n",
    "    \n",
    "    Params:\n",
    "    - output (str): csv-styled string to be converted\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame consisting of data from 'output' string variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create DataFrame of results\n",
    "    output = StringIO(output)\n",
    "    df = pd.read_csv(output)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run initial API query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['korea']\n",
    "search_types = ['datasets', 'kernels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_search_outputs(search_terms, search_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>size</th>\n",
       "      <th>lastUpdated</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>usabilityRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>7MB</td>\n",
       "      <td>2020-07-13 14:07:31</td>\n",
       "      <td>83975</td>\n",
       "      <td>1469</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>20MB</td>\n",
       "      <td>2020-04-03 16:33:49</td>\n",
       "      <td>9920</td>\n",
       "      <td>311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>99KB</td>\n",
       "      <td>2020-06-04 08:53:36</td>\n",
       "      <td>1048</td>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>772KB</td>\n",
       "      <td>2020-12-20 13:05:27</td>\n",
       "      <td>590</td>\n",
       "      <td>15</td>\n",
       "      <td>0.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>3GB</td>\n",
       "      <td>2020-03-15 08:56:42</td>\n",
       "      <td>5583</td>\n",
       "      <td>108</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id  \\\n",
       "0                     kimjihoo/coronavirusdataset   \n",
       "1                 bappekim/air-pollution-in-seoul   \n",
       "2                   bappekim/south-korea-visitors   \n",
       "3               hongsean/korea-income-and-welfare   \n",
       "4  bryanpark/korean-single-speaker-speech-dataset   \n",
       "\n",
       "                                             title   size  \\\n",
       "0  [NeurIPS 2020] Data Science for COVID-19 (DS4C)    7MB   \n",
       "1                           Air Pollution in Seoul   20MB   \n",
       "2                             South Korea Visitors   99KB   \n",
       "3                         Korea Income and Welfare  772KB   \n",
       "4             Korean Single Speaker Speech Dataset    3GB   \n",
       "\n",
       "           lastUpdated  totalDownloads  totalVotes  usabilityRating  \n",
       "0  2020-07-13 14:07:31           83975        1469         1.000000  \n",
       "1  2020-04-03 16:33:49            9920         311         1.000000  \n",
       "2  2020-06-04 08:53:36            1048          24         1.000000  \n",
       "3  2020-12-20 13:05:27             590          15         0.970588  \n",
       "4  2020-03-15 08:56:42            5583         108         0.750000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query API for full metadata for hits from initial query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Unable to find a way to store metadata in memory as opposed to saving file, but this workaround appears to be functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_object_json(object_path):\n",
    "    \"\"\"\n",
    "    Queries Kaggle for metadata json file & returns the json data as a dictionary\n",
    "    \n",
    "    Params:\n",
    "    - object_path (str): path for the dataset\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_dict (dict): dictionary containing json metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download the metadata\n",
    "    subprocess.run(['kaggle', 'datasets', 'metadata', object_path])\n",
    "\n",
    "    # Access the metadata and load it in as a dictionary\n",
    "    with open('dataset-metadata.json') as file:\n",
    "        json_data = json.load(file)\n",
    "        \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(object_paths, flatten=False):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the file/files listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths (str/list-like): string or list of strings containing the paths for the objects\n",
    "    - flatten (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df (pandas.DataFrame): DataFrame containing metadata for the requested datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "        \n",
    "    # Run first query\n",
    "    json_data = _retrieve_object_json(object_paths[0])\n",
    "        \n",
    "    # Create DataFrame to store metadata in, using columns found in first query, and then add query info\n",
    "    metadata_df = pd.DataFrame(columns=json_data.keys(), dtype=object)\n",
    "    metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    # Pulls metadata information for each dataset found above\n",
    "    for object_path in tqdm(object_paths[1::]):\n",
    "        # Download & load the metadata\n",
    "        json_data = _retrieve_object_json(object_path)\n",
    "\n",
    "        # Store the metadata into our DataFrame created above\n",
    "        metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "    \n",
    "    if flatten:\n",
    "        metadata_df = flatten_nested_df(metadata_df)\n",
    "        \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run metadata extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('korea', 'datasets') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [07:18<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('korea', 'kernels') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1019/1019 [1:08:01<00:00,  4.01s/it]\n"
     ]
    }
   ],
   "source": [
    "## Extract IDs from DataFrame, and returns as list of strings\n",
    "metadata_dict = OrderedDict()\n",
    "\n",
    "for query, df in search_output_dict.items():\n",
    "    print(f'Retrieving {query} metadata')\n",
    "    # Create object paths\n",
    "    _, search_type = query\n",
    "    object_paths = df.id.values\n",
    "    \n",
    "    metadata_dict[query] = get_metadata(object_paths, flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_no</th>\n",
       "      <th>datasetId</th>\n",
       "      <th>datasetSlug</th>\n",
       "      <th>ownerUser</th>\n",
       "      <th>usabilityRating</th>\n",
       "      <th>totalViews</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>collaborators_12_role</th>\n",
       "      <th>collaborators_12_username</th>\n",
       "      <th>collaborators_13_role</th>\n",
       "      <th>collaborators_13_username</th>\n",
       "      <th>collaborators_14_role</th>\n",
       "      <th>collaborators_14_username</th>\n",
       "      <th>collaborators_15_role</th>\n",
       "      <th>collaborators_15_username</th>\n",
       "      <th>collaborators_16_role</th>\n",
       "      <th>collaborators_16_username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>527325</td>\n",
       "      <td>527325</td>\n",
       "      <td>coronavirusdataset</td>\n",
       "      <td>kimjihoo</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>491478</td>\n",
       "      <td>1469</td>\n",
       "      <td>83975</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>...</td>\n",
       "      <td>writer</td>\n",
       "      <td>parkjuhwan</td>\n",
       "      <td>writer</td>\n",
       "      <td>leewoncheol</td>\n",
       "      <td>writer</td>\n",
       "      <td>younajung</td>\n",
       "      <td>writer</td>\n",
       "      <td>minty99</td>\n",
       "      <td>writer</td>\n",
       "      <td>byeongukyu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>576393</td>\n",
       "      <td>576393</td>\n",
       "      <td>air-pollution-in-seoul</td>\n",
       "      <td>bappekim</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81401</td>\n",
       "      <td>311</td>\n",
       "      <td>9920</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>692628</td>\n",
       "      <td>692628</td>\n",
       "      <td>south-korea-visitors</td>\n",
       "      <td>bappekim</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5879</td>\n",
       "      <td>24</td>\n",
       "      <td>1048</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>1046735</td>\n",
       "      <td>1046735</td>\n",
       "      <td>korea-income-and-welfare</td>\n",
       "      <td>hongsean</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>3060</td>\n",
       "      <td>15</td>\n",
       "      <td>590</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>19829</td>\n",
       "      <td>19829</td>\n",
       "      <td>korean-single-speaker-speech-dataset</td>\n",
       "      <td>bryanpark</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>83614</td>\n",
       "      <td>108</td>\n",
       "      <td>5583</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>glhr00/rgbt-driveability-segmentation-kaist</td>\n",
       "      <td>1340078</td>\n",
       "      <td>1340078</td>\n",
       "      <td>rgbt-driveability-segmentation-kaist</td>\n",
       "      <td>glhr00</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RGB-T driveability segmentation</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>llkdev/gender-data</td>\n",
       "      <td>231235</td>\n",
       "      <td>231235</td>\n",
       "      <td>gender-data</td>\n",
       "      <td>llkdev</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>341</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Gender Data</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>chenykfrank/immc21</td>\n",
       "      <td>1135726</td>\n",
       "      <td>1135726</td>\n",
       "      <td>immc21</td>\n",
       "      <td>chenykfrank</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>immc21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>gremmn/gas-prices</td>\n",
       "      <td>1108372</td>\n",
       "      <td>1108372</td>\n",
       "      <td>gas-prices</td>\n",
       "      <td>gremmn</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>gas prices</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>theeranartmeesathien/covid-dataset-for-prediction</td>\n",
       "      <td>1392213</td>\n",
       "      <td>1392213</td>\n",
       "      <td>covid-dataset-for-prediction</td>\n",
       "      <td>theeranartmeesathien</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Covid dataset for prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id    id_no datasetId  \\\n",
       "0                          kimjihoo/coronavirusdataset   527325    527325   \n",
       "1                      bappekim/air-pollution-in-seoul   576393    576393   \n",
       "2                        bappekim/south-korea-visitors   692628    692628   \n",
       "3                    hongsean/korea-income-and-welfare  1046735   1046735   \n",
       "4       bryanpark/korean-single-speaker-speech-dataset    19829     19829   \n",
       "..                                                 ...      ...       ...   \n",
       "174        glhr00/rgbt-driveability-segmentation-kaist  1340078   1340078   \n",
       "175                                 llkdev/gender-data   231235    231235   \n",
       "176                                 chenykfrank/immc21  1135726   1135726   \n",
       "177                                  gremmn/gas-prices  1108372   1108372   \n",
       "178  theeranartmeesathien/covid-dataset-for-prediction  1392213   1392213   \n",
       "\n",
       "                              datasetSlug             ownerUser  \\\n",
       "0                      coronavirusdataset              kimjihoo   \n",
       "1                  air-pollution-in-seoul              bappekim   \n",
       "2                    south-korea-visitors              bappekim   \n",
       "3                korea-income-and-welfare              hongsean   \n",
       "4    korean-single-speaker-speech-dataset             bryanpark   \n",
       "..                                    ...                   ...   \n",
       "174  rgbt-driveability-segmentation-kaist                glhr00   \n",
       "175                           gender-data                llkdev   \n",
       "176                                immc21           chenykfrank   \n",
       "177                            gas-prices                gremmn   \n",
       "178          covid-dataset-for-prediction  theeranartmeesathien   \n",
       "\n",
       "     usabilityRating totalViews totalVotes totalDownloads  \\\n",
       "0           1.000000     491478       1469          83975   \n",
       "1           1.000000      81401        311           9920   \n",
       "2           1.000000       5879         24           1048   \n",
       "3           0.970588       3060         15            590   \n",
       "4           0.750000      83614        108           5583   \n",
       "..               ...        ...        ...            ...   \n",
       "174         0.562500         46          0              1   \n",
       "175         0.125000        341          0              8   \n",
       "176         0.117647         45          0              2   \n",
       "177         0.117647        171          0             16   \n",
       "178         0.352941         10          0              0   \n",
       "\n",
       "                                               title  ...  \\\n",
       "0    [NeurIPS 2020] Data Science for COVID-19 (DS4C)  ...   \n",
       "1                             Air Pollution in Seoul  ...   \n",
       "2                               South Korea Visitors  ...   \n",
       "3                           Korea Income and Welfare  ...   \n",
       "4               Korean Single Speaker Speech Dataset  ...   \n",
       "..                                               ...  ...   \n",
       "174                  RGB-T driveability segmentation  ...   \n",
       "175                                      Gender Data  ...   \n",
       "176                                           immc21  ...   \n",
       "177                                       gas prices  ...   \n",
       "178                     Covid dataset for prediction  ...   \n",
       "\n",
       "    collaborators_12_role collaborators_12_username collaborators_13_role  \\\n",
       "0                  writer                parkjuhwan                writer   \n",
       "1                     NaN                       NaN                   NaN   \n",
       "2                     NaN                       NaN                   NaN   \n",
       "3                     NaN                       NaN                   NaN   \n",
       "4                     NaN                       NaN                   NaN   \n",
       "..                    ...                       ...                   ...   \n",
       "174                   NaN                       NaN                   NaN   \n",
       "175                   NaN                       NaN                   NaN   \n",
       "176                   NaN                       NaN                   NaN   \n",
       "177                   NaN                       NaN                   NaN   \n",
       "178                   NaN                       NaN                   NaN   \n",
       "\n",
       "    collaborators_13_username collaborators_14_role collaborators_14_username  \\\n",
       "0                 leewoncheol                writer                 younajung   \n",
       "1                         NaN                   NaN                       NaN   \n",
       "2                         NaN                   NaN                       NaN   \n",
       "3                         NaN                   NaN                       NaN   \n",
       "4                         NaN                   NaN                       NaN   \n",
       "..                        ...                   ...                       ...   \n",
       "174                       NaN                   NaN                       NaN   \n",
       "175                       NaN                   NaN                       NaN   \n",
       "176                       NaN                   NaN                       NaN   \n",
       "177                       NaN                   NaN                       NaN   \n",
       "178                       NaN                   NaN                       NaN   \n",
       "\n",
       "    collaborators_15_role collaborators_15_username collaborators_16_role  \\\n",
       "0                  writer                   minty99                writer   \n",
       "1                     NaN                       NaN                   NaN   \n",
       "2                     NaN                       NaN                   NaN   \n",
       "3                     NaN                       NaN                   NaN   \n",
       "4                     NaN                       NaN                   NaN   \n",
       "..                    ...                       ...                   ...   \n",
       "174                   NaN                       NaN                   NaN   \n",
       "175                   NaN                       NaN                   NaN   \n",
       "176                   NaN                       NaN                   NaN   \n",
       "177                   NaN                       NaN                   NaN   \n",
       "178                   NaN                       NaN                   NaN   \n",
       "\n",
       "    collaborators_16_username  \n",
       "0                  byeongukyu  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3                         NaN  \n",
       "4                         NaN  \n",
       "..                        ...  \n",
       "174                       NaN  \n",
       "175                       NaN  \n",
       "176                       NaN  \n",
       "177                       NaN  \n",
       "178                       NaN  \n",
       "\n",
       "[179 rows x 56 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dict[('korea', 'datasets')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine results of initial and metadata queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict (dict): dictionary of search output results\n",
    "    - metadata_dict (dict): dictionary of metadata results\n",
    "    - on (str/list-like): column name(s) to merge the two dicts on\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/kaggle/kaggle_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    Returns:\n",
    "    - df_dict (OrderedDict): OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "\n",
    "    # Ensure the on variable is proper\n",
    "    try:\n",
    "        assert len(on) == 2 or isinstance(on, str)\n",
    "        if (len(on) == 2) and (not isinstance(on, str)):\n",
    "            left_on, right_on = on\n",
    "            on = None\n",
    "    except:\n",
    "        raise ValueError('Incorrect value of \"on\" passed')\n",
    "        \n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        #Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        if on: # only one value to merge on\n",
    "            df_all = pd.merge(search_df, metadata_df, on=on, how='inner')\n",
    "        else:\n",
    "            df_all = pd.merge(search_df, metadata_df, left_on=left_on, right_on=right_on)\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'figshare')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure kaggle directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError('Save type must be bool or str')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### need to clarify left_on, right_on and if can be incorporated into function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_on = ['ref', 'title', 'usabilityRating', 'downloadCount', 'title']\n",
    "right_on = ['id', 'title', 'usabilityRating', 'totalDownloads', 'title']\n",
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = search_output_dict[('korea', 'datasets')]\n",
    "metadata_df = metadata_dict[('korea', 'datasets')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### specify differences between next two datasets - looks like initial search df then full metadata search df (result of merge function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>size</th>\n",
       "      <th>lastUpdated</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>usabilityRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>7MB</td>\n",
       "      <td>2020-07-13 14:07:31</td>\n",
       "      <td>83975</td>\n",
       "      <td>1469</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>20MB</td>\n",
       "      <td>2020-04-03 16:33:49</td>\n",
       "      <td>9920</td>\n",
       "      <td>311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>99KB</td>\n",
       "      <td>2020-06-04 08:53:36</td>\n",
       "      <td>1048</td>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>772KB</td>\n",
       "      <td>2020-12-20 13:05:27</td>\n",
       "      <td>590</td>\n",
       "      <td>15</td>\n",
       "      <td>0.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>3GB</td>\n",
       "      <td>2020-03-15 08:56:42</td>\n",
       "      <td>5583</td>\n",
       "      <td>108</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id  \\\n",
       "0                     kimjihoo/coronavirusdataset   \n",
       "1                 bappekim/air-pollution-in-seoul   \n",
       "2                   bappekim/south-korea-visitors   \n",
       "3               hongsean/korea-income-and-welfare   \n",
       "4  bryanpark/korean-single-speaker-speech-dataset   \n",
       "\n",
       "                                             title   size  \\\n",
       "0  [NeurIPS 2020] Data Science for COVID-19 (DS4C)    7MB   \n",
       "1                           Air Pollution in Seoul   20MB   \n",
       "2                             South Korea Visitors   99KB   \n",
       "3                         Korea Income and Welfare  772KB   \n",
       "4             Korean Single Speaker Speech Dataset    3GB   \n",
       "\n",
       "           lastUpdated  totalDownloads  totalVotes  usabilityRating  \n",
       "0  2020-07-13 14:07:31           83975        1469         1.000000  \n",
       "1  2020-04-03 16:33:49            9920         311         1.000000  \n",
       "2  2020-06-04 08:53:36            1048          24         1.000000  \n",
       "3  2020-12-20 13:05:27             590          15         0.970588  \n",
       "4  2020-03-15 08:56:42            5583         108         0.750000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_no</th>\n",
       "      <th>datasetId</th>\n",
       "      <th>datasetSlug</th>\n",
       "      <th>ownerUser</th>\n",
       "      <th>usabilityRating</th>\n",
       "      <th>totalViews</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>collaborators_12_role</th>\n",
       "      <th>collaborators_12_username</th>\n",
       "      <th>collaborators_13_role</th>\n",
       "      <th>collaborators_13_username</th>\n",
       "      <th>collaborators_14_role</th>\n",
       "      <th>collaborators_14_username</th>\n",
       "      <th>collaborators_15_role</th>\n",
       "      <th>collaborators_15_username</th>\n",
       "      <th>collaborators_16_role</th>\n",
       "      <th>collaborators_16_username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>527325</td>\n",
       "      <td>527325</td>\n",
       "      <td>coronavirusdataset</td>\n",
       "      <td>kimjihoo</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>491478</td>\n",
       "      <td>1469</td>\n",
       "      <td>83975</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>...</td>\n",
       "      <td>writer</td>\n",
       "      <td>parkjuhwan</td>\n",
       "      <td>writer</td>\n",
       "      <td>leewoncheol</td>\n",
       "      <td>writer</td>\n",
       "      <td>younajung</td>\n",
       "      <td>writer</td>\n",
       "      <td>minty99</td>\n",
       "      <td>writer</td>\n",
       "      <td>byeongukyu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>576393</td>\n",
       "      <td>576393</td>\n",
       "      <td>air-pollution-in-seoul</td>\n",
       "      <td>bappekim</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81401</td>\n",
       "      <td>311</td>\n",
       "      <td>9920</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>692628</td>\n",
       "      <td>692628</td>\n",
       "      <td>south-korea-visitors</td>\n",
       "      <td>bappekim</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5879</td>\n",
       "      <td>24</td>\n",
       "      <td>1048</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>1046735</td>\n",
       "      <td>1046735</td>\n",
       "      <td>korea-income-and-welfare</td>\n",
       "      <td>hongsean</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>3060</td>\n",
       "      <td>15</td>\n",
       "      <td>590</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>19829</td>\n",
       "      <td>19829</td>\n",
       "      <td>korean-single-speaker-speech-dataset</td>\n",
       "      <td>bryanpark</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>83614</td>\n",
       "      <td>108</td>\n",
       "      <td>5583</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id    id_no datasetId  \\\n",
       "0                     kimjihoo/coronavirusdataset   527325    527325   \n",
       "1                 bappekim/air-pollution-in-seoul   576393    576393   \n",
       "2                   bappekim/south-korea-visitors   692628    692628   \n",
       "3               hongsean/korea-income-and-welfare  1046735   1046735   \n",
       "4  bryanpark/korean-single-speaker-speech-dataset    19829     19829   \n",
       "\n",
       "                            datasetSlug  ownerUser  usabilityRating  \\\n",
       "0                    coronavirusdataset   kimjihoo         1.000000   \n",
       "1                air-pollution-in-seoul   bappekim         1.000000   \n",
       "2                  south-korea-visitors   bappekim         1.000000   \n",
       "3              korea-income-and-welfare   hongsean         0.970588   \n",
       "4  korean-single-speaker-speech-dataset  bryanpark         0.750000   \n",
       "\n",
       "  totalViews totalVotes totalDownloads  \\\n",
       "0     491478       1469          83975   \n",
       "1      81401        311           9920   \n",
       "2       5879         24           1048   \n",
       "3       3060         15            590   \n",
       "4      83614        108           5583   \n",
       "\n",
       "                                             title  ... collaborators_12_role  \\\n",
       "0  [NeurIPS 2020] Data Science for COVID-19 (DS4C)  ...                writer   \n",
       "1                           Air Pollution in Seoul  ...                   NaN   \n",
       "2                             South Korea Visitors  ...                   NaN   \n",
       "3                         Korea Income and Welfare  ...                   NaN   \n",
       "4             Korean Single Speaker Speech Dataset  ...                   NaN   \n",
       "\n",
       "  collaborators_12_username collaborators_13_role collaborators_13_username  \\\n",
       "0                parkjuhwan                writer               leewoncheol   \n",
       "1                       NaN                   NaN                       NaN   \n",
       "2                       NaN                   NaN                       NaN   \n",
       "3                       NaN                   NaN                       NaN   \n",
       "4                       NaN                   NaN                       NaN   \n",
       "\n",
       "  collaborators_14_role collaborators_14_username collaborators_15_role  \\\n",
       "0                writer                 younajung                writer   \n",
       "1                   NaN                       NaN                   NaN   \n",
       "2                   NaN                       NaN                   NaN   \n",
       "3                   NaN                       NaN                   NaN   \n",
       "4                   NaN                       NaN                   NaN   \n",
       "\n",
       "  collaborators_15_username collaborators_16_role collaborators_16_username  \n",
       "0                   minty99                writer                byeongukyu  \n",
       "1                       NaN                   NaN                       NaN  \n",
       "2                       NaN                   NaN                       NaN  \n",
       "3                       NaN                   NaN                       NaN  \n",
       "4                       NaN                   NaN                       NaN  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
