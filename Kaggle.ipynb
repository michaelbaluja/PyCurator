{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (1.5.12)\r\n",
      "Requirement already satisfied: requests in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.25.1)\r\n",
      "Requirement already satisfied: six>=1.10 in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.15.0)\r\n",
      "Requirement already satisfied: python-dateutil in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2.8.1)\r\n",
      "Requirement already satisfied: certifi in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (2020.12.5)\r\n",
      "Requirement already satisfied: urllib3 in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (1.26.4)\r\n",
      "Requirement already satisfied: tqdm in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (4.59.0)\r\n",
      "Requirement already satisfied: python-slugify in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (from kaggle) (5.0.2)\r\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages (from requests->kaggle) (4.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "# Installs the kaggle library\n",
    "!pip3 install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlike other API notebooks, no need to import a .py file with API token info\n",
    "#for kaggle, lives locally in ~/.kaggle/kaggle.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the Kaggle API, sign up for a Kaggle account at https://www.kaggle.com. Then go to the 'Account' tab of your user profile (https://www.kaggle.com/<username>/account) and select 'Create API Token'. This will trigger the download of kaggle.json, a file containing your API credentials. Place this file in the location ~/.kaggle/kaggle.json (on Windows in the location C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json - you can check the exact location, sans drive, with echo %HOMEPATH%). You can define a shell environment variable KAGGLE_CONFIG_DIR to change this location to $KAGGLE_CONFIG_DIR/kaggle.json (on Windows it will be %KAGGLE_CONFIG_DIR%\\kaggle.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                            deadline             category       reward  teamCount  userHasEntered  \r\n",
      "---------------------------------------------  -------------------  ----------  ---------  ---------  --------------  \r\n",
      "hhp                                            2013-04-04 07:00:00  Featured     $500,000       1350           False  \r\n",
      "datasciencebowl                                2015-03-16 23:59:00  Featured     $175,000       1049           False  \r\n",
      "stanford-covid-vaccine                         2020-10-06 23:59:00  Research      $25,000       1636           False  \r\n",
      "hubmap-kidney-segmentation                     2021-05-10 23:59:00  Research      $60,000       1200           False  \r\n",
      "osic-pulmonary-fibrosis-progression            2020-10-06 23:59:00  Featured      $55,000       2097           False  \r\n",
      "histopathologic-cancer-detection               2019-03-30 23:59:00  Playground  Knowledge       1149           False  \r\n",
      "rsna-str-pulmonary-embolism-detection          2020-10-26 23:59:00  Featured      $30,000        784           False  \r\n",
      "melbourne-university-seizure-prediction        2016-12-01 23:59:00  Research      $20,000        477           False  \r\n",
      "covid19-global-forecasting-week-1              2020-03-25 23:59:00  Research    Knowledge        545           False  \r\n",
      "aptos2019-blindness-detection                  2019-09-07 23:59:00  Featured      $50,000       2928           False  \r\n",
      "lish-moa                                       2020-11-30 23:59:00  Research      $30,000       4373           False  \r\n",
      "ultrasound-nerve-segmentation                  2016-08-18 23:59:00  Featured     $100,000        922           False  \r\n",
      "vinbigdata-chest-xray-abnormalities-detection  2021-03-30 23:59:00  Featured      $50,000       1275           False  \r\n",
      "prostate-cancer-grade-assessment               2020-07-22 23:59:00  Featured      $25,000       1010           False  \r\n",
      "trends-assessment-prediction                   2020-06-29 23:59:00  Research      $25,000       1047           False  \r\n",
      "covid19-global-forecasting-week-4              2020-04-15 23:59:00  Research    Knowledge        472           False  \r\n",
      "covid19-global-forecasting-week-3              2020-04-08 23:59:00  Research    Knowledge        452           False  \r\n",
      "covid19-global-forecasting-week-2              2020-04-06 03:56:00  Research    Knowledge        215           False  \r\n",
      "instacart-market-basket-analysis               2017-08-14 23:59:00  Featured      $25,000       2621           False  \r\n",
      "covid19-local-us-ca-forecasting-week-1         2020-03-25 23:59:00  Research    Knowledge        190           False  \r\n"
     ]
    }
   ],
   "source": [
    "#there are Pythonic options, but seems easiest to just run terminal commands from within notebook\n",
    "\n",
    "#example:\n",
    "! kaggle competitions list -s health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                                   title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \r\n",
      "----------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \r\n",
      "kaggle/kaggle-survey-2018                             2018 Kaggle Machine Learning & Data Science Survey    4MB  2018-11-03 22:35:07          15168        974  0.85294116       \r\n",
      "kaggle/kaggle-survey-2017                             2017 Kaggle Machine Learning & Data Science Survey    4MB  2017-10-27 22:03:03          22736        826  0.8235294        \r\n",
      "alopez247/pokemon                                     Pok√©mon for Data Mining and Machine Learning        715KB  2017-03-05 15:01:26          10366        239  0.85294116       \r\n",
      "kashnitsky/mlcourse                                   mlcourse.ai                                          51MB  2018-12-09 16:45:09          24694       1359  0.88235295       \r\n",
      "kaushil268/disease-prediction-using-machine-learning  Disease Prediction Using Machine Learning            30KB  2020-05-15 03:58:44           2131         38  0.8235294        \r\n",
      "henrysue/online-shoppers-intention                    Online Shoppers Intention UCI Machine Learning      252KB  2020-01-15 20:19:02           1116         30  1.0              \r\n",
      "rikdifos/credit-card-approval-prediction              Credit Card Approval Prediction                       5MB  2020-03-24 10:04:48          14212        300  1.0              \r\n",
      "cristeaioan/ffml-dataset                              Food For Machine Learning                            64MB  2020-04-24 09:10:45            697         18  0.8125           \r\n",
      "saxenapriyansh/coursera-machine-learning-andrew-ng    Coursera - Machine Learning - Andrew_Ng               4KB  2018-12-25 12:27:33            407         19  0.875            \r\n",
      "shuofxz/titanic-machine-learning-from-disaster        Titanic: Machine Learning from Disaster              33KB  2017-10-15 10:05:34           2575         41  0.29411766       \r\n",
      "vikrishnan/boston-house-prices                        Boston House Prices                                  13KB  2017-08-03 17:06:12          37687        312  0.8235294        \r\n",
      "uciml/zoo-animal-classification                       Zoo Animal Classification                             2KB  2016-12-24 18:05:10          22737        300  0.8235294        \r\n",
      "rahulsah06/machine-learning-for-diabetes-with-python  Machine Learning for Diabetes with Python             9KB  2019-09-20 08:04:12            817         20  0.5882353        \r\n",
      "mlg-ulb/creditcardfraud                               Credit Card Fraud Detection                          66MB  2018-03-23 01:17:27         328694       8042  0.85294116       \r\n",
      "oossiiris/hackerearth-machine-learning-exhibit-art    HackerEarth Machine Learning  : Exhibit A(rt)       512KB  2021-02-06 07:36:56             66         12  0.7058824        \r\n",
      "gauravsahani/andrewng-machine-learning-tweets         AndrewNg Machine Learning Tweets                    124KB  2020-06-06 13:00:45             46         13  0.7058824        \r\n",
      "sarahvch/breast-cancer-wisconsin-prognostic-data-set  Breast Cancer Wisconsin (Prognostic) Data Set        49KB  2017-03-31 22:47:50           2463         42  0.8235294        \r\n",
      "brsdincer/vehicle-detection-image-set                 Vehicle Detection Image Set                         119MB  2021-05-09 17:54:02            392         33  0.875            \r\n",
      "mirichoi0218/insurance                                 Medical Cost Personal Datasets                      16KB  2018-02-21 00:15:14          77543       1341  0.88235295       \r\n",
      "uciml/mushroom-classification                         Mushroom Classification                              34KB  2016-12-01 23:08:00          69372       1662  0.85294116       \r\n"
     ]
    }
   ],
   "source": [
    "#search datasets for term\n",
    "! kaggle datasets list -s \"machine learning\"\n",
    "\n",
    "#this returns by default one page of results at a time (matches manual search of datasets page 1)\n",
    "#need to extract \"ref\" variable, which is <owner>/<dataset-name>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching datasets is a start, but we'll also want to search notebooks, competitions, etc\n",
    "#looks like can search datasets and kernels\n",
    "#and get related files for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                         size  creationDate         \r\n",
      "---------------------------  ----  -------------------  \r\n",
      "SurveySchema.csv              8KB  2018-11-03 22:35:07  \r\n",
      "freeFormResponses.csv         1MB  2018-11-03 22:35:07  \r\n",
      "multipleChoiceResponses.csv  39MB  2018-11-03 22:35:07  \r\n"
     ]
    }
   ],
   "source": [
    "#once get list of files in format <owner>/<dataset-name>, can get list of files\n",
    "#example:\n",
    "! kaggle datasets files kaggle/kaggle-survey-2018 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/michaelbaluja/opt/anaconda3/bin/kaggle\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages/kaggle/cli.py\", line 67, in main\r\n",
      "    out = args.func(**command_args)\r\n",
      "  File \"/Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages/kaggle/api/kaggle_api_extended.py\", line 1065, in dataset_metadata_cli\r\n",
      "    meta_file = self.dataset_metadata(dataset, path)\r\n",
      "  File \"/Users/michaelbaluja/opt/anaconda3/lib/python3.8/site-packages/kaggle/api/kaggle_api_extended.py\", line 1043, in dataset_metadata\r\n",
      "    os.makedirs(effective_path)\r\n",
      "  File \"/Users/michaelbaluja/opt/anaconda3/lib/python3.8/os.py\", line 223, in makedirs\r\n",
      "    mkdir(name, mode)\r\n",
      "OSError: [Errno 30] Read-only file system: '/Kaggle_results/'\r\n"
     ]
    }
   ],
   "source": [
    "#can get metadata for a dataset\n",
    "#default downloads into current working directory\n",
    "#need a way to store in memory and iterate for all files\n",
    "! kaggle datasets metadata -p /Kaggle_results/ kaggle/kaggle-survey-2018 \n",
    "\n",
    "#this currently barfs, not sure why - something with where putting output,b ut not sure why\n",
    "#TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonic-ish Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess # Used to run unix commands\n",
    "import pandas as pd # For storing/manipulating command data\n",
    "from io import StringIO # Lets us read csv string output from command into DataFrame\n",
    "import json # Reading back the metadata files\n",
    "from tqdm import tqdm # Gives status bar on loop completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not depracated, but not needed, but spent time to write so saving in case needed later\n",
    "def extract_dataset_paths(output):\n",
    "    '''\n",
    "    Takes the output from our kaggle run command and returns the dataset paths\n",
    "    Params:\n",
    "    - output (str): command output to be parsed\n",
    "    Returns:\n",
    "    - dataset_paths (list): list of dataset paths present in output\n",
    "    '''\n",
    "    # Remove newline characters\n",
    "    output = output.replace('\\n', '')\n",
    "    \n",
    "    # Extracts all instances of output that contain a '/' character\n",
    "    # (/ used exclusively in dataset names at time of writing)\n",
    "    dataset_paths = [out for out in output.split(' ') if '/' in out]\n",
    "    return dataset_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting/extracting dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_output(search_terms, search_type):\n",
    "    '''\n",
    "    Calls the Kaggle API with the specified query terms and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_terms (str/list-like): string or list of strings that should be searched for\n",
    "    - search_type (str): objects to search over (must be either datasets or kernels)\n",
    "    \n",
    "    Returns:\n",
    "    - cumulative_output (str): output from all searches\n",
    "    '''\n",
    "    # Make sure our input is valid\n",
    "    assert len(search_terms) > 0, 'Please enter non-empty search terms'\n",
    "    assert search_type in ('datasets', 'kernels'), 'Search can only be conducted over datasets or kernels'\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(search_terms) == str:\n",
    "        search_terms = [search_terms]\n",
    "    \n",
    "    # Set search parameters\n",
    "    page_idx = 1\n",
    "    search_output = ''\n",
    "    cumulative_output = ''\n",
    "    completion_phrase = f'No {search_type} found\\n'\n",
    "    \n",
    "    # Search for each term\n",
    "    for search_term in tqdm(search_terms):\n",
    "        # Pulls the records for a single page of datasets for the given search term\n",
    "        # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "        search_output = subprocess.run(['kaggle', search_type, 'list', '-v',\n",
    "                                         '-s', f'\"{search_term}\"', \n",
    "                                         '-p', str(page_idx)], \n",
    "                                        capture_output=True).stdout.decode()\n",
    "        # Once we no longer see new output, we stop\n",
    "        while search_output != completion_phrase:\n",
    "            # Accumulate the output\n",
    "            cumulative_output = cumulative_output + search_output\n",
    "\n",
    "            # Increments the page count for searching\n",
    "            page_idx += 1\n",
    "            \n",
    "            # Pulls the records for a single page of datasets for the given search term\n",
    "            # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "            search_output = subprocess.run(['kaggle', search_type, 'list', '-v',\n",
    "                                             '-s', f'\"{search_term}\"', \n",
    "                                             '-p', str(page_idx)], \n",
    "                                            capture_output=True).stdout.decode()\n",
    "        \n",
    "    return cumulative_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_csv_output_to_dataframe(output):\n",
    "    '''\n",
    "    Given a string variable in csv format, returns a Pandas DataFrame\n",
    "    \n",
    "    Params:\n",
    "    - output (str): csv-styled string to be converted\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame consisting of data from 'output' string variable\n",
    "    '''\n",
    "    # Create DataFrame of results\n",
    "    output = StringIO(output)\n",
    "    df = pd.read_csv(output)\n",
    "    \n",
    "    # Every page of results will append a row of header values to the DataFrame, so we remove them\n",
    "    df = df[df.ref != 'ref'].reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['svm']\n",
    "search_type = 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.10s/it]\n"
     ]
    }
   ],
   "source": [
    "search_output = get_search_output(search_terms, search_type)\n",
    "search_output_df = convert_string_csv_output_to_dataframe(search_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>title</th>\n",
       "      <th>size</th>\n",
       "      <th>lastUpdated</th>\n",
       "      <th>downloadCount</th>\n",
       "      <th>voteCount</th>\n",
       "      <th>usabilityRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinod00725/svm-classification</td>\n",
       "      <td>SVM Classification</td>\n",
       "      <td>72KB</td>\n",
       "      <td>2019-06-28 11:02:07</td>\n",
       "      <td>1125</td>\n",
       "      <td>20</td>\n",
       "      <td>0.29411766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nishan192/letterrecognition-using-svm</td>\n",
       "      <td>Letter-Recognition</td>\n",
       "      <td>214KB</td>\n",
       "      <td>2019-04-13 05:42:06</td>\n",
       "      <td>1191</td>\n",
       "      <td>15</td>\n",
       "      <td>0.64705884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sarahvch/predicting-who-pays-back-loans</td>\n",
       "      <td>Predicting Who Pays Back Loans</td>\n",
       "      <td>80MB</td>\n",
       "      <td>2017-03-02 21:07:38</td>\n",
       "      <td>1715</td>\n",
       "      <td>18</td>\n",
       "      <td>0.8235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hugomathien/soccer</td>\n",
       "      <td>European Soccer Database</td>\n",
       "      <td>33MB</td>\n",
       "      <td>2016-10-23 22:31:38</td>\n",
       "      <td>133155</td>\n",
       "      <td>3442</td>\n",
       "      <td>0.7058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elikplim/forest-fires-data-set</td>\n",
       "      <td>Forest Fires Data Set</td>\n",
       "      <td>7KB</td>\n",
       "      <td>2017-09-04 04:08:24</td>\n",
       "      <td>5082</td>\n",
       "      <td>72</td>\n",
       "      <td>0.64705884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>suyiyang/nb-svm</td>\n",
       "      <td>NB_SVM</td>\n",
       "      <td>10MB</td>\n",
       "      <td>2018-02-26 19:24:26</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>nishithasaravanan/digit-svm</td>\n",
       "      <td>Digit_svm</td>\n",
       "      <td>9MB</td>\n",
       "      <td>2021-03-11 16:33:02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.11764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>bharathmukka/segmentation-svm</td>\n",
       "      <td>Segmentation_SVM</td>\n",
       "      <td>94KB</td>\n",
       "      <td>2020-06-08 16:16:33</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>lilylc/gesture-mod</td>\n",
       "      <td>Gesture_mod</td>\n",
       "      <td>6MB</td>\n",
       "      <td>2020-10-30 14:59:57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>hariwh0/outlier-detection-datasets</td>\n",
       "      <td>Outlier Datasets - original</td>\n",
       "      <td>1GB</td>\n",
       "      <td>2021-02-05 03:39:30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29411766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ref                           title  \\\n",
       "0              vinod00725/svm-classification              SVM Classification   \n",
       "1      nishan192/letterrecognition-using-svm             Letter-Recognition    \n",
       "2    sarahvch/predicting-who-pays-back-loans  Predicting Who Pays Back Loans   \n",
       "3                         hugomathien/soccer        European Soccer Database   \n",
       "4             elikplim/forest-fires-data-set           Forest Fires Data Set   \n",
       "..                                       ...                             ...   \n",
       "106                          suyiyang/nb-svm                          NB_SVM   \n",
       "107              nishithasaravanan/digit-svm                       Digit_svm   \n",
       "108            bharathmukka/segmentation-svm                Segmentation_SVM   \n",
       "109                       lilylc/gesture-mod                     Gesture_mod   \n",
       "110       hariwh0/outlier-detection-datasets     Outlier Datasets - original   \n",
       "\n",
       "      size          lastUpdated downloadCount voteCount usabilityRating  \n",
       "0     72KB  2019-06-28 11:02:07          1125        20      0.29411766  \n",
       "1    214KB  2019-04-13 05:42:06          1191        15      0.64705884  \n",
       "2     80MB  2017-03-02 21:07:38          1715        18       0.8235294  \n",
       "3     33MB  2016-10-23 22:31:38        133155      3442       0.7058824  \n",
       "4      7KB  2017-09-04 04:08:24          5082        72      0.64705884  \n",
       "..     ...                  ...           ...       ...             ...  \n",
       "106   10MB  2018-02-26 19:24:26            32         0      0.23529412  \n",
       "107    9MB  2021-03-11 16:33:02             1         0      0.11764706  \n",
       "108   94KB  2020-06-08 16:16:33             6         0      0.23529412  \n",
       "109    6MB  2020-10-30 14:59:57             0         0           0.125  \n",
       "110    1GB  2021-02-05 03:39:30             2         0      0.29411766  \n",
       "\n",
       "[111 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling dataset metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Unable to find a way to store metadata in memory as opposed to saving file, but this workaround appears to be functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_metadata_json(dataset_path):\n",
    "    '''\n",
    "    Queries Kaggle for metadata json file & returns the json data as a dictionary\n",
    "    \n",
    "    Params:\n",
    "    - dataset_path (str): path for the dataset\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_dict (dict): dictionary containing json metadata\n",
    "    '''\n",
    "    # Download the metadata\n",
    "    subprocess.run(['kaggle', 'datasets', 'metadata', dataset_path])\n",
    "\n",
    "    # Access the metadata and load it in as a dictionary\n",
    "    with open('dataset-metadata.json') as file:\n",
    "        json_data = json.load(file)\n",
    "        \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(dataset_paths):\n",
    "    '''\n",
    "    Retrieves the metadata for the file/files listed in dataset_paths\n",
    "    \n",
    "    Params:\n",
    "    - dataset_paths (str/list-like): string or list of strings containing the paths for the datasets\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df (pandas.DataFrame): DataFrame containing metadata for the requested datasets\n",
    "    '''\n",
    "    # Make sure our input is valid\n",
    "    assert len(dataset_paths) > 0, 'Please enter at least one dataset path'\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(dataset_paths) == str:\n",
    "        dataset_paths = [dataset_paths]\n",
    "        \n",
    "    # Run first query\n",
    "    json_data = _retrieve_metadata_json(dataset_paths[0])\n",
    "        \n",
    "    # Create DataFrame to store metadata in, using columns found in first query, and then add query info\n",
    "    metadata_df = pd.DataFrame(columns=json_data.keys(), dtype=object)\n",
    "    metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    # Pulls metadata information for each dataset found above\n",
    "    for dataset_path in tqdm(dataset_paths[1::]):\n",
    "        # Download & load the metadata\n",
    "        json_data = _retrieve_metadata_json(dataset_path)\n",
    "\n",
    "        # Store the metadata into our DataFrame created above\n",
    "        metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110/110 [02:04<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_paths = search_output_df.ref.values\n",
    "metadata_df = get_metadata(dataset_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_no</th>\n",
       "      <th>datasetId</th>\n",
       "      <th>datasetSlug</th>\n",
       "      <th>ownerUser</th>\n",
       "      <th>usabilityRating</th>\n",
       "      <th>totalViews</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>description</th>\n",
       "      <th>isPrivate</th>\n",
       "      <th>keywords</th>\n",
       "      <th>licenses</th>\n",
       "      <th>collaborators</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinod00725/svm-classification</td>\n",
       "      <td>249595</td>\n",
       "      <td>249595</td>\n",
       "      <td>svm-classification</td>\n",
       "      <td>vinod00725</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>6520</td>\n",
       "      <td>20</td>\n",
       "      <td>1125</td>\n",
       "      <td>SVM Classification</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[computer science]</td>\n",
       "      <td>[{'name': 'unknown'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nishan192/letterrecognition-using-svm</td>\n",
       "      <td>164400</td>\n",
       "      <td>164400</td>\n",
       "      <td>letterrecognition-using-svm</td>\n",
       "      <td>nishan192</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>10733</td>\n",
       "      <td>15</td>\n",
       "      <td>1191</td>\n",
       "      <td>Letter-Recognition</td>\n",
       "      <td>Letter Recognition using SVM</td>\n",
       "      <td>To solve an interesting letter recognition pro...</td>\n",
       "      <td>False</td>\n",
       "      <td>[computer science, image data]</td>\n",
       "      <td>[{'name': 'unknown'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sarahvch/predicting-who-pays-back-loans</td>\n",
       "      <td>906</td>\n",
       "      <td>906</td>\n",
       "      <td>predicting-who-pays-back-loans</td>\n",
       "      <td>sarahvch</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>13420</td>\n",
       "      <td>18</td>\n",
       "      <td>1715</td>\n",
       "      <td>Predicting Who Pays Back Loans</td>\n",
       "      <td>Prediction using SVM on Lending Club data 2007...</td>\n",
       "      <td># Context \\nThe data being used for this analy...</td>\n",
       "      <td>False</td>\n",
       "      <td>[lending]</td>\n",
       "      <td>[{'name': 'CC0-1.0'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hugomathien/soccer</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>soccer</td>\n",
       "      <td>hugomathien</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>1078997</td>\n",
       "      <td>3442</td>\n",
       "      <td>133155</td>\n",
       "      <td>European Soccer Database</td>\n",
       "      <td>25k+ matches, players &amp; teams attributes for E...</td>\n",
       "      <td>The ultimate Soccer database for data analysis...</td>\n",
       "      <td>False</td>\n",
       "      <td>[games, video games, football, europe, sports,...</td>\n",
       "      <td>[{'name': 'ODbL-1.0'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elikplim/forest-fires-data-set</td>\n",
       "      <td>2323</td>\n",
       "      <td>2323</td>\n",
       "      <td>forest-fires-data-set</td>\n",
       "      <td>elikplim</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>62871</td>\n",
       "      <td>72</td>\n",
       "      <td>5082</td>\n",
       "      <td>Forest Fires Data Set</td>\n",
       "      <td>predict the burned area of forest fires using ...</td>\n",
       "      <td>Source: https://archive.ics.uci.edu/ml/dataset...</td>\n",
       "      <td>False</td>\n",
       "      <td>[earth and nature, earth science, business]</td>\n",
       "      <td>[{'name': 'unknown'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>suyiyang/nb-svm</td>\n",
       "      <td>14242</td>\n",
       "      <td>14242</td>\n",
       "      <td>nb-svm</td>\n",
       "      <td>suyiyang</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>742</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>NB_SVM</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'name': 'DbCL-1.0'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>nishithasaravanan/digit-svm</td>\n",
       "      <td>1205518</td>\n",
       "      <td>1205518</td>\n",
       "      <td>digit-svm</td>\n",
       "      <td>nishithasaravanan</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Digit_svm</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'name': 'unknown'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>bharathmukka/segmentation-svm</td>\n",
       "      <td>701586</td>\n",
       "      <td>701586</td>\n",
       "      <td>segmentation-svm</td>\n",
       "      <td>bharathmukka</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Segmentation_SVM</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'name': 'CC0-1.0'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>lilylc/gesture-mod</td>\n",
       "      <td>947411</td>\n",
       "      <td>947411</td>\n",
       "      <td>gesture-mod</td>\n",
       "      <td>lilylc</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gesture_mod</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'name': 'unknown'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>hariwh0/outlier-detection-datasets</td>\n",
       "      <td>1139102</td>\n",
       "      <td>1139102</td>\n",
       "      <td>outlier-detection-datasets</td>\n",
       "      <td>hariwh0</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Outlier Datasets - original</td>\n",
       "      <td>for Outlier Detection</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'name': 'DbCL-1.0'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id    id_no datasetId  \\\n",
       "0              vinod00725/svm-classification   249595    249595   \n",
       "1      nishan192/letterrecognition-using-svm   164400    164400   \n",
       "2    sarahvch/predicting-who-pays-back-loans      906       906   \n",
       "3                         hugomathien/soccer       63        63   \n",
       "4             elikplim/forest-fires-data-set     2323      2323   \n",
       "..                                       ...      ...       ...   \n",
       "106                          suyiyang/nb-svm    14242     14242   \n",
       "107              nishithasaravanan/digit-svm  1205518   1205518   \n",
       "108            bharathmukka/segmentation-svm   701586    701586   \n",
       "109                       lilylc/gesture-mod   947411    947411   \n",
       "110       hariwh0/outlier-detection-datasets  1139102   1139102   \n",
       "\n",
       "                        datasetSlug          ownerUser  usabilityRating  \\\n",
       "0                svm-classification         vinod00725         0.294118   \n",
       "1       letterrecognition-using-svm          nishan192         0.647059   \n",
       "2    predicting-who-pays-back-loans           sarahvch         0.823529   \n",
       "3                            soccer        hugomathien         0.705882   \n",
       "4             forest-fires-data-set           elikplim         0.647059   \n",
       "..                              ...                ...              ...   \n",
       "106                          nb-svm           suyiyang         0.235294   \n",
       "107                       digit-svm  nishithasaravanan         0.117647   \n",
       "108                segmentation-svm       bharathmukka         0.235294   \n",
       "109                     gesture-mod             lilylc         0.125000   \n",
       "110      outlier-detection-datasets            hariwh0         0.294118   \n",
       "\n",
       "    totalViews totalVotes totalDownloads                           title  \\\n",
       "0         6520         20           1125              SVM Classification   \n",
       "1        10733         15           1191             Letter-Recognition    \n",
       "2        13420         18           1715  Predicting Who Pays Back Loans   \n",
       "3      1078997       3442         133155        European Soccer Database   \n",
       "4        62871         72           5082           Forest Fires Data Set   \n",
       "..         ...        ...            ...                             ...   \n",
       "106        742          0             32                          NB_SVM   \n",
       "107         24          0              1                       Digit_svm   \n",
       "108        185          0              6                Segmentation_SVM   \n",
       "109        164          0              0                     Gesture_mod   \n",
       "110        113          0              2     Outlier Datasets - original   \n",
       "\n",
       "                                              subtitle  \\\n",
       "0                                                        \n",
       "1                         Letter Recognition using SVM   \n",
       "2    Prediction using SVM on Lending Club data 2007...   \n",
       "3    25k+ matches, players & teams attributes for E...   \n",
       "4    predict the burned area of forest fires using ...   \n",
       "..                                                 ...   \n",
       "106                                                      \n",
       "107                                                      \n",
       "108                                                      \n",
       "109                                                      \n",
       "110                              for Outlier Detection   \n",
       "\n",
       "                                           description isPrivate  \\\n",
       "0                                                          False   \n",
       "1    To solve an interesting letter recognition pro...     False   \n",
       "2    # Context \\nThe data being used for this analy...     False   \n",
       "3    The ultimate Soccer database for data analysis...     False   \n",
       "4    Source: https://archive.ics.uci.edu/ml/dataset...     False   \n",
       "..                                                 ...       ...   \n",
       "106                                                        False   \n",
       "107                                                        False   \n",
       "108                                                        False   \n",
       "109                                                        False   \n",
       "110                                                        False   \n",
       "\n",
       "                                              keywords  \\\n",
       "0                                   [computer science]   \n",
       "1                       [computer science, image data]   \n",
       "2                                            [lending]   \n",
       "3    [games, video games, football, europe, sports,...   \n",
       "4          [earth and nature, earth science, business]   \n",
       "..                                                 ...   \n",
       "106                                                 []   \n",
       "107                                                 []   \n",
       "108                                                 []   \n",
       "109                                                 []   \n",
       "110                                                 []   \n",
       "\n",
       "                   licenses collaborators data  \n",
       "0     [{'name': 'unknown'}]            []   []  \n",
       "1     [{'name': 'unknown'}]            []   []  \n",
       "2     [{'name': 'CC0-1.0'}]            []   []  \n",
       "3    [{'name': 'ODbL-1.0'}]            []   []  \n",
       "4     [{'name': 'unknown'}]            []   []  \n",
       "..                      ...           ...  ...  \n",
       "106  [{'name': 'DbCL-1.0'}]            []   []  \n",
       "107   [{'name': 'unknown'}]            []   []  \n",
       "108   [{'name': 'CC0-1.0'}]            []   []  \n",
       "109   [{'name': 'unknown'}]            []   []  \n",
       "110  [{'name': 'DbCL-1.0'}]            []   []  \n",
       "\n",
       "[111 rows x 17 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
