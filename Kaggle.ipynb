{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the Kaggle API. Follow these steps in order to get the necessary credentials to continue:\n",
    "\n",
    "1. Sign up for a Kaggle account at https://www.kaggle.com.\n",
    "2. Go to the 'Account' tab of your user profile - ```https://www.kaggle.com/{username}/account```.\n",
    "3. Select 'Create New API Token' under 'API' section.\n",
    "    - This will trigger the download of kaggle.json, a file containing your API credentials. \n",
    "4. Place this file in the location:\n",
    "    - ~/.kaggle/kaggle.json (for macOS/unix)\n",
    "    - C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json (for Windows) \n",
    "    - You can check the exact location, sans drive, with echo %HOMEPATH%). \n",
    "    - You can define a shell environment variable KAGGLE_CONFIG_DIR to change this location to:\n",
    "        - $KAGGLE_CONFIG_DIR/kaggle.json (for macOS/unix)\n",
    "        - %KAGGLE_CONFIG_DIR%\\kaggle.json (for Windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Kaggle API ([Kaggle](https://www.kaggle.com/docs/api))\n",
    "- Kaggle API ([GitHub](https://github.com/Kaggle/kaggle-api)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import kaggle, installing if necessary\n",
    "try:\n",
    "    import kaggle\n",
    "except ImportError as e:\n",
    "    !pip3 install kaggle\n",
    "    import kaggle\n",
    "    \n",
    "import subprocess # Used to run unix commands\n",
    "import pandas as pd # For storing/manipulating command data\n",
    "from io import StringIO # Lets us read csv string output from command into DataFrame\n",
    "import json # Reading back the metadata files\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "import os # Exporting saved results\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting/extracting dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_outputs(search_terms, search_types, save=False):\n",
    "    \"\"\"\n",
    "    Call the Kaggle API for each search term and search type. \n",
    "    Results are retured in results['{term}_{type}'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms (list-like): collection of search terms to query over\n",
    "    - search_types (list-like): collection of search types to query over\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/kaggle/kaggle_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain len(search_terms) * len(search_types) strings\n",
    "            For ordering, the queries are done using itertools.product(search_terms, search_types)\n",
    "    \n",
    "    Returns:\n",
    "    - results (dict): dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    num_searches = len(search_terms) * len(search_types)\n",
    "    results = OrderedDict()\n",
    "    \n",
    "    for search_term, search_type in itertools.product(search_terms, search_types):\n",
    "        results[(search_term, search_type)] = get_search_output(search_term, search_type)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_output(search_term, search_type):\n",
    "    \"\"\"\n",
    "    Calls the Kaggle API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term (str): keyword to seach for\n",
    "    - search_type (str): objects to search over (must be either datasets or kernels)\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert isinstance(search_type, str), 'Search term must be a string'\n",
    "    assert search_type in ('datasets', 'kernels'), 'Search can only be conducted over datasets or kernels'\n",
    "    \n",
    "    # Set search variables\n",
    "    page_idx = 1\n",
    "    search_output = ''\n",
    "    cumulative_output = ''\n",
    "    completion_phrase = f'No {search_type} found\\n'\n",
    "    \n",
    "    # Pulls the records for a single page of datasets for the given search term\n",
    "    # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "    search_output = subprocess.run(['kaggle', search_type, 'list', '-v',\n",
    "                                     '-s', f'\"{search_term}\"', \n",
    "                                     '-p', str(page_idx)], \n",
    "                                    capture_output=True).stdout.decode()\n",
    "\n",
    "    # Once we no longer see new output, we stop\n",
    "    while search_output != completion_phrase:\n",
    "        # Accumulate the output\n",
    "        cumulative_output = cumulative_output + search_output\n",
    "\n",
    "        # Increments the page count for searching\n",
    "        page_idx += 1\n",
    "\n",
    "        # Pulls the records for a single page of datasets for the given search term\n",
    "        # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "        search_output = subprocess.run(['kaggle', search_type, 'list', '-v',\n",
    "                                         '-s', f'\"{search_term}\"', \n",
    "                                         '-p', str(page_idx)], \n",
    "                                        capture_output=True).stdout.decode()\n",
    "\n",
    "        # Remove header row\n",
    "        if search_output != completion_phrase:\n",
    "            search_output = '\\r\\n'.join(search_output.split('\\r\\n')[1::])\n",
    "    \n",
    "    # Convert search output to DataFrame\n",
    "    search_df = _convert_string_csv_output_to_dataframe(cumulative_output)\n",
    "    \n",
    "    # Rename columns to match names present in metadata df\n",
    "    search_df.rename(columns={'ref': 'id', \n",
    "                              'downloadCount': 'totalDownloads', \n",
    "                              'voteCount': 'totalVotes'}, \n",
    "                     inplace=True)\n",
    "        \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_string_csv_output_to_dataframe(output):\n",
    "    \"\"\"\n",
    "    Given a string variable in csv format, returns a Pandas DataFrame\n",
    "    \n",
    "    Params:\n",
    "    - output (str): csv-styled string to be converted\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame consisting of data from 'output' string variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create DataFrame of results\n",
    "    output = StringIO(output)\n",
    "    df = pd.read_csv(output)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['korea']\n",
    "search_types = ['datasets', 'kernels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_search_outputs(search_terms, search_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>title</th>\n",
       "      <th>size</th>\n",
       "      <th>lastUpdated</th>\n",
       "      <th>downloadCount</th>\n",
       "      <th>voteCount</th>\n",
       "      <th>usabilityRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>7MB</td>\n",
       "      <td>2020-07-13 14:07:31</td>\n",
       "      <td>83497</td>\n",
       "      <td>1466</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>20MB</td>\n",
       "      <td>2020-04-03 16:33:49</td>\n",
       "      <td>9874</td>\n",
       "      <td>311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>99KB</td>\n",
       "      <td>2020-06-04 08:53:36</td>\n",
       "      <td>1035</td>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>772KB</td>\n",
       "      <td>2020-12-20 13:05:27</td>\n",
       "      <td>582</td>\n",
       "      <td>15</td>\n",
       "      <td>0.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>3GB</td>\n",
       "      <td>2020-03-15 08:56:42</td>\n",
       "      <td>5533</td>\n",
       "      <td>107</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              ref  \\\n",
       "0                     kimjihoo/coronavirusdataset   \n",
       "1                 bappekim/air-pollution-in-seoul   \n",
       "2                   bappekim/south-korea-visitors   \n",
       "3               hongsean/korea-income-and-welfare   \n",
       "4  bryanpark/korean-single-speaker-speech-dataset   \n",
       "\n",
       "                                             title   size  \\\n",
       "0  [NeurIPS 2020] Data Science for COVID-19 (DS4C)    7MB   \n",
       "1                           Air Pollution in Seoul   20MB   \n",
       "2                             South Korea Visitors   99KB   \n",
       "3                         Korea Income and Welfare  772KB   \n",
       "4             Korean Single Speaker Speech Dataset    3GB   \n",
       "\n",
       "           lastUpdated  downloadCount  voteCount  usabilityRating  \n",
       "0  2020-07-13 14:07:31          83497       1466         1.000000  \n",
       "1  2020-04-03 16:33:49           9874        311         1.000000  \n",
       "2  2020-06-04 08:53:36           1035         24         1.000000  \n",
       "3  2020-12-20 13:05:27            582         15         0.970588  \n",
       "4  2020-03-15 08:56:42           5533        107         0.750000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling dataset metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Unable to find a way to store metadata in memory as opposed to saving file, but this workaround appears to be functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_object_json(object_path):\n",
    "    \"\"\"\n",
    "    Queries Kaggle for metadata json file & returns the json data as a dictionary\n",
    "    \n",
    "    Params:\n",
    "    - object_path (str): path for the dataset\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_dict (dict): dictionary containing json metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download the metadata\n",
    "    subprocess.run(['kaggle', 'datasets', 'metadata', object_path])\n",
    "\n",
    "    # Access the metadata and load it in as a dictionary\n",
    "    with open('dataset-metadata.json') as file:\n",
    "        json_data = json.load(file)\n",
    "        \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(object_paths):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the file/files listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths (str/list-like): string or list of strings containing the paths for the objects\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df (pandas.DataFrame): DataFrame containing metadata for the requested datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "        \n",
    "    # Run first query\n",
    "    json_data = _retrieve_object_json(object_paths[0])\n",
    "        \n",
    "    # Create DataFrame to store metadata in, using columns found in first query, and then add query info\n",
    "    metadata_df = pd.DataFrame(columns=json_data.keys(), dtype=object)\n",
    "    metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    # Pulls metadata information for each dataset found above\n",
    "    for object_path in tqdm(object_paths[1::]):\n",
    "        # Download & load the metadata\n",
    "        json_data = _retrieve_object_json(object_path)\n",
    "\n",
    "        # Store the metadata into our DataFrame created above\n",
    "        metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('korea', 'datasets') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [02:47<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('korea', 'kernels') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1019/1019 [33:23<00:00,  1.97s/it]   \n"
     ]
    }
   ],
   "source": [
    "## Extract IDs from DataFrame, and returns as list of strings\n",
    "metadata_dict = OrderedDict()\n",
    "\n",
    "for query, df in search_output_dict.items():\n",
    "    print(f'Retrieving {query} metadata')\n",
    "    # Create object paths\n",
    "    _, search_type = query\n",
    "    object_paths = df.id.values\n",
    "    \n",
    "    metadata_dict[query] = get_metadata(object_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict (dict): dictionary of search output results\n",
    "    - metadata_dict (dict): dictionary of metadata results\n",
    "    - on (str/list-like): column name(s) to merge the two dicts on\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/kaggle/kaggle_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    Returns:\n",
    "    - df_dict (OrderedDict): OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "\n",
    "    # Ensure the on variable is proper\n",
    "    try:\n",
    "        assert len(on) == 2 or isinstance(on, str)\n",
    "        if (len(on) == 2) and (not isinstance(on, str)):\n",
    "            left_on, right_on = on\n",
    "            on = None\n",
    "    except:\n",
    "        raise ValueError('Incorrect value of \"on\" passed')\n",
    "        \n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        #Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        if on: # only one value to merge on\n",
    "            df_all = pd.merge(search_df, metadata_df, on=on, how='inner')\n",
    "        else:\n",
    "            df_all = pd.merge(search_df, metadata_df, left_on=left_on, right_on=right_on)\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'figshare')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure kaggle directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError('Save type must be bool or str')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_on = ['ref', 'title', 'usabilityRating', 'downloadCount', 'title']\n",
    "right_on = ['id', 'title', 'usabilityRating', 'totalDownloads', 'title']\n",
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = search_output_dict[('korea', 'datasets')]\n",
    "metadata_df = metadata_dict[('korea', 'datasets')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>title</th>\n",
       "      <th>size</th>\n",
       "      <th>lastUpdated</th>\n",
       "      <th>downloadCount</th>\n",
       "      <th>voteCount</th>\n",
       "      <th>usabilityRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>7MB</td>\n",
       "      <td>2020-07-13 14:07:31</td>\n",
       "      <td>83497</td>\n",
       "      <td>1466</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>20MB</td>\n",
       "      <td>2020-04-03 16:33:49</td>\n",
       "      <td>9874</td>\n",
       "      <td>311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>99KB</td>\n",
       "      <td>2020-06-04 08:53:36</td>\n",
       "      <td>1035</td>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>772KB</td>\n",
       "      <td>2020-12-20 13:05:27</td>\n",
       "      <td>582</td>\n",
       "      <td>15</td>\n",
       "      <td>0.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>3GB</td>\n",
       "      <td>2020-03-15 08:56:42</td>\n",
       "      <td>5533</td>\n",
       "      <td>107</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              ref  \\\n",
       "0                     kimjihoo/coronavirusdataset   \n",
       "1                 bappekim/air-pollution-in-seoul   \n",
       "2                   bappekim/south-korea-visitors   \n",
       "3               hongsean/korea-income-and-welfare   \n",
       "4  bryanpark/korean-single-speaker-speech-dataset   \n",
       "\n",
       "                                             title   size  \\\n",
       "0  [NeurIPS 2020] Data Science for COVID-19 (DS4C)    7MB   \n",
       "1                           Air Pollution in Seoul   20MB   \n",
       "2                             South Korea Visitors   99KB   \n",
       "3                         Korea Income and Welfare  772KB   \n",
       "4             Korean Single Speaker Speech Dataset    3GB   \n",
       "\n",
       "           lastUpdated  downloadCount  voteCount  usabilityRating  \n",
       "0  2020-07-13 14:07:31          83497       1466         1.000000  \n",
       "1  2020-04-03 16:33:49           9874        311         1.000000  \n",
       "2  2020-06-04 08:53:36           1035         24         1.000000  \n",
       "3  2020-12-20 13:05:27            582         15         0.970588  \n",
       "4  2020-03-15 08:56:42           5533        107         0.750000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_no</th>\n",
       "      <th>datasetId</th>\n",
       "      <th>datasetSlug</th>\n",
       "      <th>ownerUser</th>\n",
       "      <th>usabilityRating</th>\n",
       "      <th>totalViews</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>description</th>\n",
       "      <th>isPrivate</th>\n",
       "      <th>keywords</th>\n",
       "      <th>licenses</th>\n",
       "      <th>collaborators</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>527325</td>\n",
       "      <td>527325</td>\n",
       "      <td>coronavirusdataset</td>\n",
       "      <td>kimjihoo</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>490936</td>\n",
       "      <td>1466</td>\n",
       "      <td>83497</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>DS4C: Data Science for COVID-19 in South Korea</td>\n",
       "      <td>### A portion of our dataset has been accepted...</td>\n",
       "      <td>False</td>\n",
       "      <td>[universities and colleges, biology, data visu...</td>\n",
       "      <td>[{'name': 'CC-BY-NC-SA-4.0'}]</td>\n",
       "      <td>[{'username': 'kjm0623v', 'role': 'writer'}, {...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>576393</td>\n",
       "      <td>576393</td>\n",
       "      <td>air-pollution-in-seoul</td>\n",
       "      <td>bappekim</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81243</td>\n",
       "      <td>311</td>\n",
       "      <td>9874</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>Air Pollution Measurement Information in Seoul...</td>\n",
       "      <td>### Context\\nThis dataset deals with air pollu...</td>\n",
       "      <td>False</td>\n",
       "      <td>[earth and nature, environment, pollution]</td>\n",
       "      <td>[{'name': 'CC-BY-SA-4.0'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>692628</td>\n",
       "      <td>692628</td>\n",
       "      <td>south-korea-visitors</td>\n",
       "      <td>bappekim</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5843</td>\n",
       "      <td>24</td>\n",
       "      <td>1035</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>Foreign visitors into South Korea</td>\n",
       "      <td>### Context\\n\\nThis dataset deals with the vis...</td>\n",
       "      <td>False</td>\n",
       "      <td>[global, travel]</td>\n",
       "      <td>[{'name': 'CC-BY-SA-4.0'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>1046735</td>\n",
       "      <td>1046735</td>\n",
       "      <td>korea-income-and-welfare</td>\n",
       "      <td>hongsean</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>3018</td>\n",
       "      <td>15</td>\n",
       "      <td>582</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>Where Korea wealth come from?</td>\n",
       "      <td>![](https://www.googleapis.com/download/storag...</td>\n",
       "      <td>False</td>\n",
       "      <td>[income, education, social issues and advocacy...</td>\n",
       "      <td>[{'name': 'CC0-1.0'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>19829</td>\n",
       "      <td>19829</td>\n",
       "      <td>korean-single-speaker-speech-dataset</td>\n",
       "      <td>bryanpark</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>83396</td>\n",
       "      <td>107</td>\n",
       "      <td>5533</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>KSS Dataset: Korean Single Speaker Speech Dataset</td>\n",
       "      <td># [Updated on September 28, 2019] KSS Dataset:...</td>\n",
       "      <td>False</td>\n",
       "      <td>[languages, arts and entertainment, education]</td>\n",
       "      <td>[{'name': 'CC-BY-NC-SA-4.0'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id    id_no datasetId  \\\n",
       "0                     kimjihoo/coronavirusdataset   527325    527325   \n",
       "1                 bappekim/air-pollution-in-seoul   576393    576393   \n",
       "2                   bappekim/south-korea-visitors   692628    692628   \n",
       "3               hongsean/korea-income-and-welfare  1046735   1046735   \n",
       "4  bryanpark/korean-single-speaker-speech-dataset    19829     19829   \n",
       "\n",
       "                            datasetSlug  ownerUser  usabilityRating  \\\n",
       "0                    coronavirusdataset   kimjihoo         1.000000   \n",
       "1                air-pollution-in-seoul   bappekim         1.000000   \n",
       "2                  south-korea-visitors   bappekim         1.000000   \n",
       "3              korea-income-and-welfare   hongsean         0.970588   \n",
       "4  korean-single-speaker-speech-dataset  bryanpark         0.750000   \n",
       "\n",
       "  totalViews totalVotes totalDownloads  \\\n",
       "0     490936       1466          83497   \n",
       "1      81243        311           9874   \n",
       "2       5843         24           1035   \n",
       "3       3018         15            582   \n",
       "4      83396        107           5533   \n",
       "\n",
       "                                             title  \\\n",
       "0  [NeurIPS 2020] Data Science for COVID-19 (DS4C)   \n",
       "1                           Air Pollution in Seoul   \n",
       "2                             South Korea Visitors   \n",
       "3                         Korea Income and Welfare   \n",
       "4             Korean Single Speaker Speech Dataset   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0     DS4C: Data Science for COVID-19 in South Korea   \n",
       "1  Air Pollution Measurement Information in Seoul...   \n",
       "2                  Foreign visitors into South Korea   \n",
       "3                      Where Korea wealth come from?   \n",
       "4  KSS Dataset: Korean Single Speaker Speech Dataset   \n",
       "\n",
       "                                         description isPrivate  \\\n",
       "0  ### A portion of our dataset has been accepted...     False   \n",
       "1  ### Context\\nThis dataset deals with air pollu...     False   \n",
       "2  ### Context\\n\\nThis dataset deals with the vis...     False   \n",
       "3  ![](https://www.googleapis.com/download/storag...     False   \n",
       "4  # [Updated on September 28, 2019] KSS Dataset:...     False   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [universities and colleges, biology, data visu...   \n",
       "1         [earth and nature, environment, pollution]   \n",
       "2                                   [global, travel]   \n",
       "3  [income, education, social issues and advocacy...   \n",
       "4     [languages, arts and entertainment, education]   \n",
       "\n",
       "                        licenses  \\\n",
       "0  [{'name': 'CC-BY-NC-SA-4.0'}]   \n",
       "1     [{'name': 'CC-BY-SA-4.0'}]   \n",
       "2     [{'name': 'CC-BY-SA-4.0'}]   \n",
       "3          [{'name': 'CC0-1.0'}]   \n",
       "4  [{'name': 'CC-BY-NC-SA-4.0'}]   \n",
       "\n",
       "                                       collaborators data  \n",
       "0  [{'username': 'kjm0623v', 'role': 'writer'}, {...   []  \n",
       "1                                                 []   []  \n",
       "2                                                 []   []  \n",
       "3                                                 []   []  \n",
       "4                                                 []   []  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
