{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the Kaggle API. Follow these steps in order to get the necessary credentials to continue:\n",
    "\n",
    "1. Sign up for a Kaggle account at https://www.kaggle.com.\n",
    "2. Go to the 'Account' tab of your user profile - ```https://www.kaggle.com/{username}/account```.\n",
    "3. Select 'Create New API Token' under 'API' section.\n",
    "    - This will trigger the download of kaggle.json, a file containing your API credentials. \n",
    "4. Place this file in the location:\n",
    "    - ~/.kaggle/kaggle.json (for macOS/unix)\n",
    "    - C:/Users/username/.kaggle/kaggle.json (for Windows) \n",
    "    - You can check the exact location, sans drive, with echo %HOMEPATH%). \n",
    "    - You can define a shell environment variable KAGGLE_CONFIG_DIR to change this location to:\n",
    "        - $KAGGLE_CONFIG_DIR/kaggle.json (for macOS/unix)\n",
    "        - %KAGGLE_CONFIG_DIR%\\kaggle.json (for Windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this notebook uses functions written in Python to query the Kaggle API. **This code will only work for python 3.7 or later**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Kaggle API ([Kaggle](https://www.kaggle.com/docs/api))\n",
    "- Kaggle API ([GitHub](https://github.com/Kaggle/kaggle-api)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries\n",
    "\n",
    "#### Query #1: query API query based on search terms and search types\n",
    "\n",
    "Define functions to query API based on search terms and search types:\n",
    "\n",
    "1.\tFunction `get_individual_search_output` queries the Kaggle API with the specified search term (e.g., “machine learning”) and search type (must be either “datasets” or “kernels”)\n",
    "    - Searches across all returned pages\n",
    "    - Calls function `_convert_string_csv_output_to_dataframe` which converts results from API (strings in semi-structured table) to dataframe format\n",
    "    - Result is a dataframe (one dataframe per search term/search type combination)\n",
    "\n",
    "\n",
    "3.\tFunction `get_all_search_outputs` queries the Kaggle API for all combinations of search terms and search types specified and returns the results as a dictionary of dataframes (one dataframe for each query combination)\n",
    "    - Calls function `get_individual_search_output`\n",
    "\n",
    "Run functions for specified search terms and search types. Output is ordered dictionary of dataframes (result #1 \"metadata_dict\").\n",
    "\n",
    "\n",
    "#### Query #2: query API for full metadata for hits from initial query\n",
    "\n",
    "Following the initial query to return high level metadata for hits (resuling in object \"ordered_dict\"), define functions to retrieve full metadata associated with each object.\n",
    "\n",
    "4.\tFunction `_retrieve_object_json` uses the path for each object (each dataframe in result #1 \"ordered_dict\") to query API for metadata associated with each object. Loads JSON results as dictionary.\n",
    "    \n",
    "5.\tFunction `get_metadata` extracts metadata associated with each object and formats as dataframe\n",
    "    - Calls function `retrieve_object_json`\n",
    "    - For each object, appends JSON results from `_retrieve_object_json` to a dataframe (converting to dataframe format)\n",
    "    - Output is single dataframe for each search query (matching each dataframe in result #1 \"ordered_dict\")\n",
    "    \n",
    "6. Use a `for` loop to put dataframes into an ordered dictionary, matching result #1 \"ordered_dict\" object\n",
    "    - Calls function `get_metadata`\n",
    "\n",
    "Run functions to pull metadata for each object returned by first query. Output is ordered dictionary of dataframes (result #2 \"metadata_dict\").\n",
    "\n",
    "We now have a dictionary of results from the first API query (which includes high level metadata such as ID and title) and a dictionary of results with additional metadata for each object, based on results from the first query.\n",
    "\n",
    "#### Merge results\n",
    "7.\tFunction `merge_search_and_metadata_dicts` merges these two dictionaries (\"ordered_dict\" and \"metadata_dict\") to a single ordered dictionary and (optional) saves the results as a single csv file\n",
    "    - Specify `left_on` and `right_on` column names to merge\n",
    "\n",
    "Run merge function to access full ordered dictionary and optionally save results to csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import kaggle, installing if necessary\n",
    "try:\n",
    "    import kaggle\n",
    "except ImportError as e:\n",
    "    !pip3 install kaggle\n",
    "    import kaggle\n",
    "    \n",
    "import subprocess # Used to run unix commands\n",
    "import pandas as pd # For storing/manipulating command data\n",
    "from io import StringIO # Lets us read csv string output from command into DataFrame\n",
    "import json # Reading back the metadata files\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "import os # Exporting saved results\n",
    "from collections import OrderedDict\n",
    "from utils import flatten_nested_df\n",
    "from flatten_json import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query #1: query API based on search terms and search types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, search_types, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the Kaggle API for each search term and search type. \n",
    "    Results are retured in results['{term}_{type}'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms (list-like): collection of search terms to query over\n",
    "    - search_types (list-like): collection of search types to query over\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - results (dict): dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    num_searches = len(search_terms) * len(search_types)\n",
    "    results = OrderedDict()\n",
    "    \n",
    "    for search_term, search_type in itertools.product(search_terms, search_types):\n",
    "        results[(search_term, search_type)] = get_individual_search_output(search_term, search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Calls the Kaggle API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term (str): keyword to seach for\n",
    "    - search_type (str): objects to search over (must be either datasets or kernels)\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert isinstance(search_type, str), 'Search term must be a string'\n",
    "    assert search_type in ('datasets', 'kernels'), 'Search can only be conducted over datasets or kernels'\n",
    "    \n",
    "    # Set search variables\n",
    "    page_idx = 1\n",
    "    search_output = ''\n",
    "    cumulative_output = ''\n",
    "    completion_phrase = f'No {search_type} found\\n'\n",
    "    \n",
    "    # Pulls the records for a single page of datasets for the given search term\n",
    "    # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "    search_output = subprocess.run(['kaggle', search_type, 'list', '-v',\n",
    "                                     '-s', f'\"{search_term}\"', \n",
    "                                     '-p', str(page_idx)], \n",
    "                                    capture_output=True).stdout.decode()\n",
    "\n",
    "    # Once we no longer see new output, we stop\n",
    "    while search_output != completion_phrase:\n",
    "        # Accumulate the output\n",
    "        cumulative_output = cumulative_output + search_output\n",
    "\n",
    "        # Increments the page count for searching\n",
    "        page_idx += 1\n",
    "\n",
    "        # Pulls the records for a single page of datasets for the given search term\n",
    "        # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "        search_output = subprocess.run(['kaggle', search_type, 'list', '-v',\n",
    "                                         '-s', f'\"{search_term}\"', \n",
    "                                         '-p', str(page_idx)], \n",
    "                                        capture_output=True).stdout.decode()\n",
    "\n",
    "        # Remove header row\n",
    "        if search_output != completion_phrase:\n",
    "            search_output = '\\r\\n'.join(search_output.split('\\r\\n')[1::])\n",
    "    \n",
    "    # Convert search output to DataFrame\n",
    "    search_df = _convert_string_csv_output_to_dataframe(cumulative_output)\n",
    "    \n",
    "    # Rename columns to match names present in metadata df\n",
    "    search_df.rename(columns={'ref': 'id', \n",
    "                              'downloadCount': 'totalDownloads', \n",
    "                              'voteCount': 'totalVotes'}, \n",
    "                     inplace=True)\n",
    "        \n",
    "    if flatten_output:\n",
    "        search_df = flatten_nested_df(search_df)\n",
    "    \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_string_csv_output_to_dataframe(output):\n",
    "    \"\"\"\n",
    "    Given a string variable in csv format, returns a Pandas DataFrame\n",
    "    \n",
    "    Params:\n",
    "    - output (str): csv-styled string to be converted\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame consisting of data from 'output' string variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create DataFrame of results\n",
    "    output = StringIO(output)\n",
    "    df = pd.read_csv(output)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run initial API query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['korea']\n",
    "search_types = ['datasets', 'kernels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, search_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query #2: query API for full metadata for hits from initial query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Unable to find a way to store metadata in memory as opposed to saving file, but this workaround appears to be functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_object_json(object_path, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Queries Kaggle for metadata json file & returns the json data as a dictionary\n",
    "    \n",
    "    Params:\n",
    "    - object_path (str): path for the dataset\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_dict (dict): dictionary containing json metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download the metadata\n",
    "    subprocess.run(['kaggle', 'datasets', 'metadata', object_path])\n",
    "\n",
    "    # Access the metadata and load it in as a dictionary\n",
    "    with open('dataset-metadata.json') as file:\n",
    "        json_data = json.load(file)\n",
    "        \n",
    "    if flatten_output:\n",
    "        json_data = flatten(json_data)\n",
    "        \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(object_paths, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the file/files listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths (str/list-like): string or list of strings containing the paths for the objects\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df (pandas.DataFrame): DataFrame containing metadata for the requested datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "        \n",
    "    # Run first query\n",
    "    json_data = _retrieve_object_json(object_paths[0], flatten_output)\n",
    "        \n",
    "    # Create DataFrame to store metadata in, using columns found in first query, and then add query info\n",
    "    metadata_df = pd.DataFrame(columns=json_data.keys(), dtype=object)\n",
    "    metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    # Pulls metadata information for each dataset found above\n",
    "    for object_path in tqdm(object_paths[1::]):\n",
    "        # Download & load the metadata\n",
    "        json_data = _retrieve_object_json(object_path, flatten_output)\n",
    "\n",
    "        # Store the metadata into our DataFrame created above\n",
    "        metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run metadata extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Extract IDs from DataFrame, and returns as list of strings\n",
    "metadata_dict = OrderedDict()\n",
    "\n",
    "for query, df in search_output_dict.items():\n",
    "    print(f'Retrieving {query} metadata')\n",
    "    # Create object paths\n",
    "    _, search_type = query\n",
    "    object_paths = df.id.values\n",
    "    \n",
    "    metadata_dict[query] = get_metadata(object_paths, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict[('korea', 'datasets')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine results of initial and metadata queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict (dict): dictionary of search output results\n",
    "    - metadata_dict (dict): dictionary of metadata results\n",
    "    - on (str/list-like): column name(s) to merge the two dicts on\n",
    "    - left_on (str/list-like): column name(s) to merge the left dict on\n",
    "    - right_on (str/list-like): column name(s) to merge the right dict on\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/kaggle/kaggle_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    Returns:\n",
    "    - df_dict (OrderedDict): OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "\n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'kaggle')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure kaggle directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError('Save type must be bool or str')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### need to clarify left_on, right_on and if can be incorporated into function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run merge function\n",
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results of query #1\n",
    "output_df = search_output_dict[('korea', 'datasets')]\n",
    "\n",
    "#results of query #2\n",
    "metadata_df = metadata_dict[('korea', 'datasets')]\n",
    "\n",
    "#result of merging datasets into \"full\" dataframe\n",
    "full_df = df_dict[('korea', 'datasets')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
