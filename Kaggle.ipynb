{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the Kaggle API. Follow these steps in order to get the necessary credentials to continue:\n",
    "\n",
    "1. Sign up for a Kaggle account at https://www.kaggle.com.\n",
    "2. Go to the 'Account' tab of your user profile - ```https://www.kaggle.com/{username}/account```.\n",
    "3. Select 'Create New API Token' under 'API' section.\n",
    "    - This will trigger the download of kaggle.json, a file containing your API credentials. \n",
    "4. Place this file in the location:\n",
    "    - ~/.kaggle/kaggle.json (for macOS/unix)\n",
    "    - C:/Users/username/.kaggle/kaggle.json (for Windows) \n",
    "    - You can check the exact location, sans drive, with echo %HOMEPATH%). \n",
    "    - You can define a shell environment variable KAGGLE_CONFIG_DIR to change this location to:\n",
    "        - $KAGGLE_CONFIG_DIR/kaggle.json (for macOS/unix)\n",
    "        - %KAGGLE_CONFIG_DIR%\\kaggle.json (for Windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this notebook uses functions written in Python to query the Kaggle API. **This code will only work for python 3.7 or later**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Kaggle API ([Kaggle](https://www.kaggle.com/docs/api))\n",
    "- Kaggle API ([GitHub](https://github.com/Kaggle/kaggle-api)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries\n",
    "\n",
    "#### Query #1: query API query based on search terms and search types\n",
    "\n",
    "Define functions to query API based on search terms and search types:\n",
    "\n",
    "1.\tFunction `get_individual_search_output` queries the Kaggle API with the specified search term (e.g., “machine learning”) and search type (must be either “datasets” or “kernels”)\n",
    "    - Searches across all returned pages\n",
    "    - Calls function `_convert_string_csv_output_to_dataframe` which converts results from API (strings in semi-structured table) to dataframe format\n",
    "    - Result is a dataframe (one dataframe per search term/search type combination)\n",
    "\n",
    "\n",
    "3.\tFunction `get_all_search_outputs` queries the Kaggle API for all combinations of search terms and search types specified and returns the results as a dictionary of dataframes (one dataframe for each query combination)\n",
    "    - Calls function `get_individual_search_output`\n",
    "\n",
    "Run functions for specified search terms and search types. Output is ordered dictionary of dataframes (result #1 \"metadata_dict\").\n",
    "\n",
    "\n",
    "#### Query #2: query API for full metadata for hits from initial query\n",
    "\n",
    "Following the initial query to return high level metadata for hits (resuling in object \"ordered_dict\"), define functions to retrieve full metadata associated with each object.\n",
    "\n",
    "4.\tFunction `_retrieve_object_json` uses the path for each object (each dataframe in result #1 \"ordered_dict\") to query API for metadata associated with each object. Loads JSON results as dictionary.\n",
    "    \n",
    "5.\tFunction `get_metadata` extracts metadata associated with each object and formats as dataframe\n",
    "    - Calls function `retrieve_object_json`\n",
    "    - For each object, appends JSON results from `_retrieve_object_json` to a dataframe (converting to dataframe format)\n",
    "    - Output is single dataframe for each search query (matching each dataframe in result #1 \"ordered_dict\")\n",
    "    \n",
    "6. Use a `for` loop to put dataframes into an ordered dictionary, matching result #1 \"ordered_dict\" object\n",
    "    - Calls function `get_metadata`\n",
    "\n",
    "Run functions to pull metadata for each object returned by first query. Output is ordered dictionary of dataframes (result #2 \"metadata_dict\").\n",
    "\n",
    "We now have a dictionary of results from the first API query (which includes high level metadata such as ID and title) and a dictionary of results with additional metadata for each object, based on results from the first query.\n",
    "\n",
    "#### Merge results\n",
    "7.\tFunction `merge_search_and_metadata_dicts` merges these two dictionaries (\"ordered_dict\" and \"metadata_dict\") to a single ordered dictionary and (optional) saves the results as a single csv file\n",
    "    - Specify `left_on` and `right_on` column names to merge\n",
    "\n",
    "Run merge function to access full ordered dictionary and optionally save results to csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import kaggle, installing if necessary\n",
    "try:\n",
    "    import kaggle\n",
    "except ImportError as e:\n",
    "    !pip3 install kaggle\n",
    "    import kaggle\n",
    "    \n",
    "import subprocess # Used to run unix commands\n",
    "import pandas as pd # For storing/manipulating command data\n",
    "from io import StringIO # Lets us read csv string output from command into DataFrame\n",
    "import json # Reading back the metadata files\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "import os # Exporting saved results\n",
    "from collections import OrderedDict\n",
    "from utils import flatten_nested_df\n",
    "from flatten_json import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query #1: query API based on search terms and search types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_terms, search_types, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the Kaggle API for each search term and search type. \n",
    "    Results are retured in results['{term}_{type}'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms (list-like): collection of search terms to query over\n",
    "    - search_types (list-like): collection of search types to query over\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - results (dict): dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    num_searches = len(search_terms) * len(search_types)\n",
    "    results = OrderedDict()\n",
    "    \n",
    "    for search_term, search_type in itertools.product(search_terms, search_types):\n",
    "        results[(search_term, search_type)] = get_individual_search_output(search_term, search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Calls the Kaggle API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term (str): keyword to seach for\n",
    "    - search_type (str): objects to search over (must be either datasets or kernels)\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert isinstance(search_type, str), 'Search term must be a string'\n",
    "    assert search_type in ('datasets', 'kernels'), 'Search can only be conducted over datasets or kernels'\n",
    "    \n",
    "    # Set search variables\n",
    "    page_idx = 1\n",
    "    search_output = ''\n",
    "    cumulative_output = ''\n",
    "    completion_phrase = f'No {search_type} found\\n'\n",
    "    \n",
    "    # Pulls the records for a single page of datasets for the given search term\n",
    "    # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "    search_output = subprocess.run(['kaggle', search_type, 'list', '-v',\n",
    "                                     '-s', f'\"{search_term}\"', \n",
    "                                     '-p', str(page_idx)], \n",
    "                                    capture_output=True).stdout.decode()\n",
    "\n",
    "    # Once we no longer see new output, we stop\n",
    "    while search_output != completion_phrase:\n",
    "        # Accumulate the output\n",
    "        cumulative_output = cumulative_output + search_output\n",
    "\n",
    "        # Increments the page count for searching\n",
    "        page_idx += 1\n",
    "\n",
    "        # Pulls the records for a single page of datasets for the given search term\n",
    "        # Runs the command, captures the output in stdout, reads it from stdout, and decodes it to str from binary\n",
    "        search_output = subprocess.run(['kaggle', search_type, 'list', '-v',\n",
    "                                         '-s', f'\"{search_term}\"', \n",
    "                                         '-p', str(page_idx)], \n",
    "                                        capture_output=True).stdout.decode()\n",
    "\n",
    "        # Remove header row\n",
    "        if search_output != completion_phrase:\n",
    "            search_output = '\\r\\n'.join(search_output.split('\\r\\n')[1::])\n",
    "    \n",
    "    # Convert search output to DataFrame\n",
    "    search_df = _convert_string_csv_output_to_dataframe(cumulative_output)\n",
    "    \n",
    "    # Rename columns to match names present in metadata df\n",
    "    search_df.rename(columns={'ref': 'id', \n",
    "                              'downloadCount': 'totalDownloads', \n",
    "                              'voteCount': 'totalVotes'}, \n",
    "                     inplace=True)\n",
    "        \n",
    "    if flatten_output:\n",
    "        search_df = flatten_nested_df(search_df)\n",
    "    \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_string_csv_output_to_dataframe(output):\n",
    "    \"\"\"\n",
    "    Given a string variable in csv format, returns a Pandas DataFrame\n",
    "    \n",
    "    Params:\n",
    "    - output (str): csv-styled string to be converted\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame consisting of data from 'output' string variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create DataFrame of results\n",
    "    output = StringIO(output)\n",
    "    df = pd.read_csv(output)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run initial API query functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['korea']\n",
    "search_types = ['datasets', 'kernels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, search_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>size</th>\n",
       "      <th>lastUpdated</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>usabilityRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>7MB</td>\n",
       "      <td>2020-07-13 14:07:31</td>\n",
       "      <td>84246</td>\n",
       "      <td>1471</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>20MB</td>\n",
       "      <td>2020-04-03 16:33:49</td>\n",
       "      <td>9945</td>\n",
       "      <td>311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>99KB</td>\n",
       "      <td>2020-06-04 08:53:36</td>\n",
       "      <td>1054</td>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>772KB</td>\n",
       "      <td>2020-12-20 13:05:27</td>\n",
       "      <td>591</td>\n",
       "      <td>16</td>\n",
       "      <td>0.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>3GB</td>\n",
       "      <td>2020-03-15 08:56:42</td>\n",
       "      <td>5619</td>\n",
       "      <td>109</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id  \\\n",
       "0                     kimjihoo/coronavirusdataset   \n",
       "1                 bappekim/air-pollution-in-seoul   \n",
       "2                   bappekim/south-korea-visitors   \n",
       "3               hongsean/korea-income-and-welfare   \n",
       "4  bryanpark/korean-single-speaker-speech-dataset   \n",
       "\n",
       "                                             title   size  \\\n",
       "0  [NeurIPS 2020] Data Science for COVID-19 (DS4C)    7MB   \n",
       "1                           Air Pollution in Seoul   20MB   \n",
       "2                             South Korea Visitors   99KB   \n",
       "3                         Korea Income and Welfare  772KB   \n",
       "4             Korean Single Speaker Speech Dataset    3GB   \n",
       "\n",
       "           lastUpdated  totalDownloads  totalVotes  usabilityRating  \n",
       "0  2020-07-13 14:07:31           84246        1471         1.000000  \n",
       "1  2020-04-03 16:33:49            9945         311         1.000000  \n",
       "2  2020-06-04 08:53:36            1054          24         1.000000  \n",
       "3  2020-12-20 13:05:27             591          16         0.970588  \n",
       "4  2020-03-15 08:56:42            5619         109         0.750000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query #2: query API for full metadata for hits from initial query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Unable to find a way to store metadata in memory as opposed to saving file, but this workaround appears to be functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_object_json(object_path, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Queries Kaggle for metadata json file & returns the json data as a dictionary\n",
    "    \n",
    "    Params:\n",
    "    - object_path (str): path for the dataset\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_dict (dict): dictionary containing json metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download the metadata\n",
    "    subprocess.run(['kaggle', 'datasets', 'metadata', object_path])\n",
    "\n",
    "    # Access the metadata and load it in as a dictionary\n",
    "    with open('dataset-metadata.json') as file:\n",
    "        json_data = json.load(file)\n",
    "        \n",
    "    if flatten_output:\n",
    "        json_data = flatten(json_data)\n",
    "        \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(object_paths, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the file/files listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths (str/list-like): string or list of strings containing the paths for the objects\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df (pandas.DataFrame): DataFrame containing metadata for the requested datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "        \n",
    "    # Run first query\n",
    "    json_data = _retrieve_object_json(object_paths[0], flatten_output)\n",
    "        \n",
    "    # Create DataFrame to store metadata in, using columns found in first query, and then add query info\n",
    "    metadata_df = pd.DataFrame(columns=json_data.keys(), dtype=object)\n",
    "    metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    # Pulls metadata information for each dataset found above\n",
    "    for object_path in tqdm(object_paths[1::]):\n",
    "        # Download & load the metadata\n",
    "        json_data = _retrieve_object_json(object_path, flatten_output)\n",
    "\n",
    "        # Store the metadata into our DataFrame created above\n",
    "        metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run metadata extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('korea', 'datasets') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [03:32<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('korea', 'kernels') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1019/1019 [19:56<00:00,  1.17s/it] \n"
     ]
    }
   ],
   "source": [
    "## Extract IDs from DataFrame, and returns as list of strings\n",
    "metadata_dict = OrderedDict()\n",
    "\n",
    "for query, df in search_output_dict.items():\n",
    "    print(f'Retrieving {query} metadata')\n",
    "    # Create object paths\n",
    "    _, search_type = query\n",
    "    object_paths = df.id.values\n",
    "    \n",
    "    metadata_dict[query] = get_metadata(object_paths, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_no</th>\n",
       "      <th>datasetId</th>\n",
       "      <th>datasetSlug</th>\n",
       "      <th>ownerUser</th>\n",
       "      <th>usabilityRating</th>\n",
       "      <th>totalViews</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>collaborators_15_username</th>\n",
       "      <th>collaborators_15_role</th>\n",
       "      <th>collaborators_16_username</th>\n",
       "      <th>collaborators_16_role</th>\n",
       "      <th>data</th>\n",
       "      <th>collaborators</th>\n",
       "      <th>keywords_5</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_6</th>\n",
       "      <th>keywords_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>527325</td>\n",
       "      <td>527325</td>\n",
       "      <td>coronavirusdataset</td>\n",
       "      <td>kimjihoo</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>492173</td>\n",
       "      <td>1471</td>\n",
       "      <td>84246</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>...</td>\n",
       "      <td>minty99</td>\n",
       "      <td>writer</td>\n",
       "      <td>byeongukyu</td>\n",
       "      <td>writer</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>576393</td>\n",
       "      <td>576393</td>\n",
       "      <td>air-pollution-in-seoul</td>\n",
       "      <td>bappekim</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81584</td>\n",
       "      <td>311</td>\n",
       "      <td>9945</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>692628</td>\n",
       "      <td>692628</td>\n",
       "      <td>south-korea-visitors</td>\n",
       "      <td>bappekim</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5923</td>\n",
       "      <td>24</td>\n",
       "      <td>1054</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>1046735</td>\n",
       "      <td>1046735</td>\n",
       "      <td>korea-income-and-welfare</td>\n",
       "      <td>hongsean</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>3085</td>\n",
       "      <td>16</td>\n",
       "      <td>591</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>19829</td>\n",
       "      <td>19829</td>\n",
       "      <td>korean-single-speaker-speech-dataset</td>\n",
       "      <td>bryanpark</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>83809</td>\n",
       "      <td>109</td>\n",
       "      <td>5619</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>glhr00/rgbt-driveability-segmentation-kaist</td>\n",
       "      <td>1340078</td>\n",
       "      <td>1340078</td>\n",
       "      <td>rgbt-driveability-segmentation-kaist</td>\n",
       "      <td>glhr00</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RGB-T driveability segmentation</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>llkdev/gender-data</td>\n",
       "      <td>231235</td>\n",
       "      <td>231235</td>\n",
       "      <td>gender-data</td>\n",
       "      <td>llkdev</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>342</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Gender Data</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>chenykfrank/immc21</td>\n",
       "      <td>1135726</td>\n",
       "      <td>1135726</td>\n",
       "      <td>immc21</td>\n",
       "      <td>chenykfrank</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>immc21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>gremmn/gas-prices</td>\n",
       "      <td>1108372</td>\n",
       "      <td>1108372</td>\n",
       "      <td>gas-prices</td>\n",
       "      <td>gremmn</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>gas prices</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>theeranartmeesathien/covid-dataset-for-prediction</td>\n",
       "      <td>1392213</td>\n",
       "      <td>1392213</td>\n",
       "      <td>covid-dataset-for-prediction</td>\n",
       "      <td>theeranartmeesathien</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Covid dataset for prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id    id_no datasetId  \\\n",
       "0                          kimjihoo/coronavirusdataset   527325    527325   \n",
       "1                      bappekim/air-pollution-in-seoul   576393    576393   \n",
       "2                        bappekim/south-korea-visitors   692628    692628   \n",
       "3                    hongsean/korea-income-and-welfare  1046735   1046735   \n",
       "4       bryanpark/korean-single-speaker-speech-dataset    19829     19829   \n",
       "..                                                 ...      ...       ...   \n",
       "174        glhr00/rgbt-driveability-segmentation-kaist  1340078   1340078   \n",
       "175                                 llkdev/gender-data   231235    231235   \n",
       "176                                 chenykfrank/immc21  1135726   1135726   \n",
       "177                                  gremmn/gas-prices  1108372   1108372   \n",
       "178  theeranartmeesathien/covid-dataset-for-prediction  1392213   1392213   \n",
       "\n",
       "                              datasetSlug             ownerUser  \\\n",
       "0                      coronavirusdataset              kimjihoo   \n",
       "1                  air-pollution-in-seoul              bappekim   \n",
       "2                    south-korea-visitors              bappekim   \n",
       "3                korea-income-and-welfare              hongsean   \n",
       "4    korean-single-speaker-speech-dataset             bryanpark   \n",
       "..                                    ...                   ...   \n",
       "174  rgbt-driveability-segmentation-kaist                glhr00   \n",
       "175                           gender-data                llkdev   \n",
       "176                                immc21           chenykfrank   \n",
       "177                            gas-prices                gremmn   \n",
       "178          covid-dataset-for-prediction  theeranartmeesathien   \n",
       "\n",
       "     usabilityRating totalViews totalVotes totalDownloads  \\\n",
       "0           1.000000     492173       1471          84246   \n",
       "1           1.000000      81584        311           9945   \n",
       "2           1.000000       5923         24           1054   \n",
       "3           0.970588       3085         16            591   \n",
       "4           0.750000      83809        109           5619   \n",
       "..               ...        ...        ...            ...   \n",
       "174         0.562500         47          0              1   \n",
       "175         0.125000        342          0              8   \n",
       "176         0.117647         48          0              2   \n",
       "177         0.117647        174          0             17   \n",
       "178         0.352941         13          0              1   \n",
       "\n",
       "                                               title  ...  \\\n",
       "0    [NeurIPS 2020] Data Science for COVID-19 (DS4C)  ...   \n",
       "1                             Air Pollution in Seoul  ...   \n",
       "2                               South Korea Visitors  ...   \n",
       "3                           Korea Income and Welfare  ...   \n",
       "4               Korean Single Speaker Speech Dataset  ...   \n",
       "..                                               ...  ...   \n",
       "174                  RGB-T driveability segmentation  ...   \n",
       "175                                      Gender Data  ...   \n",
       "176                                           immc21  ...   \n",
       "177                                       gas prices  ...   \n",
       "178                     Covid dataset for prediction  ...   \n",
       "\n",
       "    collaborators_15_username collaborators_15_role collaborators_16_username  \\\n",
       "0                     minty99                writer                byeongukyu   \n",
       "1                         NaN                   NaN                       NaN   \n",
       "2                         NaN                   NaN                       NaN   \n",
       "3                         NaN                   NaN                       NaN   \n",
       "4                         NaN                   NaN                       NaN   \n",
       "..                        ...                   ...                       ...   \n",
       "174                       NaN                   NaN                       NaN   \n",
       "175                       NaN                   NaN                       NaN   \n",
       "176                       NaN                   NaN                       NaN   \n",
       "177                       NaN                   NaN                       NaN   \n",
       "178                       NaN                   NaN                       NaN   \n",
       "\n",
       "    collaborators_16_role data collaborators keywords_5 keywords keywords_6  \\\n",
       "0                  writer   []           NaN        NaN      NaN        NaN   \n",
       "1                     NaN   []            []        NaN      NaN        NaN   \n",
       "2                     NaN   []            []        NaN      NaN        NaN   \n",
       "3                     NaN   []            []        NaN      NaN        NaN   \n",
       "4                     NaN   []            []        NaN      NaN        NaN   \n",
       "..                    ...  ...           ...        ...      ...        ...   \n",
       "174                   NaN   []            []        NaN      NaN        NaN   \n",
       "175                   NaN   []            []        NaN       []        NaN   \n",
       "176                   NaN   []            []        NaN       []        NaN   \n",
       "177                   NaN   []            []        NaN       []        NaN   \n",
       "178                   NaN   []            []        NaN      NaN        NaN   \n",
       "\n",
       "    keywords_7  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "..         ...  \n",
       "174        NaN  \n",
       "175        NaN  \n",
       "176        NaN  \n",
       "177        NaN  \n",
       "178        NaN  \n",
       "\n",
       "[179 rows x 59 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dict[('korea', 'datasets')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine results of initial and metadata queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict (dict): dictionary of search output results\n",
    "    - metadata_dict (dict): dictionary of metadata results\n",
    "    - on (str/list-like): column name(s) to merge the two dicts on\n",
    "    - left_on (str/list-like): column name(s) to merge the left dict on\n",
    "    - right_on (str/list-like): column name(s) to merge the right dict on\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/kaggle/kaggle_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    Returns:\n",
    "    - df_dict (OrderedDict): OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "\n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'kaggle')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure kaggle directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError('Save type must be bool or str')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### need to clarify left_on, right_on and if can be incorporated into function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run merge function\n",
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results of query #1\n",
    "output_df = search_output_dict[('korea', 'datasets')]\n",
    "\n",
    "#results of query #2\n",
    "metadata_df = metadata_dict[('korea', 'datasets')]\n",
    "\n",
    "#result of merging datasets into \"full\" dataframe\n",
    "full_df = df_dict[('korea', 'datasets')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>size</th>\n",
       "      <th>lastUpdated</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>usabilityRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>7MB</td>\n",
       "      <td>2020-07-13 14:07:31</td>\n",
       "      <td>84246</td>\n",
       "      <td>1471</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>20MB</td>\n",
       "      <td>2020-04-03 16:33:49</td>\n",
       "      <td>9945</td>\n",
       "      <td>311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>99KB</td>\n",
       "      <td>2020-06-04 08:53:36</td>\n",
       "      <td>1054</td>\n",
       "      <td>24</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>772KB</td>\n",
       "      <td>2020-12-20 13:05:27</td>\n",
       "      <td>591</td>\n",
       "      <td>16</td>\n",
       "      <td>0.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>3GB</td>\n",
       "      <td>2020-03-15 08:56:42</td>\n",
       "      <td>5619</td>\n",
       "      <td>109</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id  \\\n",
       "0                     kimjihoo/coronavirusdataset   \n",
       "1                 bappekim/air-pollution-in-seoul   \n",
       "2                   bappekim/south-korea-visitors   \n",
       "3               hongsean/korea-income-and-welfare   \n",
       "4  bryanpark/korean-single-speaker-speech-dataset   \n",
       "\n",
       "                                             title   size  \\\n",
       "0  [NeurIPS 2020] Data Science for COVID-19 (DS4C)    7MB   \n",
       "1                           Air Pollution in Seoul   20MB   \n",
       "2                             South Korea Visitors   99KB   \n",
       "3                         Korea Income and Welfare  772KB   \n",
       "4             Korean Single Speaker Speech Dataset    3GB   \n",
       "\n",
       "           lastUpdated  totalDownloads  totalVotes  usabilityRating  \n",
       "0  2020-07-13 14:07:31           84246        1471         1.000000  \n",
       "1  2020-04-03 16:33:49            9945         311         1.000000  \n",
       "2  2020-06-04 08:53:36            1054          24         1.000000  \n",
       "3  2020-12-20 13:05:27             591          16         0.970588  \n",
       "4  2020-03-15 08:56:42            5619         109         0.750000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_no</th>\n",
       "      <th>datasetId</th>\n",
       "      <th>datasetSlug</th>\n",
       "      <th>ownerUser</th>\n",
       "      <th>usabilityRating</th>\n",
       "      <th>totalViews</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>collaborators_15_username</th>\n",
       "      <th>collaborators_15_role</th>\n",
       "      <th>collaborators_16_username</th>\n",
       "      <th>collaborators_16_role</th>\n",
       "      <th>data</th>\n",
       "      <th>collaborators</th>\n",
       "      <th>keywords_5</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_6</th>\n",
       "      <th>keywords_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>527325</td>\n",
       "      <td>527325</td>\n",
       "      <td>coronavirusdataset</td>\n",
       "      <td>kimjihoo</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>492173</td>\n",
       "      <td>1471</td>\n",
       "      <td>84246</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>...</td>\n",
       "      <td>minty99</td>\n",
       "      <td>writer</td>\n",
       "      <td>byeongukyu</td>\n",
       "      <td>writer</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>576393</td>\n",
       "      <td>576393</td>\n",
       "      <td>air-pollution-in-seoul</td>\n",
       "      <td>bappekim</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81584</td>\n",
       "      <td>311</td>\n",
       "      <td>9945</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>692628</td>\n",
       "      <td>692628</td>\n",
       "      <td>south-korea-visitors</td>\n",
       "      <td>bappekim</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5923</td>\n",
       "      <td>24</td>\n",
       "      <td>1054</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>1046735</td>\n",
       "      <td>1046735</td>\n",
       "      <td>korea-income-and-welfare</td>\n",
       "      <td>hongsean</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>3085</td>\n",
       "      <td>16</td>\n",
       "      <td>591</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>19829</td>\n",
       "      <td>19829</td>\n",
       "      <td>korean-single-speaker-speech-dataset</td>\n",
       "      <td>bryanpark</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>83809</td>\n",
       "      <td>109</td>\n",
       "      <td>5619</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id    id_no datasetId  \\\n",
       "0                     kimjihoo/coronavirusdataset   527325    527325   \n",
       "1                 bappekim/air-pollution-in-seoul   576393    576393   \n",
       "2                   bappekim/south-korea-visitors   692628    692628   \n",
       "3               hongsean/korea-income-and-welfare  1046735   1046735   \n",
       "4  bryanpark/korean-single-speaker-speech-dataset    19829     19829   \n",
       "\n",
       "                            datasetSlug  ownerUser  usabilityRating  \\\n",
       "0                    coronavirusdataset   kimjihoo         1.000000   \n",
       "1                air-pollution-in-seoul   bappekim         1.000000   \n",
       "2                  south-korea-visitors   bappekim         1.000000   \n",
       "3              korea-income-and-welfare   hongsean         0.970588   \n",
       "4  korean-single-speaker-speech-dataset  bryanpark         0.750000   \n",
       "\n",
       "  totalViews totalVotes totalDownloads  \\\n",
       "0     492173       1471          84246   \n",
       "1      81584        311           9945   \n",
       "2       5923         24           1054   \n",
       "3       3085         16            591   \n",
       "4      83809        109           5619   \n",
       "\n",
       "                                             title  ...  \\\n",
       "0  [NeurIPS 2020] Data Science for COVID-19 (DS4C)  ...   \n",
       "1                           Air Pollution in Seoul  ...   \n",
       "2                             South Korea Visitors  ...   \n",
       "3                         Korea Income and Welfare  ...   \n",
       "4             Korean Single Speaker Speech Dataset  ...   \n",
       "\n",
       "  collaborators_15_username collaborators_15_role collaborators_16_username  \\\n",
       "0                   minty99                writer                byeongukyu   \n",
       "1                       NaN                   NaN                       NaN   \n",
       "2                       NaN                   NaN                       NaN   \n",
       "3                       NaN                   NaN                       NaN   \n",
       "4                       NaN                   NaN                       NaN   \n",
       "\n",
       "  collaborators_16_role data collaborators keywords_5 keywords keywords_6  \\\n",
       "0                writer   []           NaN        NaN      NaN        NaN   \n",
       "1                   NaN   []            []        NaN      NaN        NaN   \n",
       "2                   NaN   []            []        NaN      NaN        NaN   \n",
       "3                   NaN   []            []        NaN      NaN        NaN   \n",
       "4                   NaN   []            []        NaN      NaN        NaN   \n",
       "\n",
       "  keywords_7  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>size</th>\n",
       "      <th>lastUpdated</th>\n",
       "      <th>totalDownloads</th>\n",
       "      <th>totalVotes</th>\n",
       "      <th>usabilityRating</th>\n",
       "      <th>id_no</th>\n",
       "      <th>datasetId</th>\n",
       "      <th>datasetSlug</th>\n",
       "      <th>...</th>\n",
       "      <th>collaborators_15_username</th>\n",
       "      <th>collaborators_15_role</th>\n",
       "      <th>collaborators_16_username</th>\n",
       "      <th>collaborators_16_role</th>\n",
       "      <th>data</th>\n",
       "      <th>collaborators</th>\n",
       "      <th>keywords_5</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_6</th>\n",
       "      <th>keywords_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kimjihoo/coronavirusdataset</td>\n",
       "      <td>[NeurIPS 2020] Data Science for COVID-19 (DS4C)</td>\n",
       "      <td>7MB</td>\n",
       "      <td>2020-07-13 14:07:31</td>\n",
       "      <td>84246.0</td>\n",
       "      <td>1471.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>527325</td>\n",
       "      <td>527325</td>\n",
       "      <td>coronavirusdataset</td>\n",
       "      <td>...</td>\n",
       "      <td>minty99</td>\n",
       "      <td>writer</td>\n",
       "      <td>byeongukyu</td>\n",
       "      <td>writer</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bappekim/air-pollution-in-seoul</td>\n",
       "      <td>Air Pollution in Seoul</td>\n",
       "      <td>20MB</td>\n",
       "      <td>2020-04-03 16:33:49</td>\n",
       "      <td>9945.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>576393</td>\n",
       "      <td>576393</td>\n",
       "      <td>air-pollution-in-seoul</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bappekim/south-korea-visitors</td>\n",
       "      <td>South Korea Visitors</td>\n",
       "      <td>99KB</td>\n",
       "      <td>2020-06-04 08:53:36</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>692628</td>\n",
       "      <td>692628</td>\n",
       "      <td>south-korea-visitors</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hongsean/korea-income-and-welfare</td>\n",
       "      <td>Korea Income and Welfare</td>\n",
       "      <td>772KB</td>\n",
       "      <td>2020-12-20 13:05:27</td>\n",
       "      <td>591.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bryanpark/korean-single-speaker-speech-dataset</td>\n",
       "      <td>Korean Single Speaker Speech Dataset</td>\n",
       "      <td>3GB</td>\n",
       "      <td>2020-03-15 08:56:42</td>\n",
       "      <td>5619.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>19829</td>\n",
       "      <td>19829</td>\n",
       "      <td>korean-single-speaker-speech-dataset</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id  \\\n",
       "0                     kimjihoo/coronavirusdataset   \n",
       "1                 bappekim/air-pollution-in-seoul   \n",
       "2                   bappekim/south-korea-visitors   \n",
       "3               hongsean/korea-income-and-welfare   \n",
       "4  bryanpark/korean-single-speaker-speech-dataset   \n",
       "\n",
       "                                             title   size  \\\n",
       "0  [NeurIPS 2020] Data Science for COVID-19 (DS4C)    7MB   \n",
       "1                           Air Pollution in Seoul   20MB   \n",
       "2                             South Korea Visitors   99KB   \n",
       "3                         Korea Income and Welfare  772KB   \n",
       "4             Korean Single Speaker Speech Dataset    3GB   \n",
       "\n",
       "           lastUpdated  totalDownloads  totalVotes  usabilityRating   id_no  \\\n",
       "0  2020-07-13 14:07:31         84246.0      1471.0         1.000000  527325   \n",
       "1  2020-04-03 16:33:49          9945.0       311.0         1.000000  576393   \n",
       "2  2020-06-04 08:53:36          1054.0        24.0         1.000000  692628   \n",
       "3  2020-12-20 13:05:27           591.0        16.0         0.970588     NaN   \n",
       "4  2020-03-15 08:56:42          5619.0       109.0         0.750000   19829   \n",
       "\n",
       "  datasetId                           datasetSlug  ...  \\\n",
       "0    527325                    coronavirusdataset  ...   \n",
       "1    576393                air-pollution-in-seoul  ...   \n",
       "2    692628                  south-korea-visitors  ...   \n",
       "3       NaN                                   NaN  ...   \n",
       "4     19829  korean-single-speaker-speech-dataset  ...   \n",
       "\n",
       "  collaborators_15_username collaborators_15_role collaborators_16_username  \\\n",
       "0                   minty99                writer                byeongukyu   \n",
       "1                       NaN                   NaN                       NaN   \n",
       "2                       NaN                   NaN                       NaN   \n",
       "3                       NaN                   NaN                       NaN   \n",
       "4                       NaN                   NaN                       NaN   \n",
       "\n",
       "  collaborators_16_role data collaborators keywords_5 keywords keywords_6  \\\n",
       "0                writer   []           NaN        NaN      NaN        NaN   \n",
       "1                   NaN   []            []        NaN      NaN        NaN   \n",
       "2                   NaN   []            []        NaN      NaN        NaN   \n",
       "3                   NaN  NaN           NaN        NaN      NaN        NaN   \n",
       "4                   NaN   []            []        NaN      NaN        NaN   \n",
       "\n",
       "  keywords_7  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
