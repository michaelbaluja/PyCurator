{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figshare API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the Figshare API. Follow these steps in order to get the necessary credentials to continue:\n",
    "1. Create a Figshare account at https://figshare.com/account/register\n",
    "2. After logging in, click on your account photo in the top right corner, and then click on 'Applications'\n",
    "3. Access API key either by:\n",
    "    - Create an application by clicking on 'Create Application'\n",
    "    - Create an API key by clicking on 'Create Personal Token'\n",
    "4. Load API key:\n",
    "    - For repeated use, follow the ```pickle_tutorial.ipynb``` instructions to create create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'OPENML_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "    - For sparser use, users can run the credentials cell and paste their API key when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Figshare API ([Figshare](https://docs.figshare.com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For querying data from API\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "from collections import OrderedDict\n",
    "\n",
    "# For loading credentials\n",
    "import pickle\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API access tokens have been stored in credentials.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "\n",
    "# Check for credentials file\n",
    "try:\n",
    "    with open('credentials.pkl', 'rb') as credentials:\n",
    "        FIGSHARE_TOKEN = pickle.load(credentials)['FIGSHARE_TOKEN']\n",
    "except:\n",
    "    FIGSHARE_TOKEN = input('Please enter your Figshare API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search constants\n",
    "BASE_URL = 'https://api.figshare.com/v2/'\n",
    "HEADERS = {'Authorization': f'token {FIGSHARE_TOKEN}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search seems to be all by type (articles, collections, etc.)\n",
    "#All are public\n",
    "#Plan is to search (1) articles, (2) collections, (3) projects\n",
    "#Article search may be minimally useful, but if have linked DOIs in object, could be linked data\n",
    "\n",
    "#Can set up to cycle through which search (articles, collections, data) and by page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall workflow: use main search to get IDs of matching objects, then use IDs to get full object details & associated files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Object ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_outputs(search_terms, search_types, save=False):\n",
    "    \"\"\"\n",
    "    Call the Figshare API for each search term and search type. \n",
    "    Results are retured in results['{term}_{type}'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms (list-like): collection of search terms to query over\n",
    "    - search_types (list-like): collection of search types to query over\n",
    "    \n",
    "    Returns:\n",
    "    - results (dict): dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    num_searches = len(search_terms) * len(search_types)\n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term, search_type in itertools.product(search_terms, search_types):\n",
    "        results[(search_term, search_type)] = get_search_output(search_term, search_type)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_output(search_term, search_type):\n",
    "    \"\"\"\n",
    "    Calls the Figshare API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term (str): keyword to seach for\n",
    "    - search_type (str): objects to search over (must be either datasets or kernels)\n",
    "   \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert isinstance(search_term, str), 'Search term must be a string'\n",
    "    assert search_type in ('articles', 'collections', 'projects'), \\\n",
    "        'Search can only be conducted over articles, collections, or projects'\n",
    "        \n",
    "    # Set search variables\n",
    "    start_page = 1\n",
    "    page_size = 1000 # Maximum page size (min = 10)\n",
    "    output = None\n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    search_params = {\n",
    "        'search_for': search_term,\n",
    "        'page': start_page, \n",
    "        'page_size': page_size,  \n",
    "        }\n",
    "        \n",
    "    search_url = f'{BASE_URL}/{search_type}'\n",
    "\n",
    "    ## Run search for public articles\n",
    "    response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "\n",
    "    ## Put output into json format\n",
    "    output = response.json()\n",
    "    \n",
    "    # Continue searching until we reach an empty page\n",
    "    while output != []:\n",
    "        # Turn outputs into DataFrame & add page info\n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['search_page'] = search_params['page']\n",
    "        \n",
    "        # Append modified output df to our cumulative search DataFrame\n",
    "        search_df = pd.concat([search_df, output_df])\n",
    "        \n",
    "        # Increment page number to query\n",
    "        search_params['page'] += 1\n",
    "\n",
    "        ## Run search for public articles\n",
    "        response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "\n",
    "        ## Put output into json format\n",
    "        output = response.json()\n",
    "        \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['iguana']\n",
    "search_types = ['collections', 'projects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_search_outputs(search_terms, search_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_object_json(object_url):\n",
    "    '''\n",
    "    Queries Figshare for object data (json file) & returns the json data as a dictionary\n",
    "    \n",
    "    Params:\n",
    "    - object_url (str): path for the dataset\n",
    "    \n",
    "    Returns:\n",
    "    - object_data_dict (dict): dictionary containing json data\n",
    "    '''\n",
    "    \n",
    "    # Download the metadata\n",
    "    response = rq.get(object_url, headers=HEADERS)\n",
    "    json_data = response.json()\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(object_paths):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the object/objects listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths (str/list-like): string or list of strings containing the paths for the objects\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df (pandas.DataFrame): DataFrame containing metadata for the requested objects\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "    \n",
    "    #create empty pandas dataframe to put results in\n",
    "    metadata_df = pd.DataFrame()\n",
    "\n",
    "    #for each path, get full object details\n",
    "    for object_path in tqdm(object_paths):\n",
    "        #URL syntax for object details is: https://api.figshare.com/v2/{search_type}/{object_id}        \n",
    "        json_data = _retrieve_object_json(object_path)\n",
    "        \n",
    "        #appending json collapses first level, which is a start\n",
    "        #for now, can leave files, custom fields, author, etc as list of dictionary\n",
    "        metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "\n",
    "        \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract IDs from DataFrame, and returns as list of strings\n",
    "metadata_dict = OrderedDict()\n",
    "\n",
    "for query, df in search_output_dict.items():\n",
    "    # Create object paths\n",
    "    _, search_type = query\n",
    "    object_ids = df.id.convert_dtypes(convert_string=True).tolist()\n",
    "    object_paths = [f'{BASE_URL}/{search_type}/{object_id}' for object_id in object_ids]\n",
    "    \n",
    "    metadata_dict[query] = get_metadata(object_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict (dict): dictionary of search output results\n",
    "    - metadata_dict (dict): dictionary of metadata results\n",
    "    - on (str/list-like): column name(s) to merge the two dicts on\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/figshare/figshare_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    Returns:\n",
    "    - df_dict (OrderedDict): OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "\n",
    "    # Ensure the on variable is proper\n",
    "    try:\n",
    "        assert len(on) == 2 or isinstance(on, str)\n",
    "        if (len(on) == 2) and (not isinstance(on, str)):\n",
    "            left_on, right_on = on\n",
    "            on = None\n",
    "    except:\n",
    "        raise ValueError('Incorrect value of \"on\" passed')\n",
    "        \n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "        # Keep just search info, id and timeline from initial extract \n",
    "        # Timeline only present in some search types\n",
    "        columns_to_keep = ['id', 'search_type', 'search_page']\n",
    "        \n",
    "        if 'timeline' in search_df.columns:\n",
    "            columns_to_keep.append('timeline')\n",
    "            \n",
    "        search_df = search_df[columns_to_keep]\n",
    "\n",
    "        #Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        if on: # only one value to merge on\n",
    "            df_all = pd.merge(search_df, metadata_df, on=on, how='inner')\n",
    "        else:\n",
    "            df_all = pd.merge(search_df, metadata_df, left_on=left_on, right_on=right_on)\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'figshare')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure figshare directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError(f'Save type must be bool or str, not {type(save_loc)}')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict[('iguana', 'collections')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code that loops through search terms and objects\n",
    "\n",
    "### also an issue with projects objects that stops code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_type = 'collections'\n",
    "search_term = 'iguana'\n",
    "search_url = f'https://api.figshare.com/v2/{search_type}'\n",
    "\n",
    "PARAMS = {\n",
    "        'search_for': search_term, #search term\n",
    "        'page': 1, \n",
    "        'page_size': 10,  \n",
    "        }\n",
    "\n",
    "response = rq.get(search_url, params=PARAMS, headers=HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FIGSHARE_TOKEN imported from credentials.pkl\n",
    "\n",
    "\n",
    "#List of search terms\n",
    "SEARCH_TERMS = ['iguana']\n",
    "\n",
    "#List of Figshare object types to search\n",
    "SEARCH_TYPES = ['articles', 'collections', 'projects']\n",
    "\n",
    "# Set dummy json output for page loop\n",
    "output = None\n",
    "\n",
    "###### LOOP 1 - loop through search terms ######\n",
    "for search_term in SEARCH_TERMS:\n",
    "    print('Searching:', search_term)\n",
    "\n",
    "    #Specify page to return with search (results are paginated) - TO DO: FIX HARD CODING\n",
    "    PAGE = 1\n",
    "    \n",
    "    #Specify number of results included on a page (default is 10, max is 1000) - TO DO: FIX HARD CODING\n",
    "    PAGE_SIZE = 1000\n",
    "\n",
    "    #Set params term\n",
    "    PARAMS = {\n",
    "        'search_for': search_term, #search term\n",
    "        'page': PAGE, \n",
    "        'page_size': PAGE_SIZE,  \n",
    "        }\n",
    "    \n",
    "    ###### LOOP 2 - loop through object types ######\n",
    "\n",
    "    for search_type in SEARCH_TYPES:\n",
    "        print(f'\\tSearching over: {search_type}')\n",
    "        \n",
    "        URL_j = f'https://api.figshare.com/v2/{search_type}'\n",
    "\n",
    "        ## Run search for public articles\n",
    "        response = rq.get(URL_j, params=PARAMS, headers=HEADERS)\n",
    "\n",
    "        ## Put output into json format\n",
    "        output = response.json()\n",
    "        \n",
    "        # Continue searching until we reach an empty page\n",
    "        while output != []:\n",
    "            #Convert output to pd dataframe and see table format\n",
    "            df_full = pd.DataFrame(output)\n",
    "\n",
    "            ## Extract IDs\n",
    "            full_ids = list(df_full.id)\n",
    "\n",
    "            ####### LOOP 3 - loop to to extract object  details by object ID ######\n",
    "\n",
    "            #create empty pandas dataframe to put results in\n",
    "            df_detailed = pd.DataFrame()\n",
    "\n",
    "            #for each ID, get full object details\n",
    "            for i in full_ids:\n",
    "                id_i = str(i)\n",
    "                #URL syntax for object details is: https://api.figshare.com/v2/{search_type}/{object_id}\n",
    "                URL_i = f'{URL_j}/{id_i}'\n",
    "                response_i = rq.get(URL_i, headers=HEADERS)\n",
    "                output_i = response_i.json()\n",
    "                #appending json collapses first level, which is a start\n",
    "                #for now, can leave files, custom fields, author, etc as list of dictionary\n",
    "                df_detailed = df_detailed.append(output_i, ignore_index=True)\n",
    "\n",
    "                #Keep just id and timeline from initial extract (all others are in detailed extract)\n",
    "                #'timeline' is NOT in projects, so...do we really need it?\n",
    "                df_small = df_full[['id','timeline']]\n",
    "\n",
    "                #Add info about which search type and page this is associated with\n",
    "                df_detailed['Search_type'] = search_type\n",
    "                df_detailed['search_page'] = PARAMS['page']\n",
    "\n",
    "                #Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "                df_all = pd.merge(df_small, df_detailed, on='id', how='inner')\n",
    "\n",
    "            #Write to csv\n",
    "            output_file = f'Figshare_{search_term}_{search_type}.csv'\n",
    "            df_all.to_csv(output_file, index=False)\n",
    "            \n",
    "            # Increment page number to query\n",
    "            PARAMS['page'] += 1\n",
    "            \n",
    "            ## Run search for public articles\n",
    "            response = rq.get(URL_j, params=PARAMS, headers=HEADERS)\n",
    "            \n",
    "            ## Put output into json format\n",
    "            output = response.json()\n",
    "\n",
    "    print(f'Finished {search_term} search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE\n",
    "#### Specify search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGSHARE_TOKEN imported from credentials.pkl\n",
    "HEADERS = {'Authorization': 'token '+ FIGSHARE_TOKEN}\n",
    "\n",
    "#Specify search terms\n",
    "SEARCH = 'machine learning'\n",
    "\n",
    "#turn into list for loop: SEARCH = ['machine learning'] \n",
    "#with whatever search terms we decide to use\n",
    "\n",
    "#Specify which search (collections, articles, projects, data)\n",
    "ARTICLE_URL = 'https://api.figshare.com/v2/articles'\n",
    "COLLECTIONS_URL = \"https://api.figshare.com/v2/collections\"\n",
    "PROJECTS_URL = \"https://api.figshare.com/v2/projects\"\n",
    "\n",
    "#turn into list for loop: SEARCH_TYPES = ['articles', 'collections', 'projects']\n",
    "#for j in SEARCH_TYPES:\n",
    "    #URL_j = \"https://api.figshare.com/v2/\" + SEARCH_TYPES_j\n",
    "    \n",
    "#article search should include datasets\n",
    "#Only return articles with the respective type. Mapping for item_type is: \n",
    "#1 - Figure, 2 - Media, 3 - Dataset, 5 - Poster, 6 - Journal contribution, 7 - Presentation, \n",
    "#8 - Thesis, 9 - Software, 11 - Online resource, 12 - Preprint, 13 - Book, 14 - Conference contribution, \n",
    "#15 - Chapter, 16 - Peer review, 17 - Educational resource, 18 - Report, 19 - Standard, 20 - Composition, \n",
    "#21 - Funding, 22 - Physical object, 23 - Data management plan, 24 - Workflow, \n",
    "#25 - Monograph, 26 - Performance, 27 - Event, 28 - Service, 29 - Model\n",
    "\n",
    "#should make a df with this and merge to get more informative item_type column in final output\n",
    "\n",
    "#Specify page to return with search (results are paginated)\n",
    "PAGE = 1\n",
    "\n",
    "#Specify number of results included on a page (default is 10, max is 1000)\n",
    "PAGE_SIZE = 10\n",
    "\n",
    "## Could iterate through pages until get response: {'message': 'Bad Request', 'code': 'BadRequest'} \n",
    "## Seems like there should be a better way\n",
    "\n",
    "#Specify page and page size parameters\n",
    "#Other search options are available, including limit and offset, but at the moment page/page size seem most useful\n",
    "    #if set both page/page size and limit/offset, get:\n",
    "    #{'message': 'Pagination options can be set either via page/page_size or limit/offset params','code': 'ConflictingPaginationOptions'}\n",
    "    \n",
    "    #'limit': 1000, #Number of results included on a page. Used for pagination with query (optional) - not sure how differs from page_size\n",
    "    #'offset': 1 #, #Where to start the listing(the offset of the first result). Used for pagination with limit (optional)\n",
    "#We don't need to specify other search parameters like institution, group, modified since, etc.\n",
    "\n",
    "#Full search parameters\n",
    "PARAMS = {\n",
    "    'search_for': SEARCH, #search term\n",
    "    'page': PAGE, \n",
    "    'page_size': PAGE_SIZE,  \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft workflow: search public articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run search for public articles\n",
    "response = rq.get(ARTICLE_URL, params=PARAMS, headers=HEADERS)\n",
    "\n",
    "## Put output into json format\n",
    "output = response.json()\n",
    "\n",
    "#Convert output to pd dataframe and see table format\n",
    "df_full = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See what output looks like\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See what df looks like\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract IDs\n",
    "full_ids = list(df_full.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to extract article details by object ID\n",
    "\n",
    "#URL syntax is: https://api.figshare.com/v2/articles/{article_id}\n",
    "\n",
    "#create empty pandas dataframe\n",
    "df_detailed = pd.DataFrame()\n",
    "\n",
    "#for each ID, \n",
    "for i in full_ids:\n",
    "    id_i = str(i)\n",
    "    URL_i = 'https://api.figshare.com/v2/articles/' + id_i\n",
    "    #print(URL_i)\n",
    "    response_i = rq.get(URL_i, headers=HEADERS)\n",
    "    json_i = response_i.json()\n",
    "    #json_normalize collapses first level, which is a start\n",
    "    #for now, can leave files, custom fields, author, etc as list of dictionary\n",
    "    df_i = pd.json_normalize(json_i)\n",
    "    df_detailed = df_detailed.append(df_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detailed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare high level metadata extract with detailed extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column names in the \"full\" initial API call\n",
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column names in the \"detailed\" API call\n",
    "df_detailed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What do these have in common?\n",
    "overlap = list(set(df_full).intersection(set(df_detailed)))\n",
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Are they the same?\n",
    "sorted(df_full.columns) == sorted(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So which ones are in one but not the other?\n",
    "sorted(df_full.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeline is not in detailed extract, but it's the only one that's not the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine full and detailed API extracts by object ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep just id and timeline from initial extract (all others are in detailed extract)\n",
    "## CHECK IF TIMELINE VAR IS IN COLLECTIONS AND PROJECT OBJECTS\n",
    "df_small = df_full[['id','timeline']]\n",
    "\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "df_all = pd.merge(df_small, df_detailed, on = \"id\", how = \"inner\")\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List files associated with public articles (by ID)\n",
    "\n",
    "#URL syntax is: https://api.figshare.com/v2/articles/{article_id}/files\n",
    "\n",
    "#looks like files are already pulled from article details, so don't need a separate call - all info should already be there\n",
    "#at some point go back and confirm that there's no new info in files API call"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
