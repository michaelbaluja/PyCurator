{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figshare API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the Figshare API. Follow these steps in order to get the necessary credentials to continue:\n",
    "1. Create a Figshare account at https://figshare.com/account/register\n",
    "2. After logging in, click on your account photo in the top right corner, and then click on 'Applications'\n",
    "3. Access API key either by:\n",
    "    - Create an application by clicking on 'Create Application'\n",
    "    - Create an API key by clicking on 'Create Personal Token'\n",
    "4. Load API key:\n",
    "    - For repeated use, follow the ```pickle_tutorial.ipynb``` instructions to create create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'FIGSHARE_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "    - For sparser use, users can run the credentials cell and paste their API key when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Figshare API ([Figshare](https://docs.figshare.com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figshare allows a variety of search types. For this script, we search: (1) articles, (2) collections, and (3) projects.\n",
    "\n",
    "\"Articles\" in this context include a variety of item types: 1 - Figure, 2 - Media, 3 - Dataset, 5 - Poster, 6 - Journal contribution, 7 - Presentation, 8 - Thesis, 9 - Software, 11 - Online resource, 12 - Preprint, 13 - Book, 14 - Conference contribution, 15 - Chapter, 16 - Peer review, 17 - Educational resource, 18 - Report, 19 - Standard, 20 - Composition, 21 - Funding, 22 - Physical object, 23 - Data management plan, 24 - Workflow, 25 - Monograph, 26 - Performance, 27 - Event, 28 - Service, 29 - Model\n",
    "\n",
    "Figshare workflow:\n",
    "1.\tSpecify search terms in a list\n",
    "2.\tSpecify search_types in a list\n",
    "    - Search can only be conducted over articles, collections, or projects\n",
    "3.\tThe function `get_search_output` calls the Figshare API for each combination of search terms (i.e., \"machine learning\" and search types (i.e., articles, collections, projects)\n",
    "    - Results are paginated, so for each search term/search type combination, return results from each page of results and combine into a single data frame\n",
    "    - Results returned are high level only (id, title, doi, URL, etc) – full object metadata is called downstream\n",
    "4.\tFunction `get_search_output` is called within `get_search_outputs`\n",
    "    - This is where the search actually happens\n",
    "    - This function is separate because (seems like??) it returns results from `get_search_output` with addition of columns indicating search term and search type and search page - but I’m only seeing search_page in the sample_df\n",
    "5.\tUse object-level URL from `get_search_output` results to get full metadata for each object:\n",
    "    - Function `_retrieve_object_json` calls Figshare API for each URL and returns JSON response\n",
    "    - Function `get_metadata` actually calls `_retrieve_object_json` to do the search and formats the results as a dataframe \n",
    "6.\tNot quite sure what the code under “perform metadata extraction” is doing\n",
    "7.\tRunning `merge_search_and_metadata_dicts` returns `KeyError: \"['search_type'] not in index\"`\n",
    "8.\tEverything below “code that loops through search terms and objects” is old and can be deleted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For querying data from API\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "from collections import OrderedDict\n",
    "\n",
    "# For loading credentials\n",
    "import pickle\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API access tokens have been stored in credentials.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "\n",
    "# Check for credentials file\n",
    "try:\n",
    "    with open('credentials.pkl', 'rb') as credentials:\n",
    "        FIGSHARE_TOKEN = pickle.load(credentials)['FIGSHARE_TOKEN']\n",
    "except:\n",
    "    FIGSHARE_TOKEN = input('Please enter your Figshare API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search constants\n",
    "BASE_URL = 'https://api.figshare.com/v2/'\n",
    "HEADERS = {'Authorization': f'token {FIGSHARE_TOKEN}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figshare allows a variety of search types. For this script, we search: (1) articles, (2) collections, and (3) projects.\n",
    "\n",
    "\"Articles\" in this context include a variety of item types: 1 - Figure, 2 - Media, 3 - Dataset, 5 - Poster, 6 - Journal contribution, 7 - Presentation, 8 - Thesis, 9 - Software, 11 - Online resource, 12 - Preprint, 13 - Book, 14 - Conference contribution, 15 - Chapter, 16 - Peer review, 17 - Educational resource, 18 - Report, 19 - Standard, 20 - Composition, 21 - Funding, 22 - Physical object, 23 - Data management plan, 24 - Workflow, 25 - Monograph, 26 - Performance, 27 - Event, 28 - Service, 29 - Model\n",
    "\n",
    "Overall workflow:\n",
    "1. Extract object IDs for each combination of search terms (i.e., \"machine learning\" and search types (i.e., articles, collections, projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Object ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_output(search_term, search_type):\n",
    "    \"\"\"\n",
    "    Calls the Figshare API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term (str): keyword to seach for\n",
    "    - search_type (str): objects to search over (must be either datasets or kernels)\n",
    "   \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert isinstance(search_term, str), 'Search term must be a string'\n",
    "    assert search_type in ('articles', 'collections', 'projects'), \\\n",
    "        'Search can only be conducted over articles, collections, or projects'\n",
    "        \n",
    "    # Set search variables\n",
    "    start_page = 1\n",
    "    page_size = 1000 # Maximum page size (min = 10)\n",
    "    output = None\n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    search_params = {\n",
    "        'search_for': search_term,\n",
    "        'page': start_page, \n",
    "        'page_size': page_size,  \n",
    "        }\n",
    "        \n",
    "    search_url = f'{BASE_URL}/{search_type}'\n",
    "\n",
    "    ## Run search for public articles\n",
    "    response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "\n",
    "    ## Put output into json format\n",
    "    output = response.json()\n",
    "    \n",
    "    # Continue searching until we reach an empty page\n",
    "    while output != []:\n",
    "        # Turn outputs into DataFrame & add page info\n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['search_page'] = search_params['page']\n",
    "        \n",
    "        # Append modified output df to our cumulative search DataFrame\n",
    "        search_df = pd.concat([search_df, output_df])\n",
    "        \n",
    "        # Increment page number to query\n",
    "        search_params['page'] += 1\n",
    "\n",
    "        ## Run search for public articles\n",
    "        response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "\n",
    "        ## Put output into json format\n",
    "        output = response.json()\n",
    "        \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs name that is more different from above function\n",
    "\n",
    "def get_search_outputs(search_terms, search_types):\n",
    "    \"\"\"\n",
    "    Call the Figshare API for each search term and search type. \n",
    "    Results are retured in results['{term}_{type}'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms (list-like): collection of search terms to query over\n",
    "    - search_types (list-like): collection of search types to query over\n",
    "    \n",
    "    Returns:\n",
    "    - results (dict): dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    num_searches = len(search_terms) * len(search_types)\n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term, search_type in itertools.product(search_terms, search_types):\n",
    "        results[(search_term, search_type)] = get_search_output(search_term, search_type)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['iguana']\n",
    "search_types = ['collections', 'projects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_search_outputs(search_terms, search_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>handle</th>\n",
       "      <th>url</th>\n",
       "      <th>published_date</th>\n",
       "      <th>timeline</th>\n",
       "      <th>search_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3582815</td>\n",
       "      <td>Data from: Vascular patterns in iguanas and ot...</td>\n",
       "      <td>10.5061/dryad.27m63.2</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/collections/3582815</td>\n",
       "      <td>2016-11-25T19:47:11Z</td>\n",
       "      <td>{'posted': '2016-11-25T19:47:11'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4755440</td>\n",
       "      <td>Data from: Vascular patterns in iguanas and ot...</td>\n",
       "      <td>10.5061/dryad.27m63</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/collections/4755440</td>\n",
       "      <td>2019-11-26T08:07:43Z</td>\n",
       "      <td>{'posted': '2019-11-26T08:07:43'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4596320</td>\n",
       "      <td>Data from: Vascular patterns in iguanas and ot...</td>\n",
       "      <td>10.5061/dryad.27m63.1</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/collections/4596320</td>\n",
       "      <td>2019-07-30T16:44:02Z</td>\n",
       "      <td>{'posted': '2019-07-30T16:44:02'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5234804</td>\n",
       "      <td>First known trace fossil of a nesting iguana (...</td>\n",
       "      <td>10.1371/journal.pone.0242935</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/collections/5234804</td>\n",
       "      <td>2020-12-09T18:32:22Z</td>\n",
       "      <td>{'posted': '2020-12-09T18:32:22'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5311858</td>\n",
       "      <td>Systemic &lt;i&gt;Helicobacter&lt;/i&gt; infection and ass...</td>\n",
       "      <td>10.1371/journal.pone.0247010</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/collections/5311858</td>\n",
       "      <td>2021-02-19T18:33:04Z</td>\n",
       "      <td>{'posted': '2021-02-19T18:33:04'}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  3582815  Data from: Vascular patterns in iguanas and ot...   \n",
       "1  4755440  Data from: Vascular patterns in iguanas and ot...   \n",
       "2  4596320  Data from: Vascular patterns in iguanas and ot...   \n",
       "3  5234804  First known trace fossil of a nesting iguana (...   \n",
       "4  5311858  Systemic <i>Helicobacter</i> infection and ass...   \n",
       "\n",
       "                            doi handle  \\\n",
       "0         10.5061/dryad.27m63.2          \n",
       "1           10.5061/dryad.27m63          \n",
       "2         10.5061/dryad.27m63.1          \n",
       "3  10.1371/journal.pone.0242935          \n",
       "4  10.1371/journal.pone.0247010          \n",
       "\n",
       "                                               url        published_date  \\\n",
       "0  https://api.figshare.com/v2/collections/3582815  2016-11-25T19:47:11Z   \n",
       "1  https://api.figshare.com/v2/collections/4755440  2019-11-26T08:07:43Z   \n",
       "2  https://api.figshare.com/v2/collections/4596320  2019-07-30T16:44:02Z   \n",
       "3  https://api.figshare.com/v2/collections/5234804  2020-12-09T18:32:22Z   \n",
       "4  https://api.figshare.com/v2/collections/5311858  2021-02-19T18:33:04Z   \n",
       "\n",
       "                            timeline  search_page  \n",
       "0  {'posted': '2016-11-25T19:47:11'}            1  \n",
       "1  {'posted': '2019-11-26T08:07:43'}            1  \n",
       "2  {'posted': '2019-07-30T16:44:02'}            1  \n",
       "3  {'posted': '2020-12-09T18:32:22'}            1  \n",
       "4  {'posted': '2021-02-19T18:33:04'}            1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_object_json(object_url):\n",
    "    '''\n",
    "    Queries Figshare for object data (json file) & returns the json data as a dictionary\n",
    "    \n",
    "    Params:\n",
    "    - object_url (str): path for the dataset\n",
    "    \n",
    "    Returns:\n",
    "    - object_data_dict (dict): dictionary containing json data\n",
    "    '''\n",
    "    \n",
    "    # Download the metadata\n",
    "    response = requests.get(object_url, headers=HEADERS)\n",
    "    json_data = response.json()\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(object_paths):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the object/objects listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths (str/list-like): string or list of strings containing the paths for the objects\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df (pandas.DataFrame): DataFrame containing metadata for the requested objects\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "    \n",
    "    #create empty pandas dataframe to put results in\n",
    "    metadata_df = pd.DataFrame()\n",
    "\n",
    "    #for each path, get full object details\n",
    "    for object_path in tqdm(object_paths):\n",
    "        #URL syntax for object details is: https://api.figshare.com/v2/{search_type}/{object_id}        \n",
    "        json_data = _retrieve_object_json(object_path)\n",
    "        \n",
    "        #appending json collapses first level, which is a start\n",
    "        #for now, can leave files, custom fields, author, etc as list of dictionary\n",
    "        metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "\n",
    "        \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 41/41 [00:30<00:00,  1.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "## Extract IDs from DataFrame, and returns as list of strings\n",
    "metadata_dict = OrderedDict()\n",
    "\n",
    "for query, df in search_output_dict.items():\n",
    "    # Create object paths\n",
    "    _, search_type = query\n",
    "    object_ids = df.id.convert_dtypes(convert_string=True).tolist()\n",
    "    object_paths = [f'{BASE_URL}/{search_type}/{object_id}' for object_id in object_ids]\n",
    "    \n",
    "    metadata_dict[query] = get_metadata(object_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict (dict): dictionary of search output results\n",
    "    - metadata_dict (dict): dictionary of metadata results\n",
    "    - on (str/list-like): column name(s) to merge the two dicts on\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/figshare/figshare_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    Returns:\n",
    "    - df_dict (OrderedDict): OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "\n",
    "    # Ensure the on variable is proper\n",
    "    try:\n",
    "        assert len(on) == 2 or isinstance(on, str)\n",
    "        if (len(on) == 2) and (not isinstance(on, str)):\n",
    "            left_on, right_on = on\n",
    "            on = None\n",
    "    except:\n",
    "        raise ValueError('Incorrect value of \"on\" passed')\n",
    "        \n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "        # Keep just search info, id and timeline from initial extract \n",
    "        # Timeline only present in some search types\n",
    "        columns_to_keep = ['id', 'search_type', 'search_page']\n",
    "        \n",
    "        if 'timeline' in search_df.columns:\n",
    "            columns_to_keep.append('timeline')\n",
    "            \n",
    "        search_df = search_df[columns_to_keep]\n",
    "\n",
    "        #Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        if on: # only one value to merge on\n",
    "            df_all = pd.merge(search_df, metadata_df, on=on, how='inner')\n",
    "        else:\n",
    "            df_all = pd.merge(search_df, metadata_df, left_on=left_on, right_on=right_on)\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'figshare')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure figshare directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError(f'Save type must be bool or str, not {type(save_loc)}')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['search_type'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18168/3013815766.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_search_and_metadata_dicts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_output_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18168/3237517379.py\u001b[0m in \u001b[0;36mmerge_search_and_metadata_dicts\u001b[1;34m(search_dict, metadata_dict, on, save)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mcolumns_to_keep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0msearch_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumns_to_keep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m#Merge small version of \"full\" dataframe with \"detailed\" dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\slabou\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3028\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3030\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3032\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\slabou\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1266\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\slabou\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1314\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['search_type'] not in index\""
     ]
    }
   ],
   "source": [
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict[('iguana', 'collections')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code that loops through search terms and objects\n",
    "\n",
    "### also an issue with projects objects that stops code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_type = 'collections'\n",
    "search_term = 'iguana'\n",
    "search_url = f'https://api.figshare.com/v2/{search_type}'\n",
    "\n",
    "PARAMS = {\n",
    "        'search_for': search_term, #search term\n",
    "        'page': 1, \n",
    "        'page_size': 10,  \n",
    "        }\n",
    "\n",
    "response = requests.get(search_url, params=PARAMS, headers=HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FIGSHARE_TOKEN imported from credentials.pkl\n",
    "\n",
    "\n",
    "#List of search terms\n",
    "SEARCH_TERMS = ['iguana']\n",
    "\n",
    "#List of Figshare object types to search\n",
    "SEARCH_TYPES = ['articles', 'collections', 'projects']\n",
    "\n",
    "# Set dummy json output for page loop\n",
    "output = None\n",
    "\n",
    "###### LOOP 1 - loop through search terms ######\n",
    "for search_term in SEARCH_TERMS:\n",
    "    print('Searching:', search_term)\n",
    "\n",
    "    #Specify page to return with search (results are paginated) - TO DO: FIX HARD CODING\n",
    "    PAGE = 1\n",
    "    \n",
    "    #Specify number of results included on a page (default is 10, max is 1000) - TO DO: FIX HARD CODING\n",
    "    PAGE_SIZE = 1000\n",
    "\n",
    "    #Set params term\n",
    "    PARAMS = {\n",
    "        'search_for': search_term, #search term\n",
    "        'page': PAGE, \n",
    "        'page_size': PAGE_SIZE,  \n",
    "        }\n",
    "    \n",
    "    ###### LOOP 2 - loop through object types ######\n",
    "\n",
    "    for search_type in SEARCH_TYPES:\n",
    "        print(f'\\tSearching over: {search_type}')\n",
    "        \n",
    "        URL_j = f'https://api.figshare.com/v2/{search_type}'\n",
    "\n",
    "        ## Run search for public articles\n",
    "        response = requests.get(URL_j, params=PARAMS, headers=HEADERS)\n",
    "\n",
    "        ## Put output into json format\n",
    "        output = response.json()\n",
    "        \n",
    "        # Continue searching until we reach an empty page\n",
    "        while output != []:\n",
    "            #Convert output to pd dataframe and see table format\n",
    "            df_full = pd.DataFrame(output)\n",
    "\n",
    "            ## Extract IDs\n",
    "            full_ids = list(df_full.id)\n",
    "\n",
    "            ####### LOOP 3 - loop to to extract object  details by object ID ######\n",
    "\n",
    "            #create empty pandas dataframe to put results in\n",
    "            df_detailed = pd.DataFrame()\n",
    "\n",
    "            #for each ID, get full object details\n",
    "            for i in full_ids:\n",
    "                id_i = str(i)\n",
    "                #URL syntax for object details is: https://api.figshare.com/v2/{search_type}/{object_id}\n",
    "                URL_i = f'{URL_j}/{id_i}'\n",
    "                response_i = requests.get(URL_i, headers=HEADERS)\n",
    "                output_i = response_i.json()\n",
    "                #appending json collapses first level, which is a start\n",
    "                #for now, can leave files, custom fields, author, etc as list of dictionary\n",
    "                df_detailed = df_detailed.append(output_i, ignore_index=True)\n",
    "\n",
    "                #Keep just id and timeline from initial extract (all others are in detailed extract)\n",
    "                #'timeline' is NOT in projects, so...do we really need it?\n",
    "                df_small = df_full[['id','timeline']]\n",
    "\n",
    "                #Add info about which search type and page this is associated with\n",
    "                df_detailed['Search_type'] = search_type\n",
    "                df_detailed['search_page'] = PARAMS['page']\n",
    "\n",
    "                #Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "                df_all = pd.merge(df_small, df_detailed, on='id', how='inner')\n",
    "\n",
    "            #Write to csv\n",
    "            output_file = f'Figshare_{search_term}_{search_type}.csv'\n",
    "            df_all.to_csv(output_file, index=False)\n",
    "            \n",
    "            # Increment page number to query\n",
    "            PARAMS['page'] += 1\n",
    "            \n",
    "            ## Run search for public articles\n",
    "            response = requests.get(URL_j, params=PARAMS, headers=HEADERS)\n",
    "            \n",
    "            ## Put output into json format\n",
    "            output = response.json()\n",
    "\n",
    "    print(f'Finished {search_term} search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE\n",
    "#### Specify search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIGSHARE_TOKEN imported from credentials.pkl\n",
    "HEADERS = {'Authorization': 'token '+ FIGSHARE_TOKEN}\n",
    "\n",
    "#Specify search terms\n",
    "SEARCH = 'machine learning'\n",
    "\n",
    "#turn into list for loop: SEARCH = ['machine learning'] \n",
    "#with whatever search terms we decide to use\n",
    "\n",
    "#Specify which search (collections, articles, projects, data)\n",
    "ARTICLE_URL = 'https://api.figshare.com/v2/articles'\n",
    "COLLECTIONS_URL = \"https://api.figshare.com/v2/collections\"\n",
    "PROJECTS_URL = \"https://api.figshare.com/v2/projects\"\n",
    "\n",
    "#turn into list for loop: SEARCH_TYPES = ['articles', 'collections', 'projects']\n",
    "#for j in SEARCH_TYPES:\n",
    "    #URL_j = \"https://api.figshare.com/v2/\" + SEARCH_TYPES_j\n",
    "    \n",
    "#article search should include datasets\n",
    "#Only return articles with the respective type. Mapping for item_type is: \n",
    "#1 - Figure, 2 - Media, 3 - Dataset, 5 - Poster, 6 - Journal contribution, 7 - Presentation, \n",
    "#8 - Thesis, 9 - Software, 11 - Online resource, 12 - Preprint, 13 - Book, 14 - Conference contribution, \n",
    "#15 - Chapter, 16 - Peer review, 17 - Educational resource, 18 - Report, 19 - Standard, 20 - Composition, \n",
    "#21 - Funding, 22 - Physical object, 23 - Data management plan, 24 - Workflow, \n",
    "#25 - Monograph, 26 - Performance, 27 - Event, 28 - Service, 29 - Model\n",
    "\n",
    "#should make a df with this and merge to get more informative item_type column in final output\n",
    "\n",
    "#Specify page to return with search (results are paginated)\n",
    "PAGE = 1\n",
    "\n",
    "#Specify number of results included on a page (default is 10, max is 1000)\n",
    "PAGE_SIZE = 10\n",
    "\n",
    "## Could iterate through pages until get response: {'message': 'Bad Request', 'code': 'BadRequest'} \n",
    "## Seems like there should be a better way\n",
    "\n",
    "#Specify page and page size parameters\n",
    "#Other search options are available, including limit and offset, but at the moment page/page size seem most useful\n",
    "    #if set both page/page size and limit/offset, get:\n",
    "    #{'message': 'Pagination options can be set either via page/page_size or limit/offset params','code': 'ConflictingPaginationOptions'}\n",
    "    \n",
    "    #'limit': 1000, #Number of results included on a page. Used for pagination with query (optional) - not sure how differs from page_size\n",
    "    #'offset': 1 #, #Where to start the listing(the offset of the first result). Used for pagination with limit (optional)\n",
    "#We don't need to specify other search parameters like institution, group, modified since, etc.\n",
    "\n",
    "#Full search parameters\n",
    "PARAMS = {\n",
    "    'search_for': SEARCH, #search term\n",
    "    'page': PAGE, \n",
    "    'page_size': PAGE_SIZE,  \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft workflow: search public articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run search for public articles\n",
    "response = requests.get(ARTICLE_URL, params=PARAMS, headers=HEADERS)\n",
    "\n",
    "## Put output into json format\n",
    "output = response.json()\n",
    "\n",
    "#Convert output to pd dataframe and see table format\n",
    "df_full = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See what output looks like\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See what df looks like\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract IDs\n",
    "full_ids = list(df_full.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to extract article details by object ID\n",
    "\n",
    "#URL syntax is: https://api.figshare.com/v2/articles/{article_id}\n",
    "\n",
    "#create empty pandas dataframe\n",
    "df_detailed = pd.DataFrame()\n",
    "\n",
    "#for each ID, \n",
    "for i in full_ids:\n",
    "    id_i = str(i)\n",
    "    URL_i = 'https://api.figshare.com/v2/articles/' + id_i\n",
    "    #print(URL_i)\n",
    "    response_i = requests.get(URL_i, headers=HEADERS)\n",
    "    json_i = response_i.json()\n",
    "    #json_normalize collapses first level, which is a start\n",
    "    #for now, can leave files, custom fields, author, etc as list of dictionary\n",
    "    df_i = pd.json_normalize(json_i)\n",
    "    df_detailed = df_detailed.append(df_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detailed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare high level metadata extract with detailed extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column names in the \"full\" initial API call\n",
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column names in the \"detailed\" API call\n",
    "df_detailed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What do these have in common?\n",
    "overlap = list(set(df_full).intersection(set(df_detailed)))\n",
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Are they the same?\n",
    "sorted(df_full.columns) == sorted(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So which ones are in one but not the other?\n",
    "sorted(df_full.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeline is not in detailed extract, but it's the only one that's not the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine full and detailed API extracts by object ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep just id and timeline from initial extract (all others are in detailed extract)\n",
    "## CHECK IF TIMELINE VAR IS IN COLLECTIONS AND PROJECT OBJECTS\n",
    "df_small = df_full[['id','timeline']]\n",
    "\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "df_all = pd.merge(df_small, df_detailed, on = \"id\", how = \"inner\")\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List files associated with public articles (by ID)\n",
    "\n",
    "#URL syntax is: https://api.figshare.com/v2/articles/{article_id}/files\n",
    "\n",
    "#looks like files are already pulled from article details, so don't need a separate call - all info should already be there\n",
    "#at some point go back and confirm that there's no new info in files API call"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
