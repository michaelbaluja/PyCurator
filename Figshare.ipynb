{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figshare API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes the Figshare API. Follow these steps in order to get the necessary credentials to continue:\n",
    "1. Create a Figshare account at https://figshare.com/account/register\n",
    "2. After logging in, click on your account photo in the top right corner, and then click on 'Applications'\n",
    "3. Access API key either by:\n",
    "    - Create an application by clicking on 'Create Application'\n",
    "    - Create an API key by clicking on 'Create Personal Token'\n",
    "4. Load API key:\n",
    "    - For repeated use, follow the ```pickle_tutorial.ipynb``` instructions to create create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'FIGSHARE_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "    - For sparser use, users can run the credentials cell and paste their API key when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- Figshare API ([Figshare](https://docs.figshare.com))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow - basically same as kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figshare allows a variety of search types. For this script, we search: (1) articles, (2) collections, and (3) projects.\n",
    "\n",
    "\"Articles\" in this context include a variety of item types: 1 - Figure, 2 - Media, 3 - Dataset, 5 - Poster, 6 - Journal contribution, 7 - Presentation, 8 - Thesis, 9 - Software, 11 - Online resource, 12 - Preprint, 13 - Book, 14 - Conference contribution, 15 - Chapter, 16 - Peer review, 17 - Educational resource, 18 - Report, 19 - Standard, 20 - Composition, 21 - Funding, 22 - Physical object, 23 - Data management plan, 24 - Workflow, 25 - Monograph, 26 - Performance, 27 - Event, 28 - Service, 29 - Model\n",
    "\n",
    "Figshare workflow:\n",
    "1.\tSpecify search terms in a list\n",
    "2.\tSpecify search types in a list\n",
    "    - Search can only be conducted over articles, collections, or projects\n",
    "3.\tThe function `get_individual_search_output` calls the Figshare API for each combination of search terms (i.e., \"machine learning\") and search types (i.e., articles, collections, projects)\n",
    "    - Results are paginated, so for each search term/search type combination, return results from each page of results and combine into a single data frame\n",
    "    - Results returned are high level only (id, title, doi, URL, etc) – full object metadata is called downstream\n",
    "4.\tFunction `get_individual_search_output` is called within `get_all_search_outputs`\n",
    "    - Returns ordered dict\n",
    "5.\tUse object-level URL from `get_individual_search_output` results to get full metadata for each object:\n",
    "    - Function `_retrieve_object_json` calls Figshare API for each URL and returns JSON response\n",
    "    - Function `get_metadata` actually calls `_retrieve_object_json` to do the search and formats the results as a dataframe \n",
    "6.\tNot quite sure what the code under “perform metadata extraction” is doing\n",
    "7.\tRunning `merge_search_and_metadata_dicts` returns `KeyError: \"['search_type'] not in index\"`\n",
    "8.\tEverything below “code that loops through search terms and objects” is old and can be deleted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For querying data from API\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "import itertools # For efficient looping over queries\n",
    "from collections import OrderedDict\n",
    "from flatten_json import flatten\n",
    "\n",
    "# For loading credentials\n",
    "import pickle\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API access tokens have been stored in credentials.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "\n",
    "# Check for credentials file\n",
    "try:\n",
    "    with open('credentials.pkl', 'rb') as credentials:\n",
    "        FIGSHARE_TOKEN = pickle.load(credentials)['FIGSHARE_TOKEN']\n",
    "except:\n",
    "    FIGSHARE_TOKEN = input('Please enter your Figshare API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search constants\n",
    "BASE_URL = 'https://api.figshare.com/v2/'\n",
    "HEADERS = {'Authorization': f'token {FIGSHARE_TOKEN}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figshare allows a variety of search types. For this script, we search: (1) articles, (2) collections, and (3) projects.\n",
    "\n",
    "\"Articles\" in this context include a variety of item types: 1 - Figure, 2 - Media, 3 - Dataset, 5 - Poster, 6 - Journal contribution, 7 - Presentation, 8 - Thesis, 9 - Software, 11 - Online resource, 12 - Preprint, 13 - Book, 14 - Conference contribution, 15 - Chapter, 16 - Peer review, 17 - Educational resource, 18 - Report, 19 - Standard, 20 - Composition, 21 - Funding, 22 - Physical object, 23 - Data management plan, 24 - Workflow, 25 - Monograph, 26 - Performance, 27 - Event, 28 - Service, 29 - Model\n",
    "\n",
    "Overall workflow:\n",
    "1. Extract object IDs for each combination of search terms (i.e., \"machine learning\" and search types (i.e., articles, collections, projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Object ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs name that is more different from above function\n",
    "\n",
    "def get_all_search_outputs(search_terms, search_types, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the Figshare API for each search term and search type. \n",
    "    Results are retured in results['{term}_{type}'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_terms (list-like): collection of search terms to query over\n",
    "    - search_types (list-like): collection of search types to query over\n",
    "    - flatten_output (bool): optional, (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - results (dict): dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    num_searches = len(search_terms) * len(search_types)\n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_term, search_type in itertools.product(search_terms, search_types):\n",
    "        results[(search_term, search_type)] = get_individual_search_output(search_term, search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_term, search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Calls the Figshare API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_term (str): keyword to seach for\n",
    "    - search_type (str): objects to search over (must be either datasets or kernels)\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "   \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert isinstance(search_term, str), 'Search term must be a string'\n",
    "    assert search_type in ('articles', 'collections', 'projects'), \\\n",
    "        'Search can only be conducted over articles, collections, or projects'\n",
    "        \n",
    "    # Set search variables\n",
    "    start_page = 1\n",
    "    page_size = 1000 # Maximum page size (min = 10)\n",
    "    output = None\n",
    "    search_df = pd.DataFrame()\n",
    "    \n",
    "    search_params = {\n",
    "        'search_for': search_term,\n",
    "        'page': start_page, \n",
    "        'page_size': page_size,  \n",
    "        }\n",
    "        \n",
    "    search_url = f'{BASE_URL}/{search_type}'\n",
    "\n",
    "    ## Run search for public articles\n",
    "    response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "\n",
    "    ## Put output into json format\n",
    "    output = response.json()\n",
    "    \n",
    "    # Continue searching until we reach an empty page\n",
    "    while output != []:\n",
    "        # Flatten output if needed\n",
    "        if flatten_output:\n",
    "            output = [flatten(result) for result in output]\n",
    "        \n",
    "        # Turn outputs into DataFrame & add page info\n",
    "        output_df = pd.DataFrame(output)\n",
    "        output_df['search_page'] = search_params['page']\n",
    "        \n",
    "        # Append modified output df to our cumulative search DataFrame\n",
    "        search_df = pd.concat([search_df, output_df])\n",
    "        \n",
    "        # Increment page number to query\n",
    "        search_params['page'] += 1\n",
    "\n",
    "        ## Run search for public articles\n",
    "        response = requests.get(search_url, params=search_params, headers=HEADERS)\n",
    "\n",
    "        ## Put output into json format\n",
    "        output = response.json()\n",
    "    \n",
    "    return search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['iguana']\n",
    "search_types = ['collections', 'projects', 'articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_terms, search_types, flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = (search_terms[0], search_types[0])\n",
    "sample_df = search_output_dict[sample_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>handle</th>\n",
       "      <th>url</th>\n",
       "      <th>published_date</th>\n",
       "      <th>timeline_posted</th>\n",
       "      <th>search_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3582815</td>\n",
       "      <td>Data from: Vascular patterns in iguanas and ot...</td>\n",
       "      <td>10.5061/dryad.27m63.2</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/collections/3582815</td>\n",
       "      <td>2016-11-25T19:47:11Z</td>\n",
       "      <td>2016-11-25T19:47:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4755440</td>\n",
       "      <td>Data from: Vascular patterns in iguanas and ot...</td>\n",
       "      <td>10.5061/dryad.27m63</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/collections/4755440</td>\n",
       "      <td>2019-11-26T08:07:43Z</td>\n",
       "      <td>2019-11-26T08:07:43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4596320</td>\n",
       "      <td>Data from: Vascular patterns in iguanas and ot...</td>\n",
       "      <td>10.5061/dryad.27m63.1</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/collections/4596320</td>\n",
       "      <td>2019-07-30T16:44:02Z</td>\n",
       "      <td>2019-07-30T16:44:02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5234804</td>\n",
       "      <td>First known trace fossil of a nesting iguana (...</td>\n",
       "      <td>10.1371/journal.pone.0242935</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/collections/5234804</td>\n",
       "      <td>2020-12-09T18:32:22Z</td>\n",
       "      <td>2020-12-09T18:32:22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5311858</td>\n",
       "      <td>Systemic &lt;i&gt;Helicobacter&lt;/i&gt; infection and ass...</td>\n",
       "      <td>10.1371/journal.pone.0247010</td>\n",
       "      <td></td>\n",
       "      <td>https://api.figshare.com/v2/collections/5311858</td>\n",
       "      <td>2021-02-19T18:33:04Z</td>\n",
       "      <td>2021-02-19T18:33:04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  3582815  Data from: Vascular patterns in iguanas and ot...   \n",
       "1  4755440  Data from: Vascular patterns in iguanas and ot...   \n",
       "2  4596320  Data from: Vascular patterns in iguanas and ot...   \n",
       "3  5234804  First known trace fossil of a nesting iguana (...   \n",
       "4  5311858  Systemic <i>Helicobacter</i> infection and ass...   \n",
       "\n",
       "                            doi handle  \\\n",
       "0         10.5061/dryad.27m63.2          \n",
       "1           10.5061/dryad.27m63          \n",
       "2         10.5061/dryad.27m63.1          \n",
       "3  10.1371/journal.pone.0242935          \n",
       "4  10.1371/journal.pone.0247010          \n",
       "\n",
       "                                               url        published_date  \\\n",
       "0  https://api.figshare.com/v2/collections/3582815  2016-11-25T19:47:11Z   \n",
       "1  https://api.figshare.com/v2/collections/4755440  2019-11-26T08:07:43Z   \n",
       "2  https://api.figshare.com/v2/collections/4596320  2019-07-30T16:44:02Z   \n",
       "3  https://api.figshare.com/v2/collections/5234804  2020-12-09T18:32:22Z   \n",
       "4  https://api.figshare.com/v2/collections/5311858  2021-02-19T18:33:04Z   \n",
       "\n",
       "       timeline_posted  search_page  \n",
       "0  2016-11-25T19:47:11            1  \n",
       "1  2019-11-26T08:07:43            1  \n",
       "2  2019-07-30T16:44:02            1  \n",
       "3  2020-12-09T18:32:22            1  \n",
       "4  2021-02-19T18:33:04            1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_object_json(object_url, flatten_output=False):\n",
    "    '''\n",
    "    Queries Figshare for object data (json file) & returns the json data as a dictionary\n",
    "    \n",
    "    Params:\n",
    "    - object_url (str): path for the dataset\n",
    "    - flatten_output (bool): optional (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - object_data_dict (dict): dictionary containing json data\n",
    "    '''\n",
    "    \n",
    "    # Download the metadata\n",
    "    response = requests.get(object_url, headers=HEADERS)\n",
    "    json_data = response.json()\n",
    "    \n",
    "    # Flatten json\n",
    "    if flatten_output:\n",
    "        json_data = flatten(json_data)\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(object_paths, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the object/objects listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths (str/list-like): string or list of strings containing the paths for the objects\n",
    "    - flatten_output (bool): optional, (default=False)\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df (pandas.DataFrame): DataFrame containing metadata for the requested objects\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "    \n",
    "    #create empty pandas dataframe to put results in\n",
    "    metadata_df = pd.DataFrame()\n",
    "\n",
    "    #for each path, get full object details\n",
    "    for object_path in tqdm(object_paths):\n",
    "        #URL syntax for object details is: https://api.figshare.com/v2/{search_type}/{object_id}        \n",
    "        json_data = _retrieve_object_json(object_path, flatten_output)\n",
    "        \n",
    "        #appending json collapses first level, which is a start\n",
    "        #for now, can leave files, custom fields, author, etc as list of dictionary\n",
    "        metadata_df = metadata_df.append(json_data, ignore_index=True)\n",
    "        \n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('iguana', 'collections') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:50<00:00,  1.23s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('iguana', 'projects') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n",
      "  0%|          | 0/152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('iguana', 'articles') metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [03:05<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "## Extract IDs from DataFrame, and returns as list of strings\n",
    "metadata_dict = OrderedDict()\n",
    "\n",
    "for query, df in search_output_dict.items():\n",
    "    print(f'Retrieving {query} metadata')\n",
    "    # Create object paths\n",
    "    _, search_type = query\n",
    "    object_ids = df.id.convert_dtypes(convert_string=True).tolist()\n",
    "    object_paths = [f'{BASE_URL}/{search_type}/{object_id}' for object_id in object_ids]\n",
    "    \n",
    "    metadata_dict[query] = get_metadata(object_paths, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict (dict): dictionary of search output results\n",
    "    - metadata_dict (dict): dictionary of metadata results\n",
    "    - on (str/list-like): column name(s) to merge the two dicts on\n",
    "    - left_on (str/list-like): column name(s) to merge the left dict on\n",
    "    - right_on (str/list-like): column name(s) to merge the right dict on\n",
    "    - save=False, optional (bool/list-like): specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/kaggle/kaggle_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    Returns:\n",
    "    - df_dict (OrderedDict): OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "        \n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "        # Keep just search info, id and timeline from initial extract \n",
    "        # Timeline only present in some search types\n",
    "        columns_to_keep = ['id', 'search_page']\n",
    "        \n",
    "        if 'timeline' in search_df.columns:\n",
    "            columns_to_keep.append('timeline')\n",
    "            \n",
    "        search_df = search_df[columns_to_keep]\n",
    "\n",
    "        # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'figshare')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure figshare directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "                \n",
    "                search_term, search_type = query_key\n",
    "                output_file = f'{search_term}_{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError(f'Save type must be bool or str, not {type(save_loc)}')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df_dict[('iguana', 'articles')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>search_page</th>\n",
       "      <th>authors_0_full_name</th>\n",
       "      <th>authors_0_id</th>\n",
       "      <th>authors_0_is_active</th>\n",
       "      <th>authors_0_orcid_id</th>\n",
       "      <th>authors_0_url_name</th>\n",
       "      <th>authors_1_full_name</th>\n",
       "      <th>authors_1_id</th>\n",
       "      <th>authors_1_is_active</th>\n",
       "      <th>...</th>\n",
       "      <th>categories_12_parent_id</th>\n",
       "      <th>categories_12_title</th>\n",
       "      <th>categories_13_id</th>\n",
       "      <th>categories_13_parent_id</th>\n",
       "      <th>categories_13_title</th>\n",
       "      <th>tags_27</th>\n",
       "      <th>tags_28</th>\n",
       "      <th>tags_29</th>\n",
       "      <th>tags_30</th>\n",
       "      <th>tags_31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9172331</td>\n",
       "      <td>1</td>\n",
       "      <td>William Ruger Porter</td>\n",
       "      <td>813288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>_</td>\n",
       "      <td>Lawrence M. Witmer</td>\n",
       "      <td>36585.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9172307</td>\n",
       "      <td>1</td>\n",
       "      <td>William Ruger Porter</td>\n",
       "      <td>813288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>_</td>\n",
       "      <td>Lawrence M. Witmer</td>\n",
       "      <td>36585.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9172310</td>\n",
       "      <td>1</td>\n",
       "      <td>William Ruger Porter</td>\n",
       "      <td>813288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>_</td>\n",
       "      <td>Lawrence M. Witmer</td>\n",
       "      <td>36585.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9172322</td>\n",
       "      <td>1</td>\n",
       "      <td>William Ruger Porter</td>\n",
       "      <td>813288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>_</td>\n",
       "      <td>Lawrence M. Witmer</td>\n",
       "      <td>36585.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9172319</td>\n",
       "      <td>1</td>\n",
       "      <td>William Ruger Porter</td>\n",
       "      <td>813288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>_</td>\n",
       "      <td>Lawrence M. Witmer</td>\n",
       "      <td>36585.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 665 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  search_page   authors_0_full_name  authors_0_id  \\\n",
       "0  9172331            1  William Ruger Porter      813288.0   \n",
       "1  9172307            1  William Ruger Porter      813288.0   \n",
       "2  9172310            1  William Ruger Porter      813288.0   \n",
       "3  9172322            1  William Ruger Porter      813288.0   \n",
       "4  9172319            1  William Ruger Porter      813288.0   \n",
       "\n",
       "   authors_0_is_active authors_0_orcid_id authors_0_url_name  \\\n",
       "0                  0.0                                     _   \n",
       "1                  0.0                                     _   \n",
       "2                  0.0                                     _   \n",
       "3                  0.0                                     _   \n",
       "4                  0.0                                     _   \n",
       "\n",
       "  authors_1_full_name  authors_1_id  authors_1_is_active  ...  \\\n",
       "0  Lawrence M. Witmer       36585.0                  0.0  ...   \n",
       "1  Lawrence M. Witmer       36585.0                  0.0  ...   \n",
       "2  Lawrence M. Witmer       36585.0                  0.0  ...   \n",
       "3  Lawrence M. Witmer       36585.0                  0.0  ...   \n",
       "4  Lawrence M. Witmer       36585.0                  0.0  ...   \n",
       "\n",
       "  categories_12_parent_id categories_12_title  categories_13_id  \\\n",
       "0                     NaN                 NaN               NaN   \n",
       "1                     NaN                 NaN               NaN   \n",
       "2                     NaN                 NaN               NaN   \n",
       "3                     NaN                 NaN               NaN   \n",
       "4                     NaN                 NaN               NaN   \n",
       "\n",
       "   categories_13_parent_id categories_13_title  tags_27  tags_28 tags_29  \\\n",
       "0                      NaN                 NaN      NaN      NaN     NaN   \n",
       "1                      NaN                 NaN      NaN      NaN     NaN   \n",
       "2                      NaN                 NaN      NaN      NaN     NaN   \n",
       "3                      NaN                 NaN      NaN      NaN     NaN   \n",
       "4                      NaN                 NaN      NaN      NaN     NaN   \n",
       "\n",
       "   tags_30  tags_31  \n",
       "0      NaN      NaN  \n",
       "1      NaN      NaN  \n",
       "2      NaN      NaN  \n",
       "3      NaN      NaN  \n",
       "4      NaN      NaN  \n",
       "\n",
       "[5 rows x 665 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
