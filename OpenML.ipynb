{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88566069",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd74bec8",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0596f",
   "metadata": {},
   "source": [
    "This notebook utilizes the OpenML API. Follow these steps in order to get the necessary credentials to continue (additional information is available at the OpenML documentation under \"Additional Information\" below):\n",
    "\n",
    "1. Create an OpenML account at https://www.openml.org/register\n",
    "2. After logging in, open your account page (click the avatar on the top right)\n",
    "3. Open 'Account Settings', then 'API authentication' to find your API key\n",
    "\n",
    "There are multiple ways of authenticating. Any of the following will work for this notebook:\n",
    "\n",
    "Temporarily:\n",
    "- When prompted below (if none of the following methods are completed), enter your API key in the text box.\n",
    "    - This method is the easiest, but must be repeated every time the notebook is loaded.\n",
    "\n",
    "Permanently:\n",
    "- Following the pickle_tutorial.ipynb instructions, create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'OPENML_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "- Use the openml CLI tool with ```openml configure apikey MYKEY```, with MYKEY being your API key.\n",
    "- Create a plain text file ```~/.openml/config``` that contains the line ```apikey=MYKEY```, with MYKEY being your API key. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89418f8b",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ab6bc",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- OpenML API ([OpenML](https://docs.openml.org/Python-start/))\n",
    "- OpenML API ([GitHub](https://github.com/openml/openml-python)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c933039a",
   "metadata": {},
   "source": [
    "Issues:\n",
    "- When importing arff exceptions, they may not be found. If this is the case, uninstall arff and install liac-arff\n",
    "- Datasets and Tasks are slow to iterate over after ~100-120 queries. Shouldn't have anything to do with setup since the loop over query id's is the same as the API code w/ added error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8fe61",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c20a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import openml, installing if necessary\n",
    "try:\n",
    "    import openml\n",
    "except ImportError as e:\n",
    "    !pip3 install openml\n",
    "    import openml\n",
    "\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "import pickle # For loading credentials\n",
    "import warnings # For warning users who do things they shouldn't\n",
    "import os # For loading credentials\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "from itertools import product # Used for iterating over nested for loops faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "\n",
    "# Check if config file or CLI variable already set key value\n",
    "try:\n",
    "    assert openml.config.apikey != ''\n",
    "except AssertionError:\n",
    "    # Check for credentials file\n",
    "    if os.path.exists('credentials.pkl'):\n",
    "        with open('credentials.pkl', 'rb') as credentials:\n",
    "            openml.config.apikey = pickle.load(credentials)['OPENML_TOKEN']\n",
    "    else:\n",
    "        openml.config.apikey = input('Please enter your OpenML API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d6233",
   "metadata": {},
   "source": [
    "## Exception Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bebac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openml.exceptions import OpenMLServerException\n",
    "dataset_exceptions = (OpenMLServerException,)\n",
    "run_exceptions = (TypeError, OpenMLServerException)\n",
    "\n",
    "from arff import BadRelationFormat, BadDataFormat\n",
    "task_exceptions = (NotImplementedError, BadRelationFormat, BadDataFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983624d6",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_attributes(obj):\n",
    "    '''\n",
    "    Given an object, returns a list of the object's value-based variables\n",
    "    \n",
    "    Params:\n",
    "    - obj (list-like): object to be analyzed \n",
    "    \n",
    "    Returns:\n",
    "    - attributes (list): value-based variables for the object given\n",
    "    '''  \n",
    "    # This code will pull all of the attributes of the provided class that are not callable or \"private\" \n",
    "    # for the class. \n",
    "    attributes = [attr for attr in dir(obj) if \n",
    "                           not hasattr(getattr(obj, attr), '__call__')\n",
    "                           and not attr.startswith('_')]\n",
    "    \n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fab84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_all_data(query_type, exceptions=[], query_limit=None, report_error_queries=False):\n",
    "    '''\n",
    "    Retrieves all possible data that the OpenML API will return for a given query type.\n",
    "    \n",
    "    Params:\n",
    "    - query_type (str): type of data to pull. options: (datasets, runs, tasks)\n",
    "    - exceptions=[], optional (list-like): list of exceptions to handle when querying data\n",
    "        ex: exceptions=(OpenMLServerException) will gracefully skip any queries that throw an OpenMLServerException\n",
    "            (can occur when a query, such as a run, has been deleted)\n",
    "    - query_limit=None, optional (int): number of queries to return. \n",
    "    - report_error_queries=False, optional (bool)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    '''\n",
    "    # Ensure proper instance type is passed in\n",
    "    try:\n",
    "        assert query_type in ('datasets', 'runs', 'tasks')\n",
    "    except AssertionError:\n",
    "        raise ValueError(f'\\'{query_type}\\' is not a valid instance type')\n",
    "    \n",
    "    # Make sure exceptions are proper\n",
    "    # If exceptions are not iterable, turn into iterable\n",
    "    try:\n",
    "        iter(exceptions)\n",
    "    except TypeError:\n",
    "        exceptions = [exceptions]\n",
    "    finally:\n",
    "        try:\n",
    "            assert all([issubclass(exception, BaseException) for exception in exceptions])\n",
    "        except (AssertionError, TypeError):\n",
    "            raise ValueError(f'Invalid exception in \\'{exceptions}\\'')\n",
    "\n",
    "\n",
    "    # Use query type to get necessary openml api functions\n",
    "    base_command = getattr(openml, query_type)\n",
    "    list_queries = getattr(base_command, f'list_{query_type}')\n",
    "    get_query = getattr(base_command, f'get_{query_type[:-1:]}')\n",
    "\n",
    "    # Get base information about every object listed on OpenML for the given query type\n",
    "    query_dict = list_queries(size=query_limit)\n",
    "    query_df = pd.DataFrame(query_dict).transpose().reset_index(drop=True)\n",
    "    \n",
    "    # Gather specific query object\n",
    "    query_ids = query_dict.keys()\n",
    "\n",
    "    queries = []\n",
    "    error_queries = []\n",
    "    for query_id in tqdm(query_ids):\n",
    "        try:\n",
    "            queries.append(get_query(query_id))\n",
    "        except exceptions as e:\n",
    "            error_queries.append((query_id, e))\n",
    "            \n",
    "    # Report error queries\n",
    "    if report_error_queries:\n",
    "        print('Error queries:\\n', error_queries)\n",
    "            \n",
    "    # Get list of attributes the queries offer\n",
    "    query_attributes = get_value_attributes(queries[0])\n",
    "    \n",
    "    # Create DataFrame to store attributes\n",
    "    query_submission_df = pd.DataFrame(columns=query_attributes)\n",
    "\n",
    "    # Append attributes of each dataset to the DataFrame\n",
    "    for query in tqdm(queries):\n",
    "        attribute_dict = {attribute: getattr(query, attribute) for attribute in query_attributes}\n",
    "        query_submission_df = query_submission_df.append(attribute_dict, ignore_index=True)\n",
    "        \n",
    "    return query_df, query_submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d787ca",
   "metadata": {},
   "source": [
    "# Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fdb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes, we set the following \"small\"-scale range over which collections to search\n",
    "size_limit = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8ee2b",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33524689",
   "metadata": {},
   "source": [
    "Note: the dataset code could be simplified via the get_datasets() function, but for uniformity sake, we follow the convention done for the runs/tasks code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09276af4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549558ca",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the attributes of an evaluation object\n",
    "# Note: this only works because the attributes that we want to track are all parameters of the class.\n",
    "# This retrieves the same end data as the get_value_attributes function, but does not require an actual instance\n",
    "\n",
    "# To dissect, openml.evaluations.OpoenMLEvalution is the class that defines our evaluation objects.\n",
    "# The .__init__ segment calls the initialization function for the class\n",
    "# The .__code__.co_varnames segment then returns the parameters of that function\n",
    "# The [1::] returns all but the first variable. Since this is a class method, the first is always 'self'\n",
    "evaluations_attributes = openml.evaluations.OpenMLEvaluation.__init__.__code__.co_varnames[1::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50167d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get different evaluation measures we can search for\n",
    "evaluations_measures = openml.evaluations.list_evaluation_measures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cec0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame to store attributes\n",
    "evaluations_df = pd.DataFrame(columns=evaluations_attributes)\n",
    "\n",
    "# Get evaluation data for each available measure\n",
    "for measure in tqdm(evaluations_measures):\n",
    "    # Query all data for a given evaluation measure\n",
    "    evaluations_dict = openml.evaluations.list_evaluations(measure, size=size_limit)\n",
    "    \n",
    "    # Adds the queried data to the DataFrame\n",
    "    for _, query in evaluations_dict.items():\n",
    "        attribute_dict = {attribute: getattr(query, attribute) for attribute in evaluations_attributes}\n",
    "\n",
    "        evaluations_df = evaluations_df.append(attribute_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c77008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049656a",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df, runs_submission_df = retrieve_all_data(query_type='runs',\n",
    "                                                exceptions=run_exceptions,\n",
    "                                                query_limit=size_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982597e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cfd8be",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c012b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_df, tasks_submission_df = retrieve_all_data(query_type='tasks', \n",
    "                                                  exceptions=task_exceptions,\n",
    "                                                  query_limit=size_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbc67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
