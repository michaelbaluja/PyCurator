{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88566069",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668961c6",
   "metadata": {},
   "source": [
    "NOTE: A few of the changes made (global variables being referenced from inside functions) were done in order to ease the transition to object oriented design without having to change any of the function structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c2b20",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfc364",
   "metadata": {},
   "source": [
    "This notebook utilizes the OpenML API. Follow these steps in order to get the necessary credentials to continue (additional information is available at the OpenML documentation under \"Additional Information\" below):\n",
    "\n",
    "1. Create an OpenML account at https://www.openml.org/register\n",
    "2. After logging in, open your account page (click the avatar on the top right)\n",
    "3. Open 'Account Settings', then 'API authentication' to find your API key\n",
    "\n",
    "There are multiple ways of authenticating. Any of the following will work for this notebook:\n",
    "\n",
    "Temporarily:\n",
    "- When prompted below (if none of the following methods are completed), enter your API key in the text box.\n",
    "    - This method is the easiest, but must be repeated every time the notebook is loaded.\n",
    "\n",
    "Permanently:\n",
    "- Following the pickle_tutorial.ipynb instructions, create a ```./credentials.pkl``` file that holds a dictionary containing the entry ```{'OPENML_TOKEN': MYKEY}```, with MYKEY being your API key.\n",
    "- Use the openml CLI tool with ```openml configure apikey MYKEY```, with MYKEY being your API key.\n",
    "- Create a plain text file ```~/.openml/config``` that contains the line ```apikey=MYKEY```, with MYKEY being your API key. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54722c4e",
   "metadata": {},
   "source": [
    "## Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed27fb7",
   "metadata": {},
   "source": [
    "Documentation Guide:\n",
    "- OpenML API ([OpenML](https://docs.openml.org/Python-start/))\n",
    "- OpenML API ([GitHub](https://github.com/openml/openml-python)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09191da9",
   "metadata": {},
   "source": [
    "Issues:\n",
    "- When importing arff exceptions, they may not be found. If this is the case, uninstall arff and install liac-arff\n",
    "- Datasets and Tasks are slow to iterate over after ~100-120 queries. Shouldn't have anything to do with setup since the loop over query id's is the same as the API code w/ added error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8fe61",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c20a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import openml, installing if necessary\n",
    "try:\n",
    "    import openml\n",
    "except ImportError as e:\n",
    "    !pip3 install openml\n",
    "    import openml\n",
    "\n",
    "import pandas as pd # For storing/manipulating query data\n",
    "import pickle # For loading credentials\n",
    "import warnings # For warning users who do things they shouldn't\n",
    "import os # For loading credentials\n",
    "from tqdm import tqdm # Gives status bar on loop completion\n",
    "from itertools import product # Used for iterating over nested for loops faster\n",
    "from flatten_json import json\n",
    "from utils import flatten_nested_df\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "473a871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "\n",
    "# Check if config file or CLI variable already set key value\n",
    "try:\n",
    "    assert openml.config.apikey != ''\n",
    "except AssertionError:\n",
    "    # Check for credentials file\n",
    "    if os.path.exists('credentials.pkl'):\n",
    "        with open('credentials.pkl', 'rb') as credentials:\n",
    "            openml.config.apikey = pickle.load(credentials)['OPENML_TOKEN']\n",
    "    else:\n",
    "        openml.config.apikey = input('Please enter your OpenML API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983624d6",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0c4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_value_attributes(obj):\n",
    "    \"\"\"\n",
    "    Given an object, returns a list of the object's value-based variables\n",
    "    \n",
    "    Params:\n",
    "    - obj : list-like \n",
    "        object to be analyzed \n",
    "    \n",
    "    Returns:\n",
    "    - attributes : list\n",
    "        value-based variables for the object given\n",
    "    \"\"\"  \n",
    "    \n",
    "    # This code will pull all of the attributes of the provided class that are not callable or \"private\" \n",
    "    # for the class. \n",
    "    attributes = [attr for attr in dir(obj) if \n",
    "                           not hasattr(getattr(obj, attr), '__call__')\n",
    "                           and not attr.startswith('_')]\n",
    "    \n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a931001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_evaluations_search_output(flatten_output=False):\n",
    "    # Get different evaluation measures we can search for\n",
    "    evaluations_measures = openml.evaluations.list_evaluation_measures()\n",
    "    \n",
    "    # Create DataFrame to store attributes\n",
    "    evaluations_df = pd.DataFrame()\n",
    "\n",
    "    # Get evaluation data for each available measure\n",
    "    for measure in tqdm(evaluations_measures):\n",
    "        # Query all data for a given evaluation measure\n",
    "        evaluations_dict = openml.evaluations.list_evaluations(measure, size=size_limit)\n",
    "\n",
    "        try:\n",
    "            # Grab one of the evaluations in order to extract attributes\n",
    "            sample_evaluation = next(iter(evaluations_dict.items()))[1]\n",
    "        # StopIteration will occur in the preceding code if an evaluation search returns no results for a given measure\n",
    "        except StopIteration:\n",
    "            continue\n",
    "\n",
    "        # Get list of attributes the evaluation offers\n",
    "        evaluations_attributes = _get_value_attributes(sample_evaluation) \n",
    "\n",
    "        # Adds the queried data to the DataFrame\n",
    "        for query in evaluations_dict.values():\n",
    "            attribute_dict = {attribute: getattr(query, attribute) for attribute in evaluations_attributes}\n",
    "            evaluations_df = evaluations_df.append(attribute_dict, ignore_index=True)\n",
    "\n",
    "        evaluations_df = flatten_nested_df(evaluations_df)\n",
    "        \n",
    "    return evaluations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c227dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_outputs(search_types, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Call the OpenML API for each search type. \n",
    "    Results are retured in results['({type},)'] = df\n",
    "    \n",
    "    Params:\n",
    "    - search_types : list-like \n",
    "        collection of search types to query over\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - results : dict\n",
    "        dictionary consisting of returned DataFrames from get_search_output for each query\n",
    "    \"\"\"\n",
    "    \n",
    "    results = OrderedDict()\n",
    "\n",
    "    for search_type in search_types:\n",
    "        results[(search_type,)] = get_individual_search_output(search_type, flatten_output)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e584757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_search_output(search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Calls the OpenML API with the specified search term and returns the search output results.\n",
    "    \n",
    "    Params:\n",
    "    - search_type : str\n",
    "        Must be in ('conferences', 'datasets', 'evaluations', 'papers', 'tasks')\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "   \n",
    "    Returns:\n",
    "    - query_df : pandas.DataFrame\n",
    "        DataFrame containing the output of the search query\n",
    "    \"\"\"\n",
    "    # Ensure proper instance type is passed in\n",
    "    try:\n",
    "        assert search_type in ('datasets', 'runs', 'tasks', 'evaluations')\n",
    "    except AssertionError:\n",
    "        raise ValueError(f'\\'{search_type}\\' is not a valid instance type')\n",
    "    \n",
    "    # Handle special case for evaluations\n",
    "    if search_type == 'evaluations':\n",
    "        return _get_evaluations_search_output(flatten_output)\n",
    "    \n",
    "    # Use query type to get necessary openml api functions\n",
    "    base_command = getattr(openml, search_type)\n",
    "    list_queries = getattr(base_command, f'list_{search_type}')\n",
    "\n",
    "    # Get base information about every object listed on OpenML for the given query type\n",
    "    query_dict = list_queries(size=size_limit)\n",
    "    query_df = pd.DataFrame(query_dict).transpose().reset_index(drop=True)\n",
    "    \n",
    "    # Flatten the nested DataFrame\n",
    "    if flatten_output:\n",
    "        query_df = flatten_nested_df(query_df)\n",
    "    \n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d787ca",
   "metadata": {},
   "source": [
    "# Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8fdb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes, we set the following \"small\"-scale range over which collections to search\n",
    "size_limit = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a3fa641",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_types = ['datasets', 'runs', 'tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a75df89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_output_dict = get_all_search_outputs(search_types, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3661c94",
   "metadata": {},
   "source": [
    "## Get Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "290269eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_metadata(object_paths, search_type, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves the metadata for the object/objects listed in object_paths\n",
    "    \n",
    "    Params:\n",
    "    - object_paths : str/list-like\n",
    "    - search_type : str\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output\n",
    "    \n",
    "    Returns:\n",
    "    - metadata_df : pandas.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # If a singular search term is provided as a string, need to wrap it in a list\n",
    "    if type(object_paths) == str:\n",
    "        object_paths = [object_paths]\n",
    "    \n",
    "    # Make sure our input is valid\n",
    "    assert len(object_paths) > 0, 'Please enter at least one object id'\n",
    "    \n",
    "    base_command = getattr(openml, search_type)\n",
    "    get_query = getattr(base_command, f'get_{search_type[:-1:]}')\n",
    "    \n",
    "    # Request each query\n",
    "    queries = []\n",
    "    error_queries = []\n",
    "    for object_path in tqdm(object_paths):\n",
    "        try:\n",
    "            queries.append(get_query(object_path))\n",
    "        except:\n",
    "            error_queries.append(object_path)\n",
    "    \n",
    "    \n",
    "    # Get list of attributes the queries offer\n",
    "    query_attributes = _get_value_attributes(queries[0])\n",
    "\n",
    "    # Create DataFrame to store attributes\n",
    "    query_attribute_df = pd.DataFrame(columns=query_attributes)\n",
    "\n",
    "    # Append attributes of each dataset to the DataFrame\n",
    "    for query in tqdm(queries):\n",
    "        attribute_dict = {attribute: getattr(query, attribute) for attribute in query_attributes}\n",
    "        query_attribute_df = query_attribute_df.append(attribute_dict, ignore_index=True)\n",
    "        \n",
    "    # Flatten the nested DataFrame\n",
    "    if flatten_output:\n",
    "        query_attribute_df = flatten_nested_df(query_attribute_df)\n",
    "\n",
    "    return query_attribute_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3668c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_metadata(search_output_dict, flatten_output=False):\n",
    "    \"\"\"\n",
    "    Retrieves all of the metadata that relates to the provided DataFrames\n",
    "    \n",
    "    Params:\n",
    "    - search_output_dict : dict\n",
    "        Dictionary of DataFrames from get_all_search_outputs\n",
    "    - flatten_output : bool, optional (default=False)\n",
    "        flag for flattening nested columns of output  \n",
    "      \n",
    "    Returns:\n",
    "    - metadata_dict : collections.OrderedDict\n",
    "        OrderedDict of DataFrames with metadata for each query\n",
    "        Order matches the order of search_output_dict\n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dict = OrderedDict()\n",
    "\n",
    "    for query, df in search_output_dict.items():\n",
    "        print(f'Retrieving {query} metadata')\n",
    "\n",
    "        # Get ID name\n",
    "        search_type = query[0]\n",
    "\n",
    "        if search_type == 'datasets':\n",
    "            id_name = 'did'\n",
    "        elif search_type == 'runs':\n",
    "            id_name = 'run_id'\n",
    "        elif search_type == 'tasks':\n",
    "            id_name = 'tid'\n",
    "\n",
    "        # Grab the object paths as the id's from the DataFrame\n",
    "        object_paths = df[id_name].values\n",
    "\n",
    "        metadata_dict[query] = get_query_metadata(object_paths, search_type, flatten_output)\n",
    "        \n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d620dba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('datasets',) metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:08<00:00,  2.94it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 199.69it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 451.83it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 241.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('runs',) metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving ('tasks',) metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:08<00:00,  2.94it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 352.84it/s]\n"
     ]
    }
   ],
   "source": [
    "metadata_dict = get_all_metadata(search_output_dict, flatten_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d273631",
   "metadata": {},
   "source": [
    "## Combining Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad57ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_search_and_metadata_dicts(search_dict, metadata_dict, on=None, left_on=None, right_on=None, save=False):\n",
    "    \"\"\"\n",
    "    Merges together all of the search and metadata DataFrames by the given 'on' key\n",
    "    \n",
    "    Params:\n",
    "    - search_dict : dict\n",
    "        dictionary of search output results\n",
    "    - metadata_dict : dict\n",
    "        dictionary of metadata results\n",
    "    - on : str/list-like\n",
    "        column name(s) to merge the two dicts on\n",
    "    - left_on : str/list-like\n",
    "        column name(s) to merge the left dict on\n",
    "    - right_on : str/list-like\n",
    "        column name(s) to merge the right dict on\n",
    "    - save : bool, optional (default=False)\n",
    "        specifies if the output DataFrames should be saved\n",
    "        If True: saves to file of format 'data/kaggle/kaggle_{search_term}_{search_type}.csv'\n",
    "        If list-like: saves to respective location in list of save locations\n",
    "            Must contain enough strings (one per query; len(search_terms) * len(search_types))\n",
    "            \n",
    "    If the on/left_on/right_on values are not explicitely specified, behavior defaults to what is done\n",
    "    in the pandas documentation\n",
    "    \n",
    "    Returns:\n",
    "    - df_dict : OrderedDict\n",
    "        OrderedDict containing all of the merged search/metadata dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the dictionaries contain the same searches\n",
    "    assert search_dict.keys() == metadata_dict.keys(), 'Dictionaries must contain the same searches'\n",
    "    \n",
    "    num_dataframes = len(search_dict)\n",
    "    \n",
    "    # Ensure the save variable data is proper\n",
    "    try:\n",
    "        if isinstance(save, bool):\n",
    "            save = [save] * num_dataframes\n",
    "        assert len(save) == num_dataframes\n",
    "    except:\n",
    "        raise ValueError('Incorrect save value(s)')\n",
    "\n",
    "    # Merge the DataFrames\n",
    "    df_dict = OrderedDict()\n",
    "    for (query_key, search_df), (query_key, metadata_df), save_loc in zip(search_dict.items(), \n",
    "                                                                          metadata_dict.items(), \n",
    "                                                                          save):\n",
    "\n",
    "        # Merge small version of \"full\" dataframe with \"detailed\" dataframe\n",
    "        df_all = pd.merge(search_df, metadata_df, on=on, left_on=left_on, right_on=right_on, how='outer')\n",
    "            \n",
    "        # Save DataFrame\n",
    "        if save_loc:\n",
    "            data_dir = os.path.join('data', 'openml')\n",
    "            if isinstance(save_loc, str):\n",
    "                output_file = save_loc\n",
    "            elif isinstance(save_loc, bool):\n",
    "                # Ensure kaggle directory is already created\n",
    "                if not os.path.isdir(data_dir):\n",
    "                    os.path.mkdir(data_dir)\n",
    "\n",
    "                search_type = query_key[0]\n",
    "                output_file = f'{search_type}.csv'\n",
    "            else:\n",
    "                raise ValueError('Save type must be bool or str')\n",
    "\n",
    "            search_df.to_csv(os.path.join(data_dir, output_file), index=False)\n",
    "        \n",
    "        df_dict[query_key] = df_all\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e197799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = merge_search_and_metadata_dicts(search_output_dict, metadata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aff8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [01:05<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add evaluations data (doesn't have metadata so had to be handled separately)\n",
    "df_dict[('evaluations',)] = get_individual_search_output('evaluations', flatten_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11514223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>array_data</th>\n",
       "      <th>data_id</th>\n",
       "      <th>data_name</th>\n",
       "      <th>flow_id</th>\n",
       "      <th>flow_name</th>\n",
       "      <th>function</th>\n",
       "      <th>run_id</th>\n",
       "      <th>setup_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>upload_time</th>\n",
       "      <th>uploader</th>\n",
       "      <th>uploader_name</th>\n",
       "      <th>value</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.93111,0.999975,0.994856,0.0,1,0.990326]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>anneal</td>\n",
       "      <td>76.0</td>\n",
       "      <td>weka.Bagging_REPTree(1)</td>\n",
       "      <td>area_under_roc_curve</td>\n",
       "      <td>62.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-04-06 23:57:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jan van Rijn</td>\n",
       "      <td>0.995034</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.730267,0.998862,0.976922,0.0,1,0.978059]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>anneal</td>\n",
       "      <td>59.0</td>\n",
       "      <td>weka.JRip(1)</td>\n",
       "      <td>area_under_roc_curve</td>\n",
       "      <td>237.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-04-07 01:34:48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jan van Rijn</td>\n",
       "      <td>0.978916</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.973736,0.998217,0.990664,0.0,1,0.991929]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>anneal</td>\n",
       "      <td>67.0</td>\n",
       "      <td>weka.BayesNet_K2(1)</td>\n",
       "      <td>area_under_roc_curve</td>\n",
       "      <td>359.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-04-07 04:08:17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jan van Rijn</td>\n",
       "      <td>0.992099</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.936728,0.999975,0.998962,0.0,1,0.999009]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>anneal</td>\n",
       "      <td>65.0</td>\n",
       "      <td>weka.RandomForest(1)</td>\n",
       "      <td>area_under_roc_curve</td>\n",
       "      <td>413.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-04-07 04:35:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jan van Rijn</td>\n",
       "      <td>0.998598</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.874438,0.999368,0.997455,0.0,1,0.999446]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>anneal</td>\n",
       "      <td>74.0</td>\n",
       "      <td>weka.Logistic(1)</td>\n",
       "      <td>area_under_roc_curve</td>\n",
       "      <td>500.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-04-07 06:52:21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Jan van Rijn</td>\n",
       "      <td>0.996849</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>[0.245111,0.252288,0.999395]</td>\n",
       "      <td>40670.0</td>\n",
       "      <td>dna</td>\n",
       "      <td>16347.0</td>\n",
       "      <td>sklearn.pipeline.Pipeline(simpleimputer=sklear...</td>\n",
       "      <td>unweighted_recall</td>\n",
       "      <td>10423855.0</td>\n",
       "      <td>8255697.0</td>\n",
       "      <td>167140.0</td>\n",
       "      <td>2019-12-05 03:03:03</td>\n",
       "      <td>8323.0</td>\n",
       "      <td>Heinrich Peters</td>\n",
       "      <td>0.498931</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>[0.950456,0.951634,0.962515]</td>\n",
       "      <td>40670.0</td>\n",
       "      <td>dna</td>\n",
       "      <td>16347.0</td>\n",
       "      <td>sklearn.pipeline.Pipeline(simpleimputer=sklear...</td>\n",
       "      <td>unweighted_recall</td>\n",
       "      <td>10423856.0</td>\n",
       "      <td>8255805.0</td>\n",
       "      <td>167140.0</td>\n",
       "      <td>2019-12-05 03:06:05</td>\n",
       "      <td>8323.0</td>\n",
       "      <td>Heinrich Peters</td>\n",
       "      <td>0.954868</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>[0.954368,0.951634,0.964933]</td>\n",
       "      <td>40670.0</td>\n",
       "      <td>dna</td>\n",
       "      <td>16347.0</td>\n",
       "      <td>sklearn.pipeline.Pipeline(simpleimputer=sklear...</td>\n",
       "      <td>unweighted_recall</td>\n",
       "      <td>10423857.0</td>\n",
       "      <td>8255887.0</td>\n",
       "      <td>167140.0</td>\n",
       "      <td>2019-12-05 03:09:54</td>\n",
       "      <td>8323.0</td>\n",
       "      <td>Heinrich Peters</td>\n",
       "      <td>0.956978</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>[0,0,0,0,0,1]</td>\n",
       "      <td>1478.0</td>\n",
       "      <td>har</td>\n",
       "      <td>16345.0</td>\n",
       "      <td>sklearn.pipeline.Pipeline(simpleimputer=sklear...</td>\n",
       "      <td>unweighted_recall</td>\n",
       "      <td>10423858.0</td>\n",
       "      <td>8255841.0</td>\n",
       "      <td>14970.0</td>\n",
       "      <td>2019-12-05 03:14:02</td>\n",
       "      <td>8323.0</td>\n",
       "      <td>Heinrich Peters</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>[0.936115,0.922876,0.926239]</td>\n",
       "      <td>40670.0</td>\n",
       "      <td>dna</td>\n",
       "      <td>16347.0</td>\n",
       "      <td>sklearn.pipeline.Pipeline(simpleimputer=sklear...</td>\n",
       "      <td>unweighted_recall</td>\n",
       "      <td>10423859.0</td>\n",
       "      <td>8256081.0</td>\n",
       "      <td>167140.0</td>\n",
       "      <td>2019-12-05 03:18:13</td>\n",
       "      <td>8323.0</td>\n",
       "      <td>Heinrich Peters</td>\n",
       "      <td>0.92841</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>925 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      array_data  data_id data_name  flow_id  \\\n",
       "0     [0.93111,0.999975,0.994856,0.0,1,0.990326]      1.0    anneal     76.0   \n",
       "1    [0.730267,0.998862,0.976922,0.0,1,0.978059]      1.0    anneal     59.0   \n",
       "2    [0.973736,0.998217,0.990664,0.0,1,0.991929]      1.0    anneal     67.0   \n",
       "3    [0.936728,0.999975,0.998962,0.0,1,0.999009]      1.0    anneal     65.0   \n",
       "4    [0.874438,0.999368,0.997455,0.0,1,0.999446]      1.0    anneal     74.0   \n",
       "..                                           ...      ...       ...      ...   \n",
       "920                 [0.245111,0.252288,0.999395]  40670.0       dna  16347.0   \n",
       "921                 [0.950456,0.951634,0.962515]  40670.0       dna  16347.0   \n",
       "922                 [0.954368,0.951634,0.964933]  40670.0       dna  16347.0   \n",
       "923                                [0,0,0,0,0,1]   1478.0       har  16345.0   \n",
       "924                 [0.936115,0.922876,0.926239]  40670.0       dna  16347.0   \n",
       "\n",
       "                                             flow_name              function  \\\n",
       "0                              weka.Bagging_REPTree(1)  area_under_roc_curve   \n",
       "1                                         weka.JRip(1)  area_under_roc_curve   \n",
       "2                                  weka.BayesNet_K2(1)  area_under_roc_curve   \n",
       "3                                 weka.RandomForest(1)  area_under_roc_curve   \n",
       "4                                     weka.Logistic(1)  area_under_roc_curve   \n",
       "..                                                 ...                   ...   \n",
       "920  sklearn.pipeline.Pipeline(simpleimputer=sklear...     unweighted_recall   \n",
       "921  sklearn.pipeline.Pipeline(simpleimputer=sklear...     unweighted_recall   \n",
       "922  sklearn.pipeline.Pipeline(simpleimputer=sklear...     unweighted_recall   \n",
       "923  sklearn.pipeline.Pipeline(simpleimputer=sklear...     unweighted_recall   \n",
       "924  sklearn.pipeline.Pipeline(simpleimputer=sklear...     unweighted_recall   \n",
       "\n",
       "         run_id   setup_id   task_id          upload_time  uploader  \\\n",
       "0          62.0       17.0       1.0  2014-04-06 23:57:45       1.0   \n",
       "1         237.0        4.0       1.0  2014-04-07 01:34:48       1.0   \n",
       "2         359.0       12.0       1.0  2014-04-07 04:08:17       1.0   \n",
       "3         413.0       10.0       1.0  2014-04-07 04:35:45       1.0   \n",
       "4         500.0       15.0       1.0  2014-04-07 06:52:21       1.0   \n",
       "..          ...        ...       ...                  ...       ...   \n",
       "920  10423855.0  8255697.0  167140.0  2019-12-05 03:03:03    8323.0   \n",
       "921  10423856.0  8255805.0  167140.0  2019-12-05 03:06:05    8323.0   \n",
       "922  10423857.0  8255887.0  167140.0  2019-12-05 03:09:54    8323.0   \n",
       "923  10423858.0  8255841.0   14970.0  2019-12-05 03:14:02    8323.0   \n",
       "924  10423859.0  8256081.0  167140.0  2019-12-05 03:18:13    8323.0   \n",
       "\n",
       "       uploader_name     value values  \n",
       "0       Jan van Rijn  0.995034   None  \n",
       "1       Jan van Rijn  0.978916   None  \n",
       "2       Jan van Rijn  0.992099   None  \n",
       "3       Jan van Rijn  0.998598   None  \n",
       "4       Jan van Rijn  0.996849   None  \n",
       "..               ...       ...    ...  \n",
       "920  Heinrich Peters  0.498931   None  \n",
       "921  Heinrich Peters  0.954868   None  \n",
       "922  Heinrich Peters  0.956978   None  \n",
       "923  Heinrich Peters  0.166667   None  \n",
       "924  Heinrich Peters   0.92841   None  \n",
       "\n",
       "[925 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[('evaluations',)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
